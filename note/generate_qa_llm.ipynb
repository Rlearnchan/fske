{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cb996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Setting λ° ν•„μ λΌμ΄λΈλ¬λ¦¬\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# ν”„λ΅μ νΈ λ£¨νΈ κ²½λ΅ μ¶”κ°€\n",
    "sys.path.append(os.path.join(os.getcwd(), 'krx_llm_dataset'))\n",
    "\n",
    "from utils import (\n",
    "    get_law_text, process_law_data, make_index, \n",
    "    mcqa_graph, qa_graph, show_sample, show_spec,\n",
    "    read_jsonl, write_jsonl\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "# λ΅μ»¬ LLM λ¨λΈ κ²½λ΅ μ„¤μ • (ν•„μ”μ‹ μμ •)\n",
    "# κΈ°λ³Έκ°’: \"/workspace/models/exaone-4.0-32b\"\n",
    "# μ‹¤μ  λ¨λΈ κ²½λ΅μ— λ§κ² μμ •ν•μ„Έμ”\n",
    "LOCAL_MODEL_PATH = os.getenv(\"LOCAL_MODEL_PATH\", \"/workspace/models/exaone-4.0-32b\")\n",
    "\n",
    "# μ²΄ν¬ν¬μΈνΈ λ° λ°°μΉ μ„¤μ •\n",
    "CHECKPOINT_DIR = \"../data/law/checkpoints\"\n",
    "BATCH_SIZE = 5  # λ΅μ»¬ LLMμ΄λ―€λ΅ μ‘μ€ λ°°μΉ ν¬κΈ° μ‚¬μ©\n",
    "SAVE_INTERVAL = 10  # 10κ°λ§λ‹¤ μ¤‘κ°„ μ €μ¥\n",
    "\n",
    "# μ²΄ν¬ν¬μΈνΈ λ””λ ‰ν† λ¦¬ μƒμ„±\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"β… λ¨λ“ λ΅λ”© μ™„λ£\")\n",
    "print(f\"π“ λ΅μ»¬ LLM λ¨λΈ κ²½λ΅: {LOCAL_MODEL_PATH}\")\n",
    "print(f\"π“ μ²΄ν¬ν¬μΈνΈ λ””λ ‰ν† λ¦¬: {CHECKPOINT_DIR}\")\n",
    "print(f\"β™οΈ λ°°μΉ ν¬κΈ°: {BATCH_SIZE}, μ €μ¥ κ°„κ²©: {SAVE_INTERVAL}\")\n",
    "print(\"β οΈ λ¨λΈ κ²½λ΅κ°€ μ¬λ°”λ¥Έμ§€ ν™•μΈν•μ„Έμ”!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97c532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. λ²•λ¥  λ°μ΄ν„° μ½κΈ° λ° μ „μ²λ¦¬\n",
    "\n",
    "# λ²•λ¥  JSON νμΌ κ²½λ΅ μ„¤μ •\n",
    "law_file_path = \"../data/law/selected_laws.json\"  # λ²•λ¥  λ©λ΅ νμΌ\n",
    "# law_file_path = \"../data/law/law_sample.json\"  # μƒμ„Έ λ²•λ Ή λ‚΄μ© νμΌ (μ„ νƒμ μΌλ΅ μ‚¬μ©)\n",
    "\n",
    "print(f\"π“– λ²•λ¥  λ°μ΄ν„° νμΌ: {law_file_path}\")\n",
    "\n",
    "# λ²•λ¥  λ°μ΄ν„° μ½κΈ°\n",
    "try:\n",
    "    law_data = get_law_text(law_file_path)\n",
    "    print(f\"β… λ²•λ¥  λ°μ΄ν„° λ΅λ”© μ™„λ£: {len(law_data)}κ° ν•­λ©\")\n",
    "    \n",
    "    # μƒν” λ°μ΄ν„° ν™•μΈ\n",
    "    if len(law_data) > 0:\n",
    "        print(\"\\nπ“‹ μ²« λ²μ§Έ ν•­λ© μƒν”:\")\n",
    "        print(f\"μ λ©: {law_data[0]['title']}\")\n",
    "        print(f\"λ‚΄μ©: {law_data[0]['contents'][:200]}...\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"β λ²•λ¥  λ°μ΄ν„° λ΅λ”© μ‹¤ν¨: {e}\")\n",
    "    print(\"νμΌ κ²½λ΅λ¥Ό ν™•μΈν•μ„Έμ”!\")\n",
    "    law_data = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6beb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. λ°°μΉ μ²λ¦¬ λ° μ²΄ν¬ν¬μΈνΈ μ‹μ¤ν… μ •μ\n",
    "\n",
    "def load_checkpoint(checkpoint_file: str) -> Dict:\n",
    "    \"\"\"μ²΄ν¬ν¬μΈνΈ νμΌμ„ λ΅λ“ν•©λ‹λ‹¤.\"\"\"\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        try:\n",
    "            with open(checkpoint_file, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"μ²΄ν¬ν¬μΈνΈ λ΅λ“ μ¤λ¥: {e}\")\n",
    "    return {\"completed_batches\": [], \"results\": [], \"failed_items\": []}\n",
    "\n",
    "def save_checkpoint(checkpoint_file: str, data: Dict):\n",
    "    \"\"\"μ²΄ν¬ν¬μΈνΈ νμΌμ„ μ €μ¥ν•©λ‹λ‹¤.\"\"\"\n",
    "    try:\n",
    "        data['timestamp'] = time.time()\n",
    "        data['formatted_time'] = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        with open(checkpoint_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"μ²΄ν¬ν¬μΈνΈ μ €μ¥ μ¤λ¥: {e}\")\n",
    "        return False\n",
    "\n",
    "def create_batches(data: List[Dict], batch_size: int) -> List[List[Dict]]:\n",
    "    \"\"\"λ°μ΄ν„°λ¥Ό λ°°μΉ λ‹¨μ„λ΅ λ¶„ν• ν•©λ‹λ‹¤.\"\"\"\n",
    "    batches = []\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batches.append(data[i:i + batch_size])\n",
    "    return batches\n",
    "\n",
    "def process_batch_with_graph(batch_data: List[Dict], task_type: str, eval_type: str, \n",
    "                            domain_type: str = \"law\", n_datasets: int = 2, max_step: int = 1) -> Tuple[List[Dict], List[str]]:\n",
    "    \"\"\"λ°°μΉ λ°μ΄ν„°λ¥Ό Graph PipelineμΌλ΅ μ²λ¦¬ν•©λ‹λ‹¤.\"\"\"\n",
    "    \n",
    "    # Graph Pipeline μ„¤μ •\n",
    "    inputs = {\n",
    "        \"save_dir\": CHECKPOINT_DIR,  # μ„μ‹ μ €μ¥μ©\n",
    "        \"save_file_name\": f\"batch_{task_type}_{eval_type}_{int(time.time())}\",\n",
    "        \"task_type\": task_type,\n",
    "        \"eval_type\": eval_type,\n",
    "        \"domain_type\": domain_type,\n",
    "        \"n_datasets\": n_datasets,\n",
    "        \"max_step\": max_step,\n",
    "        \"n_workers\": 3,  # λ΅μ»¬ LLMμ΄λ―€λ΅ λ‚®κ² μ„¤μ •\n",
    "        \"oai_model\": \"exaone-4.0-32b\",\n",
    "        \"error_tolerance_ratio\": 0.5,  # λ†’μ€ μ¤λ¥ ν—μ©λ„\n",
    "        \"show_log_error\": True,\n",
    "        \"data\": batch_data\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Graph μ‹¤ν–‰\n",
    "        if eval_type == \"mcqa\":\n",
    "            graph_instance = mcqa_graph()\n",
    "        else:  # qa\n",
    "            graph_instance = qa_graph()\n",
    "            \n",
    "        result = graph_instance.invoke(\n",
    "            inputs=inputs, \n",
    "            config={\"recursion_limit\": 30}\n",
    "        )\n",
    "        \n",
    "        # κ²°κ³Ό νμΌ μ½κΈ°\n",
    "        result_file = os.path.join(CHECKPOINT_DIR, f\"{inputs['save_file_name']}_{task_type}_{eval_type}_{domain_type}_step_{max_step}.jsonl\")\n",
    "        \n",
    "        if os.path.exists(result_file):\n",
    "            results = read_jsonl(result_file)\n",
    "            # μ„μ‹ νμΌ μ‚­μ \n",
    "            os.remove(result_file)\n",
    "            return results, []\n",
    "        else:\n",
    "            print(f\"β οΈ κ²°κ³Ό νμΌμ„ μ°Ύμ„ μ μ—†μµλ‹λ‹¤: {result_file}\")\n",
    "            return [], [item['index'] for item in batch_data]\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"λ°°μΉ μ²λ¦¬ μ¤λ¥: {e}\")\n",
    "        return [], [item['index'] for item in batch_data]\n",
    "\n",
    "print(\"β… λ°°μΉ μ²λ¦¬ μ‹μ¤ν… μ¤€λΉ„ μ™„λ£!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c8483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. λ°μ΄ν„° μ „μ²λ¦¬ λ° λ°°μΉ μƒμ„±\n",
    "\n",
    "# ν’μ§ ν•„ν„°λ§ μ μ©\n",
    "try:\n",
    "    filtered_data = process_law_data(\n",
    "        law_file_path=law_file_path,\n",
    "        title=\"κΈμµλ²•λ¥ \",\n",
    "        if_filter_punctuation=True,\n",
    "        filter_punctuation_ratio=0.7,  # λ²•λ¥  λ¬Έμ„λ” κµ¬λ‘£μ μ΄ λ§μΌλ―€λ΅ λΉ„μ¨ λ†’μ„\n",
    "        if_filter_english=True,\n",
    "        filter_english_ratio=0.5,\n",
    "        if_filter_number=True,\n",
    "        filter_number_ratio=0.6,  # λ²•λ¥  λ¬Έμ„λ” μ«μ(μ΅°ν•­ λ²νΈ λ“±)κ°€ λ§μΌλ―€λ΅ λΉ„μ¨ λ†’μ„\n",
    "        if_remove_unicode=True,\n",
    "        if_normalize=True,\n",
    "        token_threshold=100,  # λ²•λ¥  μ΅°ν•­μ€ μ§§μ„ μ μμΌλ―€λ΅ μ„κ³„κ°’ λ‚®μ¶¤\n",
    "        return_type=\"split\",\n",
    "        chunk_size=10000,\n",
    "    )\n",
    "    \n",
    "    print(f\"β… ν’μ§ ν•„ν„°λ§ μ™„λ£: {len(filtered_data)}κ° ν•­λ©\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"β λ°μ΄ν„° ν•„ν„°λ§ μ‹¤ν¨: {e}\")\n",
    "    filtered_data = law_data  # ν•„ν„°λ§ μ‹¤ν¨μ‹ μ›λ³Έ λ°μ΄ν„° μ‚¬μ©\n",
    "\n",
    "# ν…μ¤νΈλ¥Ό μ„ν•΄ μΌλ¶€ λ°μ΄ν„°λ§ μ‚¬μ© (μ „μ²΄ λ°μ΄ν„° μ‚¬μ©μ‹ μ£Όμ„ μ²λ¦¬)\n",
    "# sample_size = 20  # ν…μ¤νΈμ© μƒν” ν¬κΈ°\n",
    "sample_size = None  # μ „μ²΄ λ°μ΄ν„° μ‚¬μ©\n",
    "\n",
    "if sample_size and len(filtered_data) > sample_size:\n",
    "    sample_data = filtered_data[:sample_size]\n",
    "    print(f\"π”¬ ν…μ¤νΈλ¥Ό μ„ν•΄ {sample_size}κ° μƒν” μ‚¬μ©\")\n",
    "else:\n",
    "    sample_data = filtered_data\n",
    "    print(f\"π“ μ „μ²΄ {len(sample_data)}κ° λ°μ΄ν„° μ‚¬μ©\")\n",
    "\n",
    "# μΈλ±μ¤ μƒμ„±\n",
    "indexed_data = make_index(sample_data, prefix=\"κΈμµλ²•λ¥ \")\n",
    "print(f\"β… μΈλ±μ¤ μƒμ„± μ™„λ£: {len(indexed_data)}κ° ν•­λ©\")\n",
    "\n",
    "# λ°°μΉ λ‹¨μ„λ΅ λ¶„ν• \n",
    "data_batches = create_batches(indexed_data, BATCH_SIZE)\n",
    "print(f\"π“¦ {len(data_batches)}κ° λ°°μΉ μƒμ„± (λ°°μΉ ν¬κΈ°: {BATCH_SIZE})\")\n",
    "\n",
    "# μƒν” ν™•μΈ\n",
    "if len(indexed_data) > 0:\n",
    "    print(f\"\\nπ“‹ μ²« λ²μ§Έ ν•­λ©:\")\n",
    "    print(f\"μΈλ±μ¤: {indexed_data[0]['index']}\")\n",
    "    print(f\"μ λ©: {indexed_data[0]['title']}\")\n",
    "    print(f\"λ‚΄μ© λ―Έλ¦¬λ³΄κΈ°: {indexed_data[0]['contents'][:150]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90aa921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. μ²΄ν¬ν¬μΈνΈ κΈ°λ° λ°°μΉ μ²λ¦¬ ν•¨μ\n",
    "\n",
    "def process_dataset_with_checkpoints(data_batches: List[List[Dict]], task_name: str, \n",
    "                                   task_type: str, eval_type: str) -> List[Dict]:\n",
    "    \"\"\"μ²΄ν¬ν¬μΈνΈλ¥Ό μ‚¬μ©ν•μ—¬ λ°μ΄ν„°μ…‹μ„ μ•μ „ν•κ² μ²λ¦¬ν•©λ‹λ‹¤.\"\"\"\n",
    "    \n",
    "    checkpoint_file = os.path.join(CHECKPOINT_DIR, f\"{task_name}_checkpoint.json\")\n",
    "    \n",
    "    # μ²΄ν¬ν¬μΈνΈ λ΅λ“\n",
    "    checkpoint = load_checkpoint(checkpoint_file)\n",
    "    completed_batches = set(checkpoint.get(\"completed_batches\", []))\n",
    "    all_results = checkpoint.get(\"results\", [])\n",
    "    failed_items = checkpoint.get(\"failed_items\", [])\n",
    "    \n",
    "    start_batch = len(completed_batches)\n",
    "    total_batches = len(data_batches)\n",
    "    \n",
    "    print(f\"π€ {task_name} μ²λ¦¬ μ‹μ‘: {start_batch}/{total_batches} λ°°μΉλ¶€ν„°\")\n",
    "    if start_batch > 0:\n",
    "        print(f\"π“‚ μ²΄ν¬ν¬μΈνΈμ—μ„ μ¬μ‹μ‘: {len(all_results)}κ° κ²°κ³Ό λ΅λ“λ¨\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx in tqdm(range(start_batch, total_batches), desc=f\"π”„ {task_name} λ°°μΉ μ²λ¦¬\"):\n",
    "        if batch_idx in completed_batches:\n",
    "            continue\n",
    "            \n",
    "        batch_data = data_batches[batch_idx]\n",
    "        print(f\"\\nπ“¦ λ°°μΉ {batch_idx + 1}/{total_batches} μ²λ¦¬ μ¤‘... ({len(batch_data)}κ° ν•­λ©)\")\n",
    "        \n",
    "        # λ°°μΉ μ²λ¦¬\n",
    "        batch_results, batch_failed = process_batch_with_graph(\n",
    "            batch_data, task_type, eval_type\n",
    "        )\n",
    "        \n",
    "        # κ²°κ³Ό μ¶”κ°€\n",
    "        all_results.extend(batch_results)\n",
    "        failed_items.extend(batch_failed)\n",
    "        completed_batches.add(batch_idx)\n",
    "        \n",
    "        print(f\"β… λ°°μΉ {batch_idx + 1} μ™„λ£: {len(batch_results)}κ° μ„±κ³µ, {len(batch_failed)}κ° μ‹¤ν¨\")\n",
    "        \n",
    "        # μ²΄ν¬ν¬μΈνΈ μ €μ¥\n",
    "        checkpoint_data = {\n",
    "            \"completed_batches\": list(completed_batches),\n",
    "            \"results\": all_results,\n",
    "            \"failed_items\": failed_items,\n",
    "            \"progress\": f\"{len(completed_batches)}/{total_batches}\",\n",
    "            \"success_count\": len(all_results),\n",
    "            \"failed_count\": len(failed_items)\n",
    "        }\n",
    "        \n",
    "        if save_checkpoint(checkpoint_file, checkpoint_data):\n",
    "            print(f\"π’Ύ μ²΄ν¬ν¬μΈνΈ μ €μ¥λ¨: {len(all_results)}κ° κ²°κ³Ό\")\n",
    "        \n",
    "        # μ§„ν–‰λ¥  λ° μμƒ μ™„λ£μ‹κ°„ ν‘μ‹\n",
    "        elapsed = time.time() - start_time\n",
    "        if elapsed > 0:\n",
    "            speed = (len(completed_batches) - start_batch) / elapsed\n",
    "            remaining_batches = total_batches - len(completed_batches)\n",
    "            eta_seconds = remaining_batches / speed if speed > 0 else 0\n",
    "            eta_minutes = eta_seconds / 60\n",
    "            \n",
    "            print(f\"β΅ μ§„ν–‰λ¥ : {len(completed_batches)}/{total_batches} ({len(completed_batches)/total_batches*100:.1f}%)\")\n",
    "            print(f\"π•’ μ†λ„: {speed:.2f} λ°°μΉ/μ΄, μμƒ μ™„λ£: {eta_minutes:.1f}λ¶„ ν›„\")\n",
    "        \n",
    "        # λ©”λ¨λ¦¬ μ •λ¦¬λ¥Ό μ„ν• μ μ‹ λ€κΈ°\n",
    "        time.sleep(2)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nπ‰ {task_name} μ™„λ£!\")\n",
    "    print(f\"π“ μ΄ κ²°κ³Ό: {len(all_results)}κ° μ„±κ³µ, {len(failed_items)}κ° μ‹¤ν¨\")\n",
    "    print(f\"β±οΈ μ†μ”μ‹κ°„: {total_time/60:.1f}λ¶„\")\n",
    "    \n",
    "    # μ™„λ£ ν›„ μ²΄ν¬ν¬μΈνΈ νμΌ μ‚­μ \n",
    "    if os.path.exists(checkpoint_file):\n",
    "        os.remove(checkpoint_file)\n",
    "        print(\"π—‘οΈ μ²΄ν¬ν¬μΈνΈ νμΌ μ •λ¦¬ μ™„λ£\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "print(\"β… μ²΄ν¬ν¬μΈνΈ κΈ°λ° λ°°μΉ μ²λ¦¬ ν•¨μ μ¤€λΉ„ μ™„λ£!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a8ebbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. MCQA λ°μ΄ν„°μ…‹ λ°°μΉ μƒμ„±\n",
    "\n",
    "print(\"π€ MCQA λ°μ΄ν„°μ…‹ λ°°μΉ μƒμ„± μ‹μ‘...\")\n",
    "\n",
    "try:\n",
    "    mcqa_results = process_dataset_with_checkpoints(\n",
    "        data_batches=data_batches,\n",
    "        task_name=\"mcqa_generation\",\n",
    "        task_type=\"knowledge\",\n",
    "        eval_type=\"mcqa\"\n",
    "    )\n",
    "    \n",
    "    print(f\"β… MCQA μƒμ„± μ™„λ£: {len(mcqa_results)}κ° λ¬Έν•­\")\n",
    "    \n",
    "    # κ²°κ³Όλ¥Ό μ„μ‹ μ €μ¥\n",
    "    mcqa_temp_file = os.path.join(CHECKPOINT_DIR, \"mcqa_results_temp.jsonl\")\n",
    "    write_jsonl(mcqa_temp_file, mcqa_results)\n",
    "    print(f\"π’Ύ MCQA μ„μ‹ μ €μ¥: {mcqa_temp_file}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"β MCQA μƒμ„± μ‹¤ν¨: {e}\")\n",
    "    print(\"λ΅μ»¬ LLM λ¨λΈ κ²½λ΅μ™€ μ„¤μ •μ„ ν™•μΈν•μ„Έμ”.\")\n",
    "    mcqa_results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da45592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. QA λ°μ΄ν„°μ…‹ λ°°μΉ μƒμ„±\n",
    "\n",
    "print(\"π€ QA λ°μ΄ν„°μ…‹ λ°°μΉ μƒμ„± μ‹μ‘...\")\n",
    "\n",
    "try:\n",
    "    qa_results = process_dataset_with_checkpoints(\n",
    "        data_batches=data_batches,\n",
    "        task_name=\"qa_generation\",\n",
    "        task_type=\"knowledge\",\n",
    "        eval_type=\"qa\"\n",
    "    )\n",
    "    \n",
    "    print(f\"β… QA μƒμ„± μ™„λ£: {len(qa_results)}κ° λ¬Έν•­\")\n",
    "    \n",
    "    # κ²°κ³Όλ¥Ό μ„μ‹ μ €μ¥\n",
    "    qa_temp_file = os.path.join(CHECKPOINT_DIR, \"qa_results_temp.jsonl\")\n",
    "    write_jsonl(qa_temp_file, qa_results)\n",
    "    print(f\"π’Ύ QA μ„μ‹ μ €μ¥: {qa_temp_file}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"β QA μƒμ„± μ‹¤ν¨: {e}\")\n",
    "    print(\"λ΅μ»¬ LLM λ¨λΈ κ²½λ΅μ™€ μ„¤μ •μ„ ν™•μΈν•μ„Έμ”.\")\n",
    "    qa_results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a62dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. μƒμ„±λ λ°μ΄ν„°μ…‹ ν™•μΈ λ° CSV λ³€ν™\n",
    "\n",
    "print(\"π“ μƒμ„±λ λ°μ΄ν„°μ…‹ ν™•μΈ λ° CSV λ³€ν™...\")\n",
    "\n",
    "def convert_options_to_numbers(options_list):\n",
    "    \"\"\"μ„ νƒμ§€λ¥Ό μ«μ ν•νƒλ΅ λ³€ν™ (data_download.ipynb ν•μ‹μ— λ§μ¶¤)\"\"\"\n",
    "    converted = []\n",
    "    for i, option in enumerate(options_list):\n",
    "        converted.append(f\"{i+1}. {option}\")\n",
    "    return converted\n",
    "\n",
    "# MCQA κ²°κ³Ό μ²λ¦¬\n",
    "mcqa_df = None\n",
    "if mcqa_results:\n",
    "    try:\n",
    "        mcqa_df = pd.DataFrame(mcqa_results)\n",
    "        print(f\"β… MCQA λ°μ΄ν„°ν”„λ μ„ μƒμ„±: {len(mcqa_df)}κ° λ¬Έν•­\")\n",
    "        \n",
    "        # μƒν” ν™•μΈ\n",
    "        if len(mcqa_df) > 0:\n",
    "            print(\"\\nπ” MCQA μƒν”:\")\n",
    "            show_sample(mcqa_df, n=1)\n",
    "            \n",
    "            # CSV λ³€ν™ (data_download.ipynb ν•μ‹)\n",
    "            mcqa_csv_path = \"../data/law/mcqa.csv\"\n",
    "            \n",
    "            if all(col in mcqa_df.columns for col in ['question', 'options', 'answer']):\n",
    "                mcqa_clean = mcqa_df.copy()\n",
    "                \n",
    "                # optionsλ¥Ό μ«μ ν•νƒλ΅ λ³€ν™\n",
    "                mcqa_clean['options'] = mcqa_clean['options'].apply(\n",
    "                    lambda x: convert_options_to_numbers(x) if isinstance(x, list) else []\n",
    "                )\n",
    "                \n",
    "                # Question μ»¬λΌ: λ¬Έμ  + μ„ νƒμ§€\n",
    "                mcqa_clean[\"Question\"] = mcqa_clean[\"question\"] + \"\\n\" + mcqa_clean[\"options\"].apply(\n",
    "                    lambda x: \"\\n\".join(x) if x else \"\"\n",
    "                )\n",
    "                \n",
    "                # Answer μ»¬λΌ: \"λ‹µλ³€: μ«μ\" ν•νƒ\n",
    "                answer_mapping = {\"A\": \"1\", \"B\": \"2\", \"C\": \"3\", \"D\": \"4\", \"E\": \"5\", \"F\": \"6\", \"G\": \"7\"}\n",
    "                mcqa_clean[\"Answer\"] = \"λ‹µλ³€: \" + mcqa_clean[\"answer\"].map(answer_mapping).fillna(mcqa_clean[\"answer\"])\n",
    "                \n",
    "                # Question, Answer μ»¬λΌλ§ μ„ νƒ\n",
    "                final_mcqa = mcqa_clean[[\"Question\", \"Answer\"]]\n",
    "                \n",
    "                # CSV μ €μ¥\n",
    "                final_mcqa.to_csv(mcqa_csv_path, index=False, encoding='utf-8-sig')\n",
    "                print(f\"π’Ύ MCQA CSV μ €μ¥ μ™„λ£: {mcqa_csv_path}\")\n",
    "                print(f\"π“ MCQA μµμΆ… ν•μ‹: Question, Answer μ»¬λΌ ({len(final_mcqa)}κ° λ¬Έν•­)\")\n",
    "            else:\n",
    "                print(\"β οΈ MCQA λ°μ΄ν„°μ— ν•„μ”ν• μ»¬λΌμ΄ μ—†μµλ‹λ‹¤.\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"β MCQA λ°μ΄ν„° μ²λ¦¬ μ‹¤ν¨: {e}\")\n",
    "else:\n",
    "    print(\"β MCQA κ²°κ³Όκ°€ μ—†μµλ‹λ‹¤.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# QA κ²°κ³Ό μ²λ¦¬\n",
    "qa_df = None\n",
    "if qa_results:\n",
    "    try:\n",
    "        qa_df = pd.DataFrame(qa_results)\n",
    "        print(f\"β… QA λ°μ΄ν„°ν”„λ μ„ μƒμ„±: {len(qa_df)}κ° λ¬Έν•­\")\n",
    "        \n",
    "        # μƒν” ν™•μΈ\n",
    "        if len(qa_df) > 0:\n",
    "            print(\"\\nπ” QA μƒν”:\")\n",
    "            for i in range(min(2, len(qa_df))):\n",
    "                print(f\"\\n--- QA {i+1} ---\")\n",
    "                print(f\"μ§λ¬Έ: {qa_df.iloc[i]['question']}\")\n",
    "                print(f\"λ‹µλ³€: {qa_df.iloc[i]['answer'][:200]}...\")\n",
    "                \n",
    "            # CSV λ³€ν™ (data_download.ipynb ν•μ‹)\n",
    "            qa_csv_path = \"../data/law/qa.csv\"\n",
    "            \n",
    "            if all(col in qa_df.columns for col in ['question', 'answer']):\n",
    "                qa_clean = qa_df.copy()\n",
    "                \n",
    "                # Question μ»¬λΌ: μ§λ¬Έ κ·Έλ€λ΅\n",
    "                qa_clean[\"Question\"] = qa_clean[\"question\"]\n",
    "                \n",
    "                # Answer μ»¬λΌ: \"λ‹µλ³€: \" + λ‹µλ³€\n",
    "                qa_clean[\"Answer\"] = \"λ‹µλ³€: \" + qa_clean[\"answer\"].astype(str)\n",
    "                \n",
    "                # Question, Answer μ»¬λΌλ§ μ„ νƒ\n",
    "                final_qa = qa_clean[[\"Question\", \"Answer\"]]\n",
    "                \n",
    "                # CSV μ €μ¥\n",
    "                final_qa.to_csv(qa_csv_path, index=False, encoding='utf-8-sig')\n",
    "                print(f\"π’Ύ QA CSV μ €μ¥ μ™„λ£: {qa_csv_path}\")\n",
    "                print(f\"π“ QA μµμΆ… ν•μ‹: Question, Answer μ»¬λΌ ({len(final_qa)}κ° λ¬Έν•­)\")\n",
    "            else:\n",
    "                print(\"β οΈ QA λ°μ΄ν„°μ— ν•„μ”ν• μ»¬λΌμ΄ μ—†μµλ‹λ‹¤.\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"β QA λ°μ΄ν„° μ²λ¦¬ μ‹¤ν¨: {e}\")\n",
    "else:\n",
    "    print(\"β QA κ²°κ³Όκ°€ μ—†μµλ‹λ‹¤.\")\n",
    "\n",
    "print(\"\\nβ… λ°μ΄ν„°μ…‹ μƒμ„± λ° CSV λ³€ν™ μ™„λ£!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97814c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. μµμΆ… CSV νμΌ ν™•μΈ λ° μ •λ¦¬\n",
    "\n",
    "print(\"π“‹ μƒμ„±λ CSV νμΌ ν™•μΈ...\")\n",
    "\n",
    "# CSV νμΌ κ²½λ΅\n",
    "mcqa_csv_path = \"../data/law/mcqa.csv\"\n",
    "qa_csv_path = \"../data/law/qa.csv\"\n",
    "\n",
    "# MCQA CSV ν™•μΈ\n",
    "if os.path.exists(mcqa_csv_path):\n",
    "    print(f\"β… MCQA CSV νμΌ μƒμ„±λ¨: {mcqa_csv_path}\")\n",
    "    mcqa_csv_df = pd.read_csv(mcqa_csv_path)\n",
    "    print(f\"π“ MCQA λ°μ΄ν„° ν¬κΈ°: {len(mcqa_csv_df)}κ° λ¬Έν•­\")\n",
    "    \n",
    "    # μ»¬λΌ μ •λ³΄ μ¶λ ¥\n",
    "    print(f\"π“‹ MCQA μ»¬λΌ: {list(mcqa_csv_df.columns)}\")\n",
    "    \n",
    "    # μ²« λ²μ§Έ ν–‰ λ―Έλ¦¬λ³΄κΈ°\n",
    "    if len(mcqa_csv_df) > 0:\n",
    "        print(\"\\nπ” MCQA CSV μ²« λ²μ§Έ λ¬Έν•­:\")\n",
    "        print(f\"Question: {mcqa_csv_df.iloc[0]['Question'][:200]}...\")\n",
    "        print(f\"Answer: {mcqa_csv_df.iloc[0]['Answer']}\")\n",
    "else:\n",
    "    print(f\"β MCQA CSV νμΌμ΄ μ—†μµλ‹λ‹¤: {mcqa_csv_path}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "\n",
    "# QA CSV ν™•μΈ\n",
    "if os.path.exists(qa_csv_path):\n",
    "    print(f\"β… QA CSV νμΌ μƒμ„±λ¨: {qa_csv_path}\")\n",
    "    qa_csv_df = pd.read_csv(qa_csv_path)\n",
    "    print(f\"π“ QA λ°μ΄ν„° ν¬κΈ°: {len(qa_csv_df)}κ° λ¬Έν•­\")\n",
    "    \n",
    "    # μ»¬λΌ μ •λ³΄ μ¶λ ¥\n",
    "    print(f\"π“‹ QA μ»¬λΌ: {list(qa_csv_df.columns)}\")\n",
    "    \n",
    "    # μ²« λ²μ§Έ ν–‰ λ―Έλ¦¬λ³΄κΈ°\n",
    "    if len(qa_csv_df) > 0:\n",
    "        print(\"\\nπ” QA CSV μ²« λ²μ§Έ λ¬Έν•­:\")\n",
    "        print(f\"Question: {qa_csv_df.iloc[0]['Question']}\")\n",
    "        print(f\"Answer: {qa_csv_df.iloc[0]['Answer'][:200]}...\")\n",
    "else:\n",
    "    print(f\"β QA CSV νμΌμ΄ μ—†μµλ‹λ‹¤: {qa_csv_path}\")\n",
    "\n",
    "# μ„μ‹ νμΌ μ •λ¦¬\n",
    "temp_files = [\n",
    "    os.path.join(CHECKPOINT_DIR, \"mcqa_results_temp.jsonl\"),\n",
    "    os.path.join(CHECKPOINT_DIR, \"qa_results_temp.jsonl\")\n",
    "]\n",
    "\n",
    "for temp_file in temp_files:\n",
    "    if os.path.exists(temp_file):\n",
    "        os.remove(temp_file)\n",
    "        print(f\"π—‘οΈ μ„μ‹ νμΌ μ‚­μ : {temp_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"π‰ λ²•λ¥  QA/MCQA λ°μ΄ν„°μ…‹ μƒμ„± μ™„λ£!\")\n",
    "print(f\"π“ μµμΆ… μ¶λ ¥ νμΌ (Question, Answer μ»¬λΌ):\")\n",
    "print(f\"  - MCQA: {mcqa_csv_path}\")\n",
    "print(f\"  - QA: {qa_csv_path}\")\n",
    "print(\"β¨ data_download.ipynb ν•μ‹μ— λ§μ¶ CSV νμΌμ΄ μƒμ„±λμ—μµλ‹λ‹¤!\")\n",
    "print(\"π”„ λ°°μΉ μ²λ¦¬ λ° μ²΄ν¬ν¬μΈνΈ μ‹μ¤ν…μΌλ΅ μ•μ „ν•κ² μ²λ¦¬λμ—μµλ‹λ‹¤.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
