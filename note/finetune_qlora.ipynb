{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c499066d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q pandas tqdm \n",
    "!pip install -q transformers==4.55.0 # llm requires >=4.46.0\n",
    "!pip install -q safetensors==0.4.3 # downgrade for torch 2.1.0\n",
    "!pip install -q bitsandbytes==0.43.2 accelerate==1.9.0 # quantization\n",
    "!pip install -q peft==0.17.0 trl==0.21.0 # finetune\n",
    "!pip install -q datasets wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d61fb2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pandas as pd, json, random\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from transformers import (AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig)\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import torch\n",
    "import re\n",
    "import wandb\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17cc985e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "!wandb login 80f8e961aab3ee77e8dc19ddc1bfd4604203e400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0662b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 객관식 여부 판단 함수\n",
    "# def is_multiple_choice(question_text):\n",
    "#     \"\"\"\n",
    "#     객관식 여부를 판단: 2개 이상의 숫자 선택지가 줄 단위로 존재할 경우 객관식으로 간주\n",
    "#     \"\"\"\n",
    "#     question_text = question_text.replace(\"\\\\n\", \"\\n\")\n",
    "#     lines = question_text.strip().split(\"\\n\")\n",
    "#     option_count = sum(bool(re.match(r\"^\\s*[1-9][0-9]?\\s\", line)) for line in lines)\n",
    "#     return option_count >= 2\n",
    "\n",
    "def is_multiple_choice(question_text):\n",
    "    question_text = question_text.replace(\"\\\\n\", \"\\n\")\n",
    "    lines = question_text.strip().split(\"\\n\")\n",
    "    pat = re.compile(r\"^\\s*[\\(\\[]?\\d{1}[\\)\\]\\.]?\\s+\")\n",
    "    option_count = sum(bool(pat.match(line)) for line in lines)\n",
    "    return option_count >= 2\n",
    "\n",
    "\n",
    "# # 질문과 선택지 분리 함수\n",
    "# def extract_question_and_choices(full_text):\n",
    "#     \"\"\"\n",
    "#     전체 질문 문자열에서 질문 본문과 선택지 리스트를 분리\n",
    "#     \"\"\"\n",
    "#     lines = full_text.strip().split(\"\\n\")\n",
    "#     q_lines = []\n",
    "#     options = []\n",
    "\n",
    "#     for line in lines:\n",
    "#         if re.match(r\"^\\s*[1-9][0-9]?\\s\", line):\n",
    "#             options.append(line.strip())\n",
    "#         else:\n",
    "#             q_lines.append(line.strip())\n",
    "    \n",
    "#     question = \" \".join(q_lines)\n",
    "#     return question, options\n",
    "\n",
    "# 숫자/원형숫자 옵션 라인 매칭\n",
    "OPTION_LINE = re.compile(r\"^\\s*(?:[\\(\\[]?\\d{1,2}[\\)\\]\\.]?|[\\u2460-\\u2473])\\s+\")\n",
    "\n",
    "def extract_question_and_choices(full_text: str):\n",
    "    \"\"\"\n",
    "    전체 질문 문자열에서 질문 본문과 선택지 리스트를 분리\n",
    "    - \"\\\\n\" 같은 리터럴 줄바꿈도 처리\n",
    "    - 옵션이 여러 줄로 이어지면 뒤줄을 같은 선택지에 이어 붙임\n",
    "    - 옵션 앞 번호/기호는 유지\n",
    "    \"\"\"\n",
    "    # 리터럴 줄바꿈과 다양한 개행 정규화\n",
    "    text = (full_text\n",
    "            .replace(\"\\\\r\\\\n\", \"\\n\")\n",
    "            .replace(\"\\\\n\", \"\\n\")\n",
    "            .replace(\"\\r\\n\", \"\\n\")\n",
    "            .replace(\"\\r\", \"\\n\"))\n",
    "\n",
    "    lines = [ln.strip() for ln in text.strip().split(\"\\n\") if ln.strip()]\n",
    "\n",
    "    q_lines = []\n",
    "    options = []\n",
    "    in_options = False\n",
    "\n",
    "    for ln in lines:\n",
    "        if OPTION_LINE.match(ln):\n",
    "            in_options = True\n",
    "            options.append(ln.strip())  # 번호 포함 그대로\n",
    "        else:\n",
    "            if in_options and options:\n",
    "                # 옵션 줄 다음 줄이 붙는 경우\n",
    "                options[-1] += \" \" + ln\n",
    "            else:\n",
    "                q_lines.append(ln)\n",
    "\n",
    "    question = \" \".join(q_lines).strip()\n",
    "    return question, options\n",
    "\n",
    "\n",
    "# 프롬프트 생성기\n",
    "def make_prompt_auto(row):\n",
    "    Question = str(row[\"Question\"]).strip()\n",
    "    Answer = str(row[\"Answer\"]).split(\"답변:\")[-1].strip()\n",
    "    if is_multiple_choice(Question):\n",
    "        question, options = extract_question_and_choices(Question)\n",
    "        prompt = (\n",
    "                \"당신은 금융보안 전문가입니다.\\n\"\n",
    "                # \"아래 질문에 대해 적절한 **정답 선택지 번호만 출력**하세요.\\n\\n\"\n",
    "                \"아래 질문에 대해 적절한 선택지를 출력하세요.\\n\\n\"\n",
    "                f\"질문: {question}\\n\"\n",
    "                \"선택지:\\n\"\n",
    "                f\"{chr(10).join(options)}\\n\\n\"\n",
    "                \"답변:\"\n",
    "                )\n",
    "    else:\n",
    "        prompt = (\n",
    "                \"당신은 금융보안 전문가입니다.\\n\"\n",
    "                \"아래 주관식 질문에 대해 정확하고 간략한 설명을 작성하세요.\\n\\n\"\n",
    "                f\"질문: {Question}\\n\\n\"\n",
    "                \"답변:\"\n",
    "                )\n",
    "    response = Answer\n",
    "\n",
    "    return prompt, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fc21556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "\n",
    "dfs = []\n",
    "dfs.append(pd.read_csv(\"../data/CyberMetric/mcqa_enhanced.csv\"))\n",
    "dfs.append(pd.read_csv(\"../data/FinShibainu/mcqa_enhanced.csv\"))\n",
    "dfs.append(pd.read_csv(\"../data/FinShibainu/qa.csv\"))\n",
    "dfs.append(pd.read_csv(\"../data/SecBench/mcqa_enhanced.csv\"))\n",
    "dfs.append(pd.read_csv(\"../data/SecBench/qa.csv\"))\n",
    "\n",
    "full = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27f85b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>다음 중 정보의 비밀을 나타내는 것은 무엇입니까?\\n1. 가용성\\n2. 인증\\n3....</td>\n",
       "      <td>답변: 4, 기밀성</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>어떤 유형의 인증은 당신이 아는 것, 당신이 가진 것, 당신이있는 것과 같은 여러 ...</td>\n",
       "      <td>답변: 3, 다중 인증 인증</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>발가락은 무엇을 의미합니까?\\n1. 평가 목표\\n2. 평가 시간\\n3. 평가 유형\\...</td>\n",
       "      <td>답변: 1, 평가 목표</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>시스템이 리소스에 대한 액세스를 요청하는 사용자가 실제로 자신이 주장하는 사람인지 ...</td>\n",
       "      <td>답변: 4, 인증</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>기밀성, 무결성 및 데이터 및 자산의 가용성에 대한 확인 및 보증을 포함하여 정보 ...</td>\n",
       "      <td>답변: 2, 정보 보증</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100508</th>\n",
       "      <td>블록 체인 기술에서 거래 검증 가능성을 유지하면서 거래 개인 정보를 향상시키기 위해...</td>\n",
       "      <td>답변: 거래 개인 정보 보호를 향상시키기 위해 제로 지식 증거를 사용하는 방법은 당...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100509</th>\n",
       "      <td>Linux 시스템에서 실제 권한 에스컬레이션 취약성 (예 : CVE-2019-186...</td>\n",
       "      <td>답변: CVE-2019-18634는 Linux 시스템에 영향을 미치는 권한 에스컬레...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100510</th>\n",
       "      <td>PHP에서 문자열을 정의하기 위해 이중 인용문과 단일 따옴표 사용의 차이점은 무엇입니까?</td>\n",
       "      <td>답변: PHP에서 이중 인용문으로 정의 된 문자열은 변수를 구문 분석하는 반면 단일...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100511</th>\n",
       "      <td>PHP 코드를 식별하기 위해 PHP에 구문 분석 태그를 작성하는 방법은 무엇입니까?</td>\n",
       "      <td>답변: PHP에서 구문 분석 태그를 작성하는 세 가지 방법, 즉 '&lt;? php?&gt;'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100512</th>\n",
       "      <td>다중 사용자 Linux 시스템에서 허가 관리 (예 : 사용자 및 그룹 권한)를 통해...</td>\n",
       "      <td>답변: 다중 사용자 Linux 시스템에서 중요한 파일 시스템 디렉토리에 대한 무단 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100513 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Question  \\\n",
       "0       다음 중 정보의 비밀을 나타내는 것은 무엇입니까?\\n1. 가용성\\n2. 인증\\n3....   \n",
       "1       어떤 유형의 인증은 당신이 아는 것, 당신이 가진 것, 당신이있는 것과 같은 여러 ...   \n",
       "2       발가락은 무엇을 의미합니까?\\n1. 평가 목표\\n2. 평가 시간\\n3. 평가 유형\\...   \n",
       "3       시스템이 리소스에 대한 액세스를 요청하는 사용자가 실제로 자신이 주장하는 사람인지 ...   \n",
       "4       기밀성, 무결성 및 데이터 및 자산의 가용성에 대한 확인 및 보증을 포함하여 정보 ...   \n",
       "...                                                   ...   \n",
       "100508  블록 체인 기술에서 거래 검증 가능성을 유지하면서 거래 개인 정보를 향상시키기 위해...   \n",
       "100509  Linux 시스템에서 실제 권한 에스컬레이션 취약성 (예 : CVE-2019-186...   \n",
       "100510  PHP에서 문자열을 정의하기 위해 이중 인용문과 단일 따옴표 사용의 차이점은 무엇입니까?   \n",
       "100511     PHP 코드를 식별하기 위해 PHP에 구문 분석 태그를 작성하는 방법은 무엇입니까?   \n",
       "100512  다중 사용자 Linux 시스템에서 허가 관리 (예 : 사용자 및 그룹 권한)를 통해...   \n",
       "\n",
       "                                                   Answer  \n",
       "0                                              답변: 4, 기밀성  \n",
       "1                                         답변: 3, 다중 인증 인증  \n",
       "2                                            답변: 1, 평가 목표  \n",
       "3                                               답변: 4, 인증  \n",
       "4                                            답변: 2, 정보 보증  \n",
       "...                                                   ...  \n",
       "100508  답변: 거래 개인 정보 보호를 향상시키기 위해 제로 지식 증거를 사용하는 방법은 당...  \n",
       "100509  답변: CVE-2019-18634는 Linux 시스템에 영향을 미치는 권한 에스컬레...  \n",
       "100510  답변: PHP에서 이중 인용문으로 정의 된 문자열은 변수를 구문 분석하는 반면 단일...  \n",
       "100511  답변: PHP에서 구문 분석 태그를 작성하는 세 가지 방법, 즉 '<? php?>'...  \n",
       "100512  답변: 다중 사용자 Linux 시스템에서 중요한 파일 시스템 디렉토리에 대한 무단 ...  \n",
       "\n",
       "[100513 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcb95101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 데이터: 100,513개 → 샘플링: 10,051개 (10%)\n"
     ]
    }
   ],
   "source": [
    "# 튜플을 딕셔너리로 변환\n",
    "tuples = [make_prompt_auto(r) for _, r in full.iterrows()]\n",
    "records = [{\"prompt\": prompt, \"completion\": response} for prompt, response in tuples] # SFTTrainer 에서 completion 필드 사용\n",
    "random.seed(42)\n",
    "random.shuffle(records)\n",
    "\n",
    "# 10% 샘플링 (테스트용)\n",
    "sample_size = int(len(records) * 0.1)\n",
    "sampled_records = records[:sample_size]\n",
    "print(f\"전체 데이터: {len(records):,}개 → 샘플링: {len(sampled_records):,}개 (10%)\")\n",
    "\n",
    "# 간단 split\n",
    "# n = int(len(records)*0.95)\n",
    "n = int(len(sampled_records)*0.95)\n",
    "train_ds = Dataset.from_list(sampled_records[:n])\n",
    "eval_ds  = Dataset.from_list(sampled_records[n:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd113ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'completion'],\n",
      "    num_rows: 9548\n",
      "}) Dataset({\n",
      "    features: ['prompt', 'completion'],\n",
      "    num_rows: 503\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_ds, eval_ds)\n",
    "print(train_ds[0]) # 짧은 MCQA\n",
    "print(train_ds[1]) # 긴 QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11558692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d5708660a7043288c2b08890bfaf7d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 모델 선택\n",
    "models = [\n",
    "    \"gemma-ko-7b\", # baseline\n",
    "    \"ax-4.0-light-7b\", # skt\n",
    "    # \"polyglot-12.8b\",\n",
    "    # \"koalpaca-polyglot-12.8b\",\n",
    "    \"midm-2.0-11.5b\", # kt\n",
    "    # \"HyperCLOVAX-SEED-Think-14B\", # naver\n",
    "    # \"kanana-1.5-15.7b-a3b-instruct\", # kakao\n",
    "    # \"exaone-4.0-32b\" # lg\n",
    "]\n",
    "selected_model = models[1]\n",
    "model_path = f\"../../models/{selected_model}\" # 로컬 저장 모델 경로\n",
    "\n",
    "# 4bit 설정\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # NaN 방지\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# # 8bit 설정\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_8bit=True,          # 4bit → 8bit\n",
    "#     llm_int8_threshold=6.0,     # 기본값 (필요 시 조정)\n",
    "#     llm_int8_has_fp16_weight=False  # True로 하면 일부 레이어 FP16 유지\n",
    "# )\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    # padding_side=\"left\",\n",
    "    padding_side=\"right\" # 학습 시 right 권장\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 모델 로드\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\", # GPU 자동 배정\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"eager\",\n",
    "    # trust_remote_code=True # naver\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d9698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(102400, 3584)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((3584,), eps=1e-05)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=102400, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model # 모델 확인. target_modules에 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d753be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA Setting\n",
    "\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "# LoRA (Low-Rank Adaptation) 설정\n",
    "peft_config = LoraConfig(\n",
    "    # LoRA 핵심 파라미터\n",
    "    r=16,                       # 랭크(rank): LoRA 행렬의 차원, 낮을수록 파라미터 적음, 높을수록 표현력 증가\n",
    "                                # 일반적으로 8~64 사용, 16은 성능과 효율성의 균형점\n",
    "    \n",
    "    lora_alpha=32,              # 스케일링 파라미터: LoRA 업데이트의 강도 조절\n",
    "                                # 일반적으로 r의 2배 설정 (16*2=32)\n",
    "                                # 높을수록 LoRA의 영향력 증가\n",
    "    \n",
    "    lora_dropout=0.05,          # 드롭아웃 비율: 과적합 방지\n",
    "                                # 0.05 = 5% 드롭아웃, 일반적으로 0.05~0.1 사용\n",
    "    \n",
    "    # 적용할 모듈 지정 (Transformer의 주요 선형 레이어들)\n",
    "    target_modules=[\n",
    "        \"q_proj\",               # Query 프로젝션 레이어\n",
    "        \"k_proj\",               # Key 프로젝션 레이어  \n",
    "        \"v_proj\",               # Value 프로젝션 레이어\n",
    "        \"o_proj\",               # Output 프로젝션 레이어\n",
    "        \"gate_proj\",            # Gate 프로젝션 레이어 (FFN)\n",
    "        \"up_proj\",              # Up 프로젝션 레이어 (FFN)\n",
    "        \"down_proj\"             # Down 프로젝션 레이어 (FFN)\n",
    "    ],\n",
    "    \n",
    "    task_type=\"CAUSAL_LM\"       # 태스크 타입: 인과적 언어모델 (다음 토큰 예측)\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, peft_config)\n",
    "model.train() # 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6064e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 40,370,176 || all params: 7,299,995,136 || trainable%: 0.5530\n"
     ]
    }
   ],
   "source": [
    "# === QLoRA 준비 직후 점검 셀 ===\n",
    "from peft import PeftModel\n",
    "\n",
    "# 1) 훈련 모드 + 입력 그라드 (4bit/8bit에서 필요)\n",
    "model.gradient_checkpointing_enable()  # gradient_checkpointing 옵션 썼다면 확실히 켜주기\n",
    "try:\n",
    "    model.enable_input_require_grads()  # 일부 모델에서 필요 (임베딩 입력 grad 허용)\n",
    "except Exception as e:\n",
    "    print(\"[warn] enable_input_require_grads() skip:\", e)\n",
    "\n",
    "model.train()  # <- 반드시 훈련 모드 전환\n",
    "\n",
    "# 2) LoRA 적용/훈련 파라미터 확인\n",
    "if hasattr(model, \"print_trainable_parameters\"):\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "trainable = [(n, p) for n, p in model.named_parameters() if p.requires_grad]\n",
    "print(f\"[check] # of trainable params: {len(trainable)}\")\n",
    "print(\"  (예시 3개)\", [n for n, _ in trainable[:3]])\n",
    "\n",
    "# LoRA 모듈이 실제로 붙었는지 빠르게 확인\n",
    "has_lora = any(\"lora_\" in n.lower() or \"lora_A\" in n or \"lora_B\" in n for n, _ in trainable)\n",
    "print(f\"[check] lora modules present? {has_lora}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27c6e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 훈련 설정 완료!\n",
      "총 에포크: 5\n",
      "실제 배치 크기: 8 × 16 = 128\n",
      "로깅 주기: 16스텝마다\n"
     ]
    }
   ],
   "source": [
    "# 개선된 훈련 설정\n",
    "tokenizer.model_max_length = 128 # 긴 답변 필요 없음\n",
    "\n",
    "cfg = SFTConfig(\n",
    "    output_dir=f\"../../models/{selected_model}_qlora\",\n",
    "    \n",
    "    # 훈련 스케줄링 - 본격 학습용 설정\n",
    "    num_train_epochs=5,           \n",
    "    per_device_train_batch_size=8, # 실제 학습 시 확대 (GPU VRAM 체크)\n",
    "    gradient_accumulation_steps=8, # optimizer step 주기\n",
    "    \n",
    "    # 학습률 및 최적화\n",
    "    learning_rate=1e-4,           # 안정적인 학습률\n",
    "    weight_decay=0.01,            # 과적합 방지\n",
    "    max_grad_norm=1.0,            # 그래디언트 클리핑\n",
    "    \n",
    "    # 로깅 및 저장 (개선된 주기)\n",
    "    logging_steps=16,             \n",
    "    logging_strategy=\"steps\",\n",
    "    # logging_first_step=True,\n",
    "    save_steps=64,               \n",
    "    # save_total_limit=3,         # 최대 3개 체크포인트 유지\n",
    "    \n",
    "    # 평가 설정\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=64,               \n",
    "    load_best_model_at_end=True,  # 최고 성능 모델 자동 선택\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    \n",
    "    # 하드웨어 최적화\n",
    "    bf16=True,\n",
    "    dataloader_num_workers=4,\n",
    "    dataloader_pin_memory=True,\n",
    "    \n",
    "    # 학습률 스케줄러\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    \n",
    "    # 메모리 최적화\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    # 기타 설정\n",
    "    remove_unused_columns=False,\n",
    "    report_to=[\"wandb\"],            \n",
    "    run_name=f\"{selected_model}_qlora_{datetime.datetime.now().strftime('%m%d_%H%M')}\",\n",
    "    # disable_tqdm=True,            # tqdm이 stdout 삼키는 이슈 회피\n",
    ")\n",
    "\n",
    "print(\"✅ 훈련 설정 완료!\")\n",
    "print(f\"총 에포크: {cfg.num_train_epochs}\")\n",
    "print(f\"실제 배치 크기: {cfg.per_device_train_batch_size} × {cfg.gradient_accumulation_steps} = {cfg.per_device_train_batch_size * cfg.gradient_accumulation_steps}\")\n",
    "print(f\"로깅 주기: {cfg.logging_steps}스텝마다\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea8c9d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d5690f8c47480da70b4d747c43d86a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9548 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 128). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00437bf6074549caba7fc077fc62509c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/503 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 원본 훈련 샘플: 9,548개\n",
      "✅ 최종 훈련 샘플: 9,548개\n",
      "✅ 제거된 샘플: 0개 (모든 샘플 보존!)\n",
      "✅ 최대 토큰 수: 127\n",
      "✅ 평균 토큰 수: 113.5\n"
     ]
    }
   ],
   "source": [
    "def truncate_keep_all_samples(dataset, max_length=None):\n",
    "    \"\"\"모든 샘플을 보존하면서 길이만 조정\"\"\"\n",
    "    if max_length is None:\n",
    "        max_length = tokenizer.model_max_length\n",
    "    \n",
    "    def truncate_sample(sample):\n",
    "        prompt = sample[\"prompt\"]\n",
    "        completion = sample[\"completion\"]\n",
    "        \n",
    "        # 전체 길이 체크\n",
    "        full_text = prompt + completion\n",
    "        full_tokens = tokenizer.encode(full_text, add_special_tokens=True)\n",
    "        \n",
    "        if len(full_tokens) <= max_length:\n",
    "            return sample  # 그대로 유지\n",
    "        \n",
    "        # 프롬프트가 너무 길면 프롬프트도 자르기\n",
    "        prompt_tokens = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "        completion_tokens = tokenizer.encode(completion, add_special_tokens=False)\n",
    "        \n",
    "        total_needed = len(prompt_tokens) + len(completion_tokens) + 2  # BOS/EOS\n",
    "        \n",
    "        if total_needed <= max_length:\n",
    "            return sample  # 실제로는 문제없음\n",
    "        \n",
    "        # 프롬프트:완성 = 7:3 비율로 할당\n",
    "        prompt_quota = int(max_length * 0.7)\n",
    "        completion_quota = max_length - prompt_quota - 2\n",
    "        \n",
    "        # 프롬프트 자르기 (뒤쪽 유지)\n",
    "        if len(prompt_tokens) > prompt_quota:\n",
    "            truncated_prompt_tokens = prompt_tokens[-prompt_quota:]\n",
    "            truncated_prompt = tokenizer.decode(truncated_prompt_tokens, skip_special_tokens=True)\n",
    "        else:\n",
    "            truncated_prompt = prompt\n",
    "            completion_quota = max_length - len(prompt_tokens) - 2\n",
    "        \n",
    "        # 완성 자르기 (앞쪽 유지)\n",
    "        if len(completion_tokens) > completion_quota:\n",
    "            truncated_completion_tokens = completion_tokens[:completion_quota]\n",
    "            truncated_completion = tokenizer.decode(truncated_completion_tokens, skip_special_tokens=True)\n",
    "        else:\n",
    "            truncated_completion = completion\n",
    "        \n",
    "        # # EOS 토큰 보장\n",
    "        # if not truncated_completion.endswith('<|im_end|>'):\n",
    "        #     truncated_completion += '<|im_end|>'\n",
    "        \n",
    "        return {\"prompt\": truncated_prompt, \"completion\": truncated_completion}\n",
    "    \n",
    "    return dataset.map(truncate_sample, num_proc=1)\n",
    "\n",
    "\n",
    "# 모든 샘플 보존하며 자르기\n",
    "final_train_ds = truncate_keep_all_samples(train_ds)\n",
    "final_eval_ds = truncate_keep_all_samples(eval_ds)\n",
    "\n",
    "print(f\"✅ 원본 훈련 샘플: {len(train_ds):,}개\")\n",
    "print(f\"✅ 최종 훈련 샘플: {len(final_train_ds):,}개\")\n",
    "print(f\"✅ 제거된 샘플: 0개 (모든 샘플 보존!)\")\n",
    "\n",
    "# 길이 검증\n",
    "max_lens = []\n",
    "for i in range(min(100, len(final_train_ds))):\n",
    "    sample = final_train_ds[i]\n",
    "    tokens = tokenizer.encode(sample[\"prompt\"] + sample[\"completion\"], add_special_tokens=True)\n",
    "    max_lens.append(len(tokens))\n",
    "\n",
    "print(f\"✅ 최대 토큰 수: {max(max_lens)}\")\n",
    "print(f\"✅ 평균 토큰 수: {sum(max_lens)/len(max_lens):.1f}\")\n",
    "\n",
    "# 변수 업데이트\n",
    "train_ds = final_train_ds\n",
    "eval_ds = final_eval_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b911ac66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a9c39ee98e64380bf1f1cdc6fe143cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/9548 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "328af5b6b61a44f68ef9575d7989a545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/503 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize 오류 해결\n",
    "\n",
    "import unicodedata, re\n",
    "\n",
    "ZWS = \"\\u200b\"          # zero-width space\n",
    "NBSP = \"\\xa0\"           # non-breaking space\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = s.replace(\"\\r\\n\",\"\\n\").replace(\"\\r\",\"\\n\")   # 개행 통일\n",
    "    s = s.replace(NBSP,\" \").replace(ZWS,\"\")         # 이상 공백 제거\n",
    "    s = unicodedata.normalize(\"NFKC\", s)            # 유니코드 정규화\n",
    "    # 문자열에 직접 박아둔 특수토큰은 제거(토크나이저에 맡김)\n",
    "    for tok in (\"<s>\",\"</s>\",\"<bos>\",\"</eos>\",\"<<SYS>>\",\"<<USER>>\",\"<<ASSISTANT>>\", \"<|im_end|>\"):\n",
    "        s = s.replace(tok,\"\")\n",
    "    return s\n",
    "\n",
    "def normalize_pair(ex):\n",
    "    p = clean_text(ex[\"prompt\"]).rstrip()           # 뒤 공백/개행 제거\n",
    "    c = clean_text(ex[\"completion\"]).lstrip()       # 앞 공백 제거\n",
    "    if not p.endswith(\"\\n\"):                        # 경계 개행 1개 강제\n",
    "        p += \"\\n\"\n",
    "    return {\"prompt\": p, \"completion\": c}\n",
    "\n",
    "train_ds = train_ds.map(normalize_pair, num_proc=4)\n",
    "eval_ds  = eval_ds.map(normalize_pair,  num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ff1ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'completion'],\n",
      "    num_rows: 9548\n",
      "}) Dataset({\n",
      "    features: ['prompt', 'completion'],\n",
      "    num_rows: 503\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_ds, eval_ds)\n",
    "print(train_ds[0])\n",
    "print(train_ds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82cb8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 라벨 유효성 점검 셀 ===\n",
    "def _count_supervised_tokens(example):\n",
    "    # labels에 -100이 아닌 토큰이 최소 1개는 있어야 학습 가능\n",
    "    return {\"supervised_tokens\": int(sum(1 for t in example[\"labels\"] if t != -100))}\n",
    "\n",
    "tmp = train_ds.map(_count_supervised_tokens)\n",
    "num_bad = sum(1 for x in tmp[\"supervised_tokens\"] if x == 0)\n",
    "print(f\"[check] supervised tokens == 0 인 샘플 수: {num_bad}\")\n",
    "\n",
    "assert num_bad == 0, \"labels가 전부 -100인 샘플이 있습니다. answer_start 인덱싱/라벨 마스킹을 점검하세요.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecd1677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b96dd88cf1e421c9c0102265e5fc898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/9548 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "894ad12377f8400094537959244632cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/9548 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a877028a96486ba0f3be7566d7bb93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/9548 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2872ccaf03bb4356942d0d83230048ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/503 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9274a7b8b8404f83abcd7f8dae29feea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/503 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea94cadab1d2465d9526da98ae7c01a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/503 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "\n",
    "@dataclass\n",
    "class CompletionOnlyCollator:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    answer_marker: str = \"답변:\\n\"\n",
    "    max_length: int = 128\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        # 1) prompt + completion + EOS 로 합치기\n",
    "        eos = self.tokenizer.eos_token or \"\"\n",
    "        texts = [ex[\"prompt\"] + ex[\"completion\"] + eos for ex in features]\n",
    "\n",
    "        batch = self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=True,\n",
    "        )\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        # 2) \"prompt + answer_marker\" 길이만큼 -100 마스킹\n",
    "        for i, ex in enumerate(features):\n",
    "            prefix = ex[\"prompt\"] + self.answer_marker\n",
    "            prefix_ids = self.tokenizer(\n",
    "                prefix,\n",
    "                add_special_tokens=True,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\",\n",
    "            )[\"input_ids\"][0]\n",
    "\n",
    "            cut = min(len(prefix_ids), labels.size(1))\n",
    "            labels[i, :cut] = -100\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "# ⚠️ 생성은 \"키워드 인자\"로\n",
    "collator = CompletionOnlyCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    answer_marker=\"답변:\\n\",\n",
    "    max_length=tokenizer.model_max_length or 128\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    args=cfg,\n",
    "    data_collator=collator,\n",
    "    # data_collator=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfeb6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Callback\n",
    "\n",
    "from transformers import TrainerCallback\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import math\n",
    "\n",
    "class CustomCallback(TrainerCallback):\n",
    "    tz_seoul = pytz.timezone(\"Asia/Seoul\")\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and \"loss\" in logs:\n",
    "            now = datetime.now(self.tz_seoul).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            print(f\"[{now}] step {state.global_step}\\tloss {logs['loss']:.4f}\", flush=True)\n",
    "\n",
    "class ManualGradNormCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs.get(\"model\")\n",
    "        if model is None: return\n",
    "\n",
    "        total_sq = 0.0\n",
    "        for _, p in model.named_parameters():\n",
    "            if p.grad is not None:\n",
    "                g = p.grad.detach()\n",
    "                total_sq += float(g.norm(2)**2)\n",
    "        manual = math.sqrt(total_sq)\n",
    "\n",
    "        try:\n",
    "            import wandb\n",
    "            wandb.log({\"train/manual_grad_norm\": manual}, step=state.global_step)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "trainer.add_callback(CustomCallback())\n",
    "trainer.add_callback(ManualGradNormCallback())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c1fa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 한 배치 grad 존재 확인 셀 ===\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "dl = DataLoader(train_ds, batch_size=1, shuffle=True, collate_fn=default_data_collator)\n",
    "batch = next(iter(dl))\n",
    "batch = {k: torch.tensor(v).to(model.device) for k, v in batch.items()}  # 안전하게 텐서화 + 디바이스 이동\n",
    "\n",
    "model.zero_grad(set_to_none=True)\n",
    "out = model(**batch)\n",
    "loss = out.loss\n",
    "print(\"[check] single-batch loss:\", float(loss))\n",
    "loss.backward()\n",
    "\n",
    "first = None\n",
    "for n, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        first = (n, p)\n",
    "        break\n",
    "\n",
    "if first is None:\n",
    "    raise RuntimeError(\"훈련 가능한 파라미터가 없습니다. LoRA 설정을 확인하세요.\")\n",
    "\n",
    "n, p = first\n",
    "print(f\"[grad-check] first trainable: {n}\")\n",
    "print(\"[grad-check] grad is None? ->\", p.grad is None)\n",
    "if p.grad is not None:\n",
    "    print(\"[grad-check] grad norm ->\", float(p.grad.norm()))\n",
    "\n",
    "# 수동 전체 grad norm도 계산\n",
    "total_norm_sq = 0.0\n",
    "for _, p in model.named_parameters():\n",
    "    if p.grad is not None:\n",
    "        g = p.grad.detach()\n",
    "        total_norm_sq += float(g.norm(2)**2)\n",
    "manual_grad_norm = total_norm_sq ** 0.5\n",
    "print(\"[grad-check] manual total grad_norm =\", manual_grad_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5596d587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 훈련 시작...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhc5754\u001b[0m (\u001b[33mbhc5754-hyalobio\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/fske/note/wandb/run-20250810_080040-tu5651gd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhc5754-hyalobio/huggingface/runs/tu5651gd' target=\"_blank\">ax-4.0-light-7b_qlora_0810_0759</a></strong> to <a href='https://wandb.ai/bhc5754-hyalobio/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhc5754-hyalobio/huggingface' target=\"_blank\">https://wandb.ai/bhc5754-hyalobio/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhc5754-hyalobio/huggingface/runs/tu5651gd' target=\"_blank\">https://wandb.ai/bhc5754-hyalobio/huggingface/runs/tu5651gd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='267' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [267/375 55:07 < 22:27, 0.08 it/s, Epoch 3.55/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>9.696300</td>\n",
       "      <td>10.047052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>9.162000</td>\n",
       "      <td>10.047052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-10 17:04:00] step 16\tloss 9.7146\n",
      "[2025-08-10 17:07:18] step 32\tloss 9.3173\n",
      "[2025-08-10 17:10:36] step 48\tloss 9.0676\n",
      "[2025-08-10 17:13:55] step 64\tloss 9.2231\n",
      "[2025-08-10 17:17:08] step 80\tloss 8.9972\n",
      "[2025-08-10 17:20:27] step 96\tloss 9.5488\n",
      "[2025-08-10 17:23:45] step 112\tloss 9.2084\n",
      "[2025-08-10 17:27:03] step 128\tloss 9.6963\n",
      "[2025-08-10 17:30:40] step 144\tloss 8.9933\n",
      "[2025-08-10 17:33:54] step 160\tloss 9.4604\n",
      "[2025-08-10 17:37:12] step 176\tloss 8.9456\n",
      "[2025-08-10 17:40:30] step 192\tloss 9.3939\n",
      "[2025-08-10 17:43:48] step 208\tloss 9.5084\n",
      "[2025-08-10 17:47:07] step 224\tloss 9.1312\n",
      "[2025-08-10 17:50:21] step 240\tloss 9.5661\n",
      "[2025-08-10 17:53:39] step 256\tloss 9.1620\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🎯 훈련 시작...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 어댑터 저장\u001b[39;00m\n\u001b[1;32m      5\u001b[0m adapter_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mselected_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_qlora/adapter\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2238\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2236\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2239\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2582\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2575\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2576\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2577\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2578\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2579\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2580\u001b[0m )\n\u001b[1;32m   2581\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2582\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2585\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2586\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2587\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2588\u001b[0m ):\n\u001b[1;32m   2589\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2590\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:904\u001b[0m, in \u001b[0;36mSFTTrainer.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaybe_activation_offload_context:\n\u001b[0;32m--> 904\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3845\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3842\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   3843\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 3845\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3847\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:2578\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2576\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2577\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2578\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._post_run_cell_hook of <wandb.sdk.wandb_init._WandbInit object at 0x727849cb26b0>> (for post_run_cell), with arguments args (<ExecutionResult object at 72784a3db250, execution_count=28 error_before_exec=None error_in_exec= info=<ExecutionInfo object at 72784a3db130, raw_cell=\"print(\"🎯 훈련 시작...\")\n",
      "trainer.train()\n",
      "\n",
      "# 어댑터 저장\n",
      "adap..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226465762d706f64227d/workspace/fske/note/finetune_qlora.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py:593\u001b[0m, in \u001b[0;36m_WandbInit._post_run_cell_hook\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresuming backend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 593\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface.py:787\u001b[0m, in \u001b[0;36mInterfaceBase.publish_resume\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpublish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    786\u001b[0m     resume \u001b[38;5;241m=\u001b[39m pb\u001b[38;5;241m.\u001b[39mResumeRequest()\n\u001b[0;32m--> 787\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_shared.py:294\u001b[0m, in \u001b[0;36mInterfaceShared._publish_resume\u001b[0;34m(self, resume)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m, resume: pb\u001b[38;5;241m.\u001b[39mResumeRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    293\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(resume\u001b[38;5;241m=\u001b[39mresume)\n\u001b[0;32m--> 294\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_sock.py:39\u001b[0m, in \u001b[0;36mInterfaceSock._publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m, local: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign(record)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_record_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py:166\u001b[0m, in \u001b[0;36mSockClient.send_record_publish\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    164\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrequest_id \u001b[38;5;241m=\u001b[39m record\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mmailbox_slot\n\u001b[1;32m    165\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrecord_publish\u001b[38;5;241m.\u001b[39mCopyFrom(record)\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_server_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_req\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py:146\u001b[0m, in \u001b[0;36mSockClient.send_server_request\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_server_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg: spb\u001b[38;5;241m.\u001b[39mServerRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py:143\u001b[0m, in \u001b[0;36mSockClient._send_message\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    141\u001b[0m header \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<BI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m), raw_size)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendall_with_error_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py:122\u001b[0m, in \u001b[0;36mSockClient._sendall_with_error_handle\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    120\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 122\u001b[0m     sent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;66;03m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sent \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "print(\"🎯 훈련 시작...\")\n",
    "trainer.train()\n",
    "\n",
    "# 어댑터 저장\n",
    "adapter_dir = f\"../../models/{selected_model}_qlora/adapter\"\n",
    "os.makedirs(adapter_dir, exist_ok=True)\n",
    "\n",
    "trainer.model.save_pretrained(adapter_dir)\n",
    "tokenizer.save_pretrained(adapter_dir)\n",
    "\n",
    "print(f\"✅ 어댑터 저장 완료: {adapter_dir}\")\n",
    "print(f\"훈련 완료 후 GPU 메모리: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9958c663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 완료 후 어댑터 병합 (선택사항)\n",
    "# 훈련이 완료되면 이 셀을 실행하여 최종 모델을 생성하세요\n",
    "\n",
    "def merge_adapter():\n",
    "    \"\"\"어댑터를 베이스 모델과 병합하여 최종 모델 생성\"\"\"\n",
    "    \n",
    "    adapter_dir = f\"../../models/{selected_model}_qlora/adapter\"\n",
    "    merged_dir = f\"../../models/{selected_model}_qlora/merged\"\n",
    "    \n",
    "    print(\"🔄 어댑터 병합 시작...\")\n",
    "    \n",
    "    # 메모리 정리\n",
    "    import gc\n",
    "    if 'model' in globals(): del model\n",
    "    if 'base_model' in globals(): del base_model  \n",
    "    if 'trainer' in globals(): del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # 베이스 모델 재로딩 (풀프리시전)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=\"eager\"\n",
    "    )\n",
    "    \n",
    "    # 어댑터 병합\n",
    "    model = PeftModel.from_pretrained(model, adapter_dir)\n",
    "    model = model.merge_and_unload()\n",
    "    \n",
    "    # 저장\n",
    "    model.save_pretrained(merged_dir)\n",
    "    tokenizer.save_pretrained(merged_dir)\n",
    "    \n",
    "    print(f\"✅ 최종 모델 저장 완료: {merged_dir}\")\n",
    "    return merged_dir\n",
    "\n",
    "# 사용법: merge_adapter() 함수를 호출하여 실행\n",
    "\n",
    "merge_adapter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28135989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
