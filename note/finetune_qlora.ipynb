{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c499066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pandas tqdm \n",
    "!pip install -q transformers==4.55.0 # llm requires >=4.46.0\n",
    "!pip install -q safetensors==0.4.3 # downgrade for torch 2.1.0\n",
    "!pip install -q bitsandbytes==0.43.2 accelerate==1.9.0 # quantization\n",
    "!pip install -q peft trl # finetune\n",
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61fb2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pandas as pd, json, random\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from transformers import (AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig)\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0662b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 객관식 여부 판단 함수\n",
    "def is_multiple_choice(question_text):\n",
    "    \"\"\"\n",
    "    객관식 여부를 판단: 2개 이상의 숫자 선택지가 줄 단위로 존재할 경우 객관식으로 간주\n",
    "    \"\"\"\n",
    "    lines = question_text.strip().split(\"\\n\")\n",
    "    option_count = sum(bool(re.match(r\"^\\s*[1-9][0-9]?\\s\", line)) for line in lines)\n",
    "    return option_count >= 2\n",
    "\n",
    "\n",
    "# 질문과 선택지 분리 함수\n",
    "def extract_question_and_choices(full_text):\n",
    "    \"\"\"\n",
    "    전체 질문 문자열에서 질문 본문과 선택지 리스트를 분리\n",
    "    \"\"\"\n",
    "    lines = full_text.strip().split(\"\\n\")\n",
    "    q_lines = []\n",
    "    options = []\n",
    "\n",
    "    for line in lines:\n",
    "        if re.match(r\"^\\s*[1-9][0-9]?\\s\", line):\n",
    "            options.append(line.strip())\n",
    "        else:\n",
    "            q_lines.append(line.strip())\n",
    "    \n",
    "    question = \" \".join(q_lines)\n",
    "    return question, options\n",
    "\n",
    "# 프롬프트 생성기\n",
    "def make_prompt_auto(row):\n",
    "    Question = str(row[\"Question\"]).strip()\n",
    "    Answer = str(row[\"Answer\"]).split(\"답변:\")[-1].strip()\n",
    "    if is_multiple_choice(Question):\n",
    "        question, options = extract_question_and_choices(Question)\n",
    "        prompt = (\n",
    "                \"당신은 금융보안 전문가입니다.\\n\"\n",
    "                \"아래 질문에 대해 적절한 **정답 선택지 번호만 출력**하세요.\\n\\n\"\n",
    "                f\"질문: {question}\\n\"\n",
    "                \"선택지:\\n\"\n",
    "                f\"{chr(10).join(options)}\\n\\n\"\n",
    "                \"답변:\"\n",
    "                )\n",
    "    else:\n",
    "        prompt = (\n",
    "                \"당신은 금융보안 전문가입니다.\\n\"\n",
    "                \"아래 주관식 질문에 대해 정확하고 간략한 설명을 작성하세요.\\n\\n\"\n",
    "                f\"질문: {Question}\\n\\n\"\n",
    "                \"답변:\"\n",
    "                )\n",
    "    response = \n",
    "\n",
    "    return prompt, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc21556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "\n",
    "dfs = []\n",
    "dfs.append(pd.read_csv(\"../data/CyberMetric/mcqa.csv\", \"CyberMetric\"))\n",
    "dfs.append(pd.read_csv(\"../data/FinShibainu/mcqa.csv\", \"FinShibainu\"))\n",
    "dfs.append(pd.read_csv(\"../data/FinShibainu/qa.csv\", \"FinShibainu\"))\n",
    "dfs.append(pd.read_csv(\"../data/SecBench/mcqa.csv\", \"SecBench\"))\n",
    "dfs.append(pd.read_csv(\"../data/SecBench/qa.csv\", \"SecBench\"))\n",
    "\n",
    "full = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f85b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb95101",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [make_prompt_auto(q) for _, r in full.iterrows()]\n",
    "random.shuffle(records)\n",
    "\n",
    "# 간단 split\n",
    "n = int(len(records)*0.98)\n",
    "train_ds = Dataset.from_list(records[:n])\n",
    "eval_ds  = Dataset.from_list(records[n:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd113ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6c3b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11558692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 선택\n",
    "models = [\n",
    "    \"gemma-ko-7b\", # baseline\n",
    "    \"ax-4.0-light-7b\", # skt\n",
    "    # \"polyglot-12.8b\",\n",
    "    # \"koalpaca-polyglot-12.8b\",\n",
    "    \"midm-2.0-11.5b\", # kt\n",
    "    # \"HyperCLOVAX-SEED-Think-14B\", # naver\n",
    "    # \"kanana-1.5-15.7b-a3b-instruct\", # kakao\n",
    "    # \"exaone-4.0-32b\" # lg\n",
    "]\n",
    "selected_model = models[0]\n",
    "model_path = f\"/workspace/models/{selected_model}\" # 로컬 저장 모델 경로\n",
    "\n",
    "# 4bit 설정\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # NaN 방지\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# # 8bit 설정\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_8bit=True,          # 4bit → 8bit\n",
    "#     llm_int8_threshold=6.0,     # 기본값 (필요 시 조정)\n",
    "#     llm_int8_has_fp16_weight=False  # True로 하면 일부 레이어 FP16 유지\n",
    "# )\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    padding_side=\"left\"\n",
    ")\n",
    "\n",
    "# 모델 로드\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\", # GPU 자동 배정\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"eager\",\n",
    "    # trust_remote_code=True # naver\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d753be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA Setting\n",
    "\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16, lora_alpha=32, lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],  # 모델에 맞게\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(base_model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55601b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) 텍스트 컬럼 지정 (SFT)\n",
    "def formatting_func(samples):\n",
    "    # SFTTrainer는 list[str] 반환을 기대. 여기서 \"prompt+response\"를 결합해서 지도학습\n",
    "    texts = []\n",
    "    for p, r in zip(samples[\"prompt\"], samples[\"response\"]):\n",
    "        texts.append(p + r)\n",
    "    return texts\n",
    "\n",
    "cfg = SFTConfig(\n",
    "    output_dir=f\"/workspace/models/{selected_model}_qlora\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=20,\n",
    "    save_steps=500,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    bf16=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    gradient_checkpointing=True,\n",
    "    max_seq_length=2048,\n",
    "    dataset_num_proc=1,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    formatting_func=formatting_func,  # 데이터 전처리 함수\n",
    "    args=cfg,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(f\"/workspace/models/{selected_model}_qlora/adapter\")  # LoRA 어댑터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525c8a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (선택) 추론 편의 위해 베이스와 병합하여 단일 가중치 생성\n",
    "try:\n",
    "    merged = trainer.model.merge_and_unload()\n",
    "    merged.save_pretrained(f\"/workspace/models/{selected_model}_qlora/merged\")\n",
    "    tokenizer.save_pretrained(f\"/workspace/models/{selected_model}_qlora/merged\")\n",
    "except Exception as e:\n",
    "    print(\"merge skipped:\", e)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
