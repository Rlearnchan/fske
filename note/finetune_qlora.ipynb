{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c499066d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q pandas tqdm \n",
    "!pip install -q transformers==4.55.0 # llm requires >=4.46.0\n",
    "!pip install -q safetensors==0.4.3 # downgrade for torch 2.1.0\n",
    "!pip install -q bitsandbytes==0.43.2 accelerate==1.9.0 # quantization\n",
    "!pip install -q peft==0.17.0 trl==0.21.0 # finetune\n",
    "!pip install -q datasets wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d61fb2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pandas as pd, json, random\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from transformers import (AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig)\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import torch\n",
    "import re\n",
    "import wandb\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17cc985e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "!wandb login 80f8e961aab3ee77e8dc19ddc1bfd4604203e400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0662b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°ê´€ì‹ ì—¬ë¶€ íŒë‹¨ í•¨ìˆ˜\n",
    "# def is_multiple_choice(question_text):\n",
    "#     \"\"\"\n",
    "#     ê°ê´€ì‹ ì—¬ë¶€ë¥¼ íŒë‹¨: 2ê°œ ì´ìƒì˜ ìˆ«ì ì„ íƒì§€ê°€ ì¤„ ë‹¨ìœ„ë¡œ ì¡´ì¬í•  ê²½ìš° ê°ê´€ì‹ìœ¼ë¡œ ê°„ì£¼\n",
    "#     \"\"\"\n",
    "#     question_text = question_text.replace(\"\\\\n\", \"\\n\")\n",
    "#     lines = question_text.strip().split(\"\\n\")\n",
    "#     option_count = sum(bool(re.match(r\"^\\s*[1-9][0-9]?\\s\", line)) for line in lines)\n",
    "#     return option_count >= 2\n",
    "\n",
    "def is_multiple_choice(question_text):\n",
    "    question_text = question_text.replace(\"\\\\n\", \"\\n\")\n",
    "    lines = question_text.strip().split(\"\\n\")\n",
    "    pat = re.compile(r\"^\\s*[\\(\\[]?\\d{1}[\\)\\]\\.]?\\s+\")\n",
    "    option_count = sum(bool(pat.match(line)) for line in lines)\n",
    "    return option_count >= 2\n",
    "\n",
    "\n",
    "# # ì§ˆë¬¸ê³¼ ì„ íƒì§€ ë¶„ë¦¬ í•¨ìˆ˜\n",
    "# def extract_question_and_choices(full_text):\n",
    "#     \"\"\"\n",
    "#     ì „ì²´ ì§ˆë¬¸ ë¬¸ìì—´ì—ì„œ ì§ˆë¬¸ ë³¸ë¬¸ê³¼ ì„ íƒì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¶„ë¦¬\n",
    "#     \"\"\"\n",
    "#     lines = full_text.strip().split(\"\\n\")\n",
    "#     q_lines = []\n",
    "#     options = []\n",
    "\n",
    "#     for line in lines:\n",
    "#         if re.match(r\"^\\s*[1-9][0-9]?\\s\", line):\n",
    "#             options.append(line.strip())\n",
    "#         else:\n",
    "#             q_lines.append(line.strip())\n",
    "    \n",
    "#     question = \" \".join(q_lines)\n",
    "#     return question, options\n",
    "\n",
    "# ìˆ«ì/ì›í˜•ìˆ«ì ì˜µì…˜ ë¼ì¸ ë§¤ì¹­\n",
    "OPTION_LINE = re.compile(r\"^\\s*(?:[\\(\\[]?\\d{1,2}[\\)\\]\\.]?|[\\u2460-\\u2473])\\s+\")\n",
    "\n",
    "def extract_question_and_choices(full_text: str):\n",
    "    \"\"\"\n",
    "    ì „ì²´ ì§ˆë¬¸ ë¬¸ìì—´ì—ì„œ ì§ˆë¬¸ ë³¸ë¬¸ê³¼ ì„ íƒì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¶„ë¦¬\n",
    "    - \"\\\\n\" ê°™ì€ ë¦¬í„°ëŸ´ ì¤„ë°”ê¿ˆë„ ì²˜ë¦¬\n",
    "    - ì˜µì…˜ì´ ì—¬ëŸ¬ ì¤„ë¡œ ì´ì–´ì§€ë©´ ë’¤ì¤„ì„ ê°™ì€ ì„ íƒì§€ì— ì´ì–´ ë¶™ì„\n",
    "    - ì˜µì…˜ ì• ë²ˆí˜¸/ê¸°í˜¸ëŠ” ìœ ì§€\n",
    "    \"\"\"\n",
    "    # ë¦¬í„°ëŸ´ ì¤„ë°”ê¿ˆê³¼ ë‹¤ì–‘í•œ ê°œí–‰ ì •ê·œí™”\n",
    "    text = (full_text\n",
    "            .replace(\"\\\\r\\\\n\", \"\\n\")\n",
    "            .replace(\"\\\\n\", \"\\n\")\n",
    "            .replace(\"\\r\\n\", \"\\n\")\n",
    "            .replace(\"\\r\", \"\\n\"))\n",
    "\n",
    "    lines = [ln.strip() for ln in text.strip().split(\"\\n\") if ln.strip()]\n",
    "\n",
    "    q_lines = []\n",
    "    options = []\n",
    "    in_options = False\n",
    "\n",
    "    for ln in lines:\n",
    "        if OPTION_LINE.match(ln):\n",
    "            in_options = True\n",
    "            options.append(ln.strip())  # ë²ˆí˜¸ í¬í•¨ ê·¸ëŒ€ë¡œ\n",
    "        else:\n",
    "            if in_options and options:\n",
    "                # ì˜µì…˜ ì¤„ ë‹¤ìŒ ì¤„ì´ ë¶™ëŠ” ê²½ìš°\n",
    "                options[-1] += \" \" + ln\n",
    "            else:\n",
    "                q_lines.append(ln)\n",
    "\n",
    "    question = \" \".join(q_lines).strip()\n",
    "    return question, options\n",
    "\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ìƒì„±ê¸°\n",
    "def make_prompt_auto(row):\n",
    "    Question = str(row[\"Question\"]).strip()\n",
    "    Answer = str(row[\"Answer\"]).split(\"ë‹µë³€:\")[-1].strip()\n",
    "    if is_multiple_choice(Question):\n",
    "        question, options = extract_question_and_choices(Question)\n",
    "        prompt = (\n",
    "                \"ë‹¹ì‹ ì€ ê¸ˆìœµë³´ì•ˆ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\\n\"\n",
    "                # \"ì•„ë˜ ì§ˆë¬¸ì— ëŒ€í•´ ì ì ˆí•œ **ì •ë‹µ ì„ íƒì§€ ë²ˆí˜¸ë§Œ ì¶œë ¥**í•˜ì„¸ìš”.\\n\\n\"\n",
    "                \"ì•„ë˜ ì§ˆë¬¸ì— ëŒ€í•´ ì ì ˆí•œ ì„ íƒì§€ë¥¼ ì¶œë ¥í•˜ì„¸ìš”.\\n\\n\"\n",
    "                f\"ì§ˆë¬¸: {question}\\n\"\n",
    "                \"ì„ íƒì§€:\\n\"\n",
    "                f\"{chr(10).join(options)}\\n\\n\"\n",
    "                \"ë‹µë³€:\"\n",
    "                )\n",
    "    else:\n",
    "        prompt = (\n",
    "                \"ë‹¹ì‹ ì€ ê¸ˆìœµë³´ì•ˆ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\\n\"\n",
    "                \"ì•„ë˜ ì£¼ê´€ì‹ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ê°„ëµí•œ ì„¤ëª…ì„ ì‘ì„±í•˜ì„¸ìš”.\\n\\n\"\n",
    "                f\"ì§ˆë¬¸: {Question}\\n\\n\"\n",
    "                \"ë‹µë³€:\"\n",
    "                )\n",
    "    response = Answer\n",
    "\n",
    "    return prompt, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fc21556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¡œë“œ\n",
    "\n",
    "dfs = []\n",
    "dfs.append(pd.read_csv(\"../data/CyberMetric/mcqa_enhanced.csv\"))\n",
    "dfs.append(pd.read_csv(\"../data/FinShibainu/mcqa_enhanced.csv\"))\n",
    "dfs.append(pd.read_csv(\"../data/FinShibainu/qa.csv\"))\n",
    "dfs.append(pd.read_csv(\"../data/SecBench/mcqa_enhanced.csv\"))\n",
    "dfs.append(pd.read_csv(\"../data/SecBench/qa.csv\"))\n",
    "\n",
    "full = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27f85b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ë‹¤ìŒ ì¤‘ ì •ë³´ì˜ ë¹„ë°€ì„ ë‚˜íƒ€ë‚´ëŠ” ê²ƒì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?\\n1. ê°€ìš©ì„±\\n2. ì¸ì¦\\n3....</td>\n",
       "      <td>ë‹µë³€: 4, ê¸°ë°€ì„±</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ì–´ë–¤ ìœ í˜•ì˜ ì¸ì¦ì€ ë‹¹ì‹ ì´ ì•„ëŠ” ê²ƒ, ë‹¹ì‹ ì´ ê°€ì§„ ê²ƒ, ë‹¹ì‹ ì´ìˆëŠ” ê²ƒê³¼ ê°™ì€ ì—¬ëŸ¬ ...</td>\n",
       "      <td>ë‹µë³€: 3, ë‹¤ì¤‘ ì¸ì¦ ì¸ì¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ë°œê°€ë½ì€ ë¬´ì—‡ì„ ì˜ë¯¸í•©ë‹ˆê¹Œ?\\n1. í‰ê°€ ëª©í‘œ\\n2. í‰ê°€ ì‹œê°„\\n3. í‰ê°€ ìœ í˜•\\...</td>\n",
       "      <td>ë‹µë³€: 1, í‰ê°€ ëª©í‘œ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ì‹œìŠ¤í…œì´ ë¦¬ì†ŒìŠ¤ì— ëŒ€í•œ ì•¡ì„¸ìŠ¤ë¥¼ ìš”ì²­í•˜ëŠ” ì‚¬ìš©ìê°€ ì‹¤ì œë¡œ ìì‹ ì´ ì£¼ì¥í•˜ëŠ” ì‚¬ëŒì¸ì§€ ...</td>\n",
       "      <td>ë‹µë³€: 4, ì¸ì¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ê¸°ë°€ì„±, ë¬´ê²°ì„± ë° ë°ì´í„° ë° ìì‚°ì˜ ê°€ìš©ì„±ì— ëŒ€í•œ í™•ì¸ ë° ë³´ì¦ì„ í¬í•¨í•˜ì—¬ ì •ë³´ ...</td>\n",
       "      <td>ë‹µë³€: 2, ì •ë³´ ë³´ì¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100508</th>\n",
       "      <td>ë¸”ë¡ ì²´ì¸ ê¸°ìˆ ì—ì„œ ê±°ë˜ ê²€ì¦ ê°€ëŠ¥ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ê±°ë˜ ê°œì¸ ì •ë³´ë¥¼ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´...</td>\n",
       "      <td>ë‹µë³€: ê±°ë˜ ê°œì¸ ì •ë³´ ë³´í˜¸ë¥¼ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì œë¡œ ì§€ì‹ ì¦ê±°ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì€ ë‹¹...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100509</th>\n",
       "      <td>Linux ì‹œìŠ¤í…œì—ì„œ ì‹¤ì œ ê¶Œí•œ ì—ìŠ¤ì»¬ë ˆì´ì…˜ ì·¨ì•½ì„± (ì˜ˆ : CVE-2019-186...</td>\n",
       "      <td>ë‹µë³€: CVE-2019-18634ëŠ” Linux ì‹œìŠ¤í…œì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ê¶Œí•œ ì—ìŠ¤ì»¬ë ˆ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100510</th>\n",
       "      <td>PHPì—ì„œ ë¬¸ìì—´ì„ ì •ì˜í•˜ê¸° ìœ„í•´ ì´ì¤‘ ì¸ìš©ë¬¸ê³¼ ë‹¨ì¼ ë”°ì˜´í‘œ ì‚¬ìš©ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?</td>\n",
       "      <td>ë‹µë³€: PHPì—ì„œ ì´ì¤‘ ì¸ìš©ë¬¸ìœ¼ë¡œ ì •ì˜ ëœ ë¬¸ìì—´ì€ ë³€ìˆ˜ë¥¼ êµ¬ë¬¸ ë¶„ì„í•˜ëŠ” ë°˜ë©´ ë‹¨ì¼...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100511</th>\n",
       "      <td>PHP ì½”ë“œë¥¼ ì‹ë³„í•˜ê¸° ìœ„í•´ PHPì— êµ¬ë¬¸ ë¶„ì„ íƒœê·¸ë¥¼ ì‘ì„±í•˜ëŠ” ë°©ë²•ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?</td>\n",
       "      <td>ë‹µë³€: PHPì—ì„œ êµ¬ë¬¸ ë¶„ì„ íƒœê·¸ë¥¼ ì‘ì„±í•˜ëŠ” ì„¸ ê°€ì§€ ë°©ë²•, ì¦‰ '&lt;? php?&gt;'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100512</th>\n",
       "      <td>ë‹¤ì¤‘ ì‚¬ìš©ì Linux ì‹œìŠ¤í…œì—ì„œ í—ˆê°€ ê´€ë¦¬ (ì˜ˆ : ì‚¬ìš©ì ë° ê·¸ë£¹ ê¶Œí•œ)ë¥¼ í†µí•´...</td>\n",
       "      <td>ë‹µë³€: ë‹¤ì¤‘ ì‚¬ìš©ì Linux ì‹œìŠ¤í…œì—ì„œ ì¤‘ìš”í•œ íŒŒì¼ ì‹œìŠ¤í…œ ë””ë ‰í† ë¦¬ì— ëŒ€í•œ ë¬´ë‹¨ ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100513 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Question  \\\n",
       "0       ë‹¤ìŒ ì¤‘ ì •ë³´ì˜ ë¹„ë°€ì„ ë‚˜íƒ€ë‚´ëŠ” ê²ƒì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?\\n1. ê°€ìš©ì„±\\n2. ì¸ì¦\\n3....   \n",
       "1       ì–´ë–¤ ìœ í˜•ì˜ ì¸ì¦ì€ ë‹¹ì‹ ì´ ì•„ëŠ” ê²ƒ, ë‹¹ì‹ ì´ ê°€ì§„ ê²ƒ, ë‹¹ì‹ ì´ìˆëŠ” ê²ƒê³¼ ê°™ì€ ì—¬ëŸ¬ ...   \n",
       "2       ë°œê°€ë½ì€ ë¬´ì—‡ì„ ì˜ë¯¸í•©ë‹ˆê¹Œ?\\n1. í‰ê°€ ëª©í‘œ\\n2. í‰ê°€ ì‹œê°„\\n3. í‰ê°€ ìœ í˜•\\...   \n",
       "3       ì‹œìŠ¤í…œì´ ë¦¬ì†ŒìŠ¤ì— ëŒ€í•œ ì•¡ì„¸ìŠ¤ë¥¼ ìš”ì²­í•˜ëŠ” ì‚¬ìš©ìê°€ ì‹¤ì œë¡œ ìì‹ ì´ ì£¼ì¥í•˜ëŠ” ì‚¬ëŒì¸ì§€ ...   \n",
       "4       ê¸°ë°€ì„±, ë¬´ê²°ì„± ë° ë°ì´í„° ë° ìì‚°ì˜ ê°€ìš©ì„±ì— ëŒ€í•œ í™•ì¸ ë° ë³´ì¦ì„ í¬í•¨í•˜ì—¬ ì •ë³´ ...   \n",
       "...                                                   ...   \n",
       "100508  ë¸”ë¡ ì²´ì¸ ê¸°ìˆ ì—ì„œ ê±°ë˜ ê²€ì¦ ê°€ëŠ¥ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ê±°ë˜ ê°œì¸ ì •ë³´ë¥¼ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´...   \n",
       "100509  Linux ì‹œìŠ¤í…œì—ì„œ ì‹¤ì œ ê¶Œí•œ ì—ìŠ¤ì»¬ë ˆì´ì…˜ ì·¨ì•½ì„± (ì˜ˆ : CVE-2019-186...   \n",
       "100510  PHPì—ì„œ ë¬¸ìì—´ì„ ì •ì˜í•˜ê¸° ìœ„í•´ ì´ì¤‘ ì¸ìš©ë¬¸ê³¼ ë‹¨ì¼ ë”°ì˜´í‘œ ì‚¬ìš©ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?   \n",
       "100511     PHP ì½”ë“œë¥¼ ì‹ë³„í•˜ê¸° ìœ„í•´ PHPì— êµ¬ë¬¸ ë¶„ì„ íƒœê·¸ë¥¼ ì‘ì„±í•˜ëŠ” ë°©ë²•ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?   \n",
       "100512  ë‹¤ì¤‘ ì‚¬ìš©ì Linux ì‹œìŠ¤í…œì—ì„œ í—ˆê°€ ê´€ë¦¬ (ì˜ˆ : ì‚¬ìš©ì ë° ê·¸ë£¹ ê¶Œí•œ)ë¥¼ í†µí•´...   \n",
       "\n",
       "                                                   Answer  \n",
       "0                                              ë‹µë³€: 4, ê¸°ë°€ì„±  \n",
       "1                                         ë‹µë³€: 3, ë‹¤ì¤‘ ì¸ì¦ ì¸ì¦  \n",
       "2                                            ë‹µë³€: 1, í‰ê°€ ëª©í‘œ  \n",
       "3                                               ë‹µë³€: 4, ì¸ì¦  \n",
       "4                                            ë‹µë³€: 2, ì •ë³´ ë³´ì¦  \n",
       "...                                                   ...  \n",
       "100508  ë‹µë³€: ê±°ë˜ ê°œì¸ ì •ë³´ ë³´í˜¸ë¥¼ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì œë¡œ ì§€ì‹ ì¦ê±°ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì€ ë‹¹...  \n",
       "100509  ë‹µë³€: CVE-2019-18634ëŠ” Linux ì‹œìŠ¤í…œì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ê¶Œí•œ ì—ìŠ¤ì»¬ë ˆ...  \n",
       "100510  ë‹µë³€: PHPì—ì„œ ì´ì¤‘ ì¸ìš©ë¬¸ìœ¼ë¡œ ì •ì˜ ëœ ë¬¸ìì—´ì€ ë³€ìˆ˜ë¥¼ êµ¬ë¬¸ ë¶„ì„í•˜ëŠ” ë°˜ë©´ ë‹¨ì¼...  \n",
       "100511  ë‹µë³€: PHPì—ì„œ êµ¬ë¬¸ ë¶„ì„ íƒœê·¸ë¥¼ ì‘ì„±í•˜ëŠ” ì„¸ ê°€ì§€ ë°©ë²•, ì¦‰ '<? php?>'...  \n",
       "100512  ë‹µë³€: ë‹¤ì¤‘ ì‚¬ìš©ì Linux ì‹œìŠ¤í…œì—ì„œ ì¤‘ìš”í•œ íŒŒì¼ ì‹œìŠ¤í…œ ë””ë ‰í† ë¦¬ì— ëŒ€í•œ ë¬´ë‹¨ ...  \n",
       "\n",
       "[100513 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcb95101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì „ì²´ ë°ì´í„°: 100,513ê°œ â†’ ìƒ˜í”Œë§: 10,051ê°œ (10%)\n"
     ]
    }
   ],
   "source": [
    "# íŠœí”Œì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜\n",
    "tuples = [make_prompt_auto(r) for _, r in full.iterrows()]\n",
    "records = [{\"prompt\": prompt, \"completion\": response} for prompt, response in tuples] # SFTTrainer ì—ì„œ completion í•„ë“œ ì‚¬ìš©\n",
    "random.seed(42)\n",
    "random.shuffle(records)\n",
    "\n",
    "# 10% ìƒ˜í”Œë§ (í…ŒìŠ¤íŠ¸ìš©)\n",
    "sample_size = int(len(records) * 0.1)\n",
    "sampled_records = records[:sample_size]\n",
    "print(f\"ì „ì²´ ë°ì´í„°: {len(records):,}ê°œ â†’ ìƒ˜í”Œë§: {len(sampled_records):,}ê°œ (10%)\")\n",
    "\n",
    "# ê°„ë‹¨ split\n",
    "# n = int(len(records)*0.95)\n",
    "n = int(len(sampled_records)*0.95)\n",
    "train_ds = Dataset.from_list(sampled_records[:n])\n",
    "eval_ds  = Dataset.from_list(sampled_records[n:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd113ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'completion'],\n",
      "    num_rows: 9548\n",
      "}) Dataset({\n",
      "    features: ['prompt', 'completion'],\n",
      "    num_rows: 503\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_ds, eval_ds)\n",
    "print(train_ds[0]) # ì§§ì€ MCQA\n",
    "print(train_ds[1]) # ê¸´ QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11558692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d5708660a7043288c2b08890bfaf7d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ëª¨ë¸ ì„ íƒ\n",
    "models = [\n",
    "    \"gemma-ko-7b\", # baseline\n",
    "    \"ax-4.0-light-7b\", # skt\n",
    "    # \"polyglot-12.8b\",\n",
    "    # \"koalpaca-polyglot-12.8b\",\n",
    "    \"midm-2.0-11.5b\", # kt\n",
    "    # \"HyperCLOVAX-SEED-Think-14B\", # naver\n",
    "    # \"kanana-1.5-15.7b-a3b-instruct\", # kakao\n",
    "    # \"exaone-4.0-32b\" # lg\n",
    "]\n",
    "selected_model = models[1]\n",
    "model_path = f\"../../models/{selected_model}\" # ë¡œì»¬ ì €ì¥ ëª¨ë¸ ê²½ë¡œ\n",
    "\n",
    "# 4bit ì„¤ì •\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # NaN ë°©ì§€\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# # 8bit ì„¤ì •\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_8bit=True,          # 4bit â†’ 8bit\n",
    "#     llm_int8_threshold=6.0,     # ê¸°ë³¸ê°’ (í•„ìš” ì‹œ ì¡°ì •)\n",
    "#     llm_int8_has_fp16_weight=False  # Trueë¡œ í•˜ë©´ ì¼ë¶€ ë ˆì´ì–´ FP16 ìœ ì§€\n",
    "# )\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    # padding_side=\"left\",\n",
    "    padding_side=\"right\" # í•™ìŠµ ì‹œ right ê¶Œì¥\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\", # GPU ìë™ ë°°ì •\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"eager\",\n",
    "    # trust_remote_code=True # naver\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d9698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(102400, 3584)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((3584,), eps=1e-05)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=102400, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model # ëª¨ë¸ í™•ì¸. target_modulesì— ì…ë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d753be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA Setting\n",
    "\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "# LoRA (Low-Rank Adaptation) ì„¤ì •\n",
    "peft_config = LoraConfig(\n",
    "    # LoRA í•µì‹¬ íŒŒë¼ë¯¸í„°\n",
    "    r=16,                       # ë­í¬(rank): LoRA í–‰ë ¬ì˜ ì°¨ì›, ë‚®ì„ìˆ˜ë¡ íŒŒë¼ë¯¸í„° ì ìŒ, ë†’ì„ìˆ˜ë¡ í‘œí˜„ë ¥ ì¦ê°€\n",
    "                                # ì¼ë°˜ì ìœ¼ë¡œ 8~64 ì‚¬ìš©, 16ì€ ì„±ëŠ¥ê³¼ íš¨ìœ¨ì„±ì˜ ê· í˜•ì \n",
    "    \n",
    "    lora_alpha=32,              # ìŠ¤ì¼€ì¼ë§ íŒŒë¼ë¯¸í„°: LoRA ì—…ë°ì´íŠ¸ì˜ ê°•ë„ ì¡°ì ˆ\n",
    "                                # ì¼ë°˜ì ìœ¼ë¡œ rì˜ 2ë°° ì„¤ì • (16*2=32)\n",
    "                                # ë†’ì„ìˆ˜ë¡ LoRAì˜ ì˜í–¥ë ¥ ì¦ê°€\n",
    "    \n",
    "    lora_dropout=0.05,          # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨: ê³¼ì í•© ë°©ì§€\n",
    "                                # 0.05 = 5% ë“œë¡­ì•„ì›ƒ, ì¼ë°˜ì ìœ¼ë¡œ 0.05~0.1 ì‚¬ìš©\n",
    "    \n",
    "    # ì ìš©í•  ëª¨ë“ˆ ì§€ì • (Transformerì˜ ì£¼ìš” ì„ í˜• ë ˆì´ì–´ë“¤)\n",
    "    target_modules=[\n",
    "        \"q_proj\",               # Query í”„ë¡œì ì…˜ ë ˆì´ì–´\n",
    "        \"k_proj\",               # Key í”„ë¡œì ì…˜ ë ˆì´ì–´  \n",
    "        \"v_proj\",               # Value í”„ë¡œì ì…˜ ë ˆì´ì–´\n",
    "        \"o_proj\",               # Output í”„ë¡œì ì…˜ ë ˆì´ì–´\n",
    "        \"gate_proj\",            # Gate í”„ë¡œì ì…˜ ë ˆì´ì–´ (FFN)\n",
    "        \"up_proj\",              # Up í”„ë¡œì ì…˜ ë ˆì´ì–´ (FFN)\n",
    "        \"down_proj\"             # Down í”„ë¡œì ì…˜ ë ˆì´ì–´ (FFN)\n",
    "    ],\n",
    "    \n",
    "    task_type=\"CAUSAL_LM\"       # íƒœìŠ¤í¬ íƒ€ì…: ì¸ê³¼ì  ì–¸ì–´ëª¨ë¸ (ë‹¤ìŒ í† í° ì˜ˆì¸¡)\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, peft_config)\n",
    "model.train() # ì¶”ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6064e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 40,370,176 || all params: 7,299,995,136 || trainable%: 0.5530\n"
     ]
    }
   ],
   "source": [
    "# === QLoRA ì¤€ë¹„ ì§í›„ ì ê²€ ì…€ ===\n",
    "from peft import PeftModel\n",
    "\n",
    "# 1) í›ˆë ¨ ëª¨ë“œ + ì…ë ¥ ê·¸ë¼ë“œ (4bit/8bitì—ì„œ í•„ìš”)\n",
    "model.gradient_checkpointing_enable()  # gradient_checkpointing ì˜µì…˜ ì¼ë‹¤ë©´ í™•ì‹¤íˆ ì¼œì£¼ê¸°\n",
    "try:\n",
    "    model.enable_input_require_grads()  # ì¼ë¶€ ëª¨ë¸ì—ì„œ í•„ìš” (ì„ë² ë”© ì…ë ¥ grad í—ˆìš©)\n",
    "except Exception as e:\n",
    "    print(\"[warn] enable_input_require_grads() skip:\", e)\n",
    "\n",
    "model.train()  # <- ë°˜ë“œì‹œ í›ˆë ¨ ëª¨ë“œ ì „í™˜\n",
    "\n",
    "# 2) LoRA ì ìš©/í›ˆë ¨ íŒŒë¼ë¯¸í„° í™•ì¸\n",
    "if hasattr(model, \"print_trainable_parameters\"):\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "trainable = [(n, p) for n, p in model.named_parameters() if p.requires_grad]\n",
    "print(f\"[check] # of trainable params: {len(trainable)}\")\n",
    "print(\"  (ì˜ˆì‹œ 3ê°œ)\", [n for n, _ in trainable[:3]])\n",
    "\n",
    "# LoRA ëª¨ë“ˆì´ ì‹¤ì œë¡œ ë¶™ì—ˆëŠ”ì§€ ë¹ ë¥´ê²Œ í™•ì¸\n",
    "has_lora = any(\"lora_\" in n.lower() or \"lora_A\" in n or \"lora_B\" in n for n, _ in trainable)\n",
    "print(f\"[check] lora modules present? {has_lora}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27c6e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í›ˆë ¨ ì„¤ì • ì™„ë£Œ!\n",
      "ì´ ì—í¬í¬: 5\n",
      "ì‹¤ì œ ë°°ì¹˜ í¬ê¸°: 8 Ã— 16 = 128\n",
      "ë¡œê¹… ì£¼ê¸°: 16ìŠ¤í…ë§ˆë‹¤\n"
     ]
    }
   ],
   "source": [
    "# ê°œì„ ëœ í›ˆë ¨ ì„¤ì •\n",
    "tokenizer.model_max_length = 128 # ê¸´ ë‹µë³€ í•„ìš” ì—†ìŒ\n",
    "\n",
    "cfg = SFTConfig(\n",
    "    output_dir=f\"../../models/{selected_model}_qlora\",\n",
    "    \n",
    "    # í›ˆë ¨ ìŠ¤ì¼€ì¤„ë§ - ë³¸ê²© í•™ìŠµìš© ì„¤ì •\n",
    "    num_train_epochs=5,           \n",
    "    per_device_train_batch_size=8, # ì‹¤ì œ í•™ìŠµ ì‹œ í™•ëŒ€ (GPU VRAM ì²´í¬)\n",
    "    gradient_accumulation_steps=8, # optimizer step ì£¼ê¸°\n",
    "    \n",
    "    # í•™ìŠµë¥  ë° ìµœì í™”\n",
    "    learning_rate=1e-4,           # ì•ˆì •ì ì¸ í•™ìŠµë¥ \n",
    "    weight_decay=0.01,            # ê³¼ì í•© ë°©ì§€\n",
    "    max_grad_norm=1.0,            # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘\n",
    "    \n",
    "    # ë¡œê¹… ë° ì €ì¥ (ê°œì„ ëœ ì£¼ê¸°)\n",
    "    logging_steps=16,             \n",
    "    logging_strategy=\"steps\",\n",
    "    # logging_first_step=True,\n",
    "    save_steps=64,               \n",
    "    # save_total_limit=3,         # ìµœëŒ€ 3ê°œ ì²´í¬í¬ì¸íŠ¸ ìœ ì§€\n",
    "    \n",
    "    # í‰ê°€ ì„¤ì •\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=64,               \n",
    "    load_best_model_at_end=True,  # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ìë™ ì„ íƒ\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    \n",
    "    # í•˜ë“œì›¨ì–´ ìµœì í™”\n",
    "    bf16=True,\n",
    "    dataloader_num_workers=4,\n",
    "    dataloader_pin_memory=True,\n",
    "    \n",
    "    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ìµœì í™”\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    # ê¸°íƒ€ ì„¤ì •\n",
    "    remove_unused_columns=False,\n",
    "    report_to=[\"wandb\"],            \n",
    "    run_name=f\"{selected_model}_qlora_{datetime.datetime.now().strftime('%m%d_%H%M')}\",\n",
    "    # disable_tqdm=True,            # tqdmì´ stdout ì‚¼í‚¤ëŠ” ì´ìŠˆ íšŒí”¼\n",
    ")\n",
    "\n",
    "print(\"âœ… í›ˆë ¨ ì„¤ì • ì™„ë£Œ!\")\n",
    "print(f\"ì´ ì—í¬í¬: {cfg.num_train_epochs}\")\n",
    "print(f\"ì‹¤ì œ ë°°ì¹˜ í¬ê¸°: {cfg.per_device_train_batch_size} Ã— {cfg.gradient_accumulation_steps} = {cfg.per_device_train_batch_size * cfg.gradient_accumulation_steps}\")\n",
    "print(f\"ë¡œê¹… ì£¼ê¸°: {cfg.logging_steps}ìŠ¤í…ë§ˆë‹¤\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea8c9d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d5690f8c47480da70b4d747c43d86a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9548 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 128). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00437bf6074549caba7fc077fc62509c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/503 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì›ë³¸ í›ˆë ¨ ìƒ˜í”Œ: 9,548ê°œ\n",
      "âœ… ìµœì¢… í›ˆë ¨ ìƒ˜í”Œ: 9,548ê°œ\n",
      "âœ… ì œê±°ëœ ìƒ˜í”Œ: 0ê°œ (ëª¨ë“  ìƒ˜í”Œ ë³´ì¡´!)\n",
      "âœ… ìµœëŒ€ í† í° ìˆ˜: 127\n",
      "âœ… í‰ê·  í† í° ìˆ˜: 113.5\n"
     ]
    }
   ],
   "source": [
    "def truncate_keep_all_samples(dataset, max_length=None):\n",
    "    \"\"\"ëª¨ë“  ìƒ˜í”Œì„ ë³´ì¡´í•˜ë©´ì„œ ê¸¸ì´ë§Œ ì¡°ì •\"\"\"\n",
    "    if max_length is None:\n",
    "        max_length = tokenizer.model_max_length\n",
    "    \n",
    "    def truncate_sample(sample):\n",
    "        prompt = sample[\"prompt\"]\n",
    "        completion = sample[\"completion\"]\n",
    "        \n",
    "        # ì „ì²´ ê¸¸ì´ ì²´í¬\n",
    "        full_text = prompt + completion\n",
    "        full_tokens = tokenizer.encode(full_text, add_special_tokens=True)\n",
    "        \n",
    "        if len(full_tokens) <= max_length:\n",
    "            return sample  # ê·¸ëŒ€ë¡œ ìœ ì§€\n",
    "        \n",
    "        # í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ í”„ë¡¬í”„íŠ¸ë„ ìë¥´ê¸°\n",
    "        prompt_tokens = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "        completion_tokens = tokenizer.encode(completion, add_special_tokens=False)\n",
    "        \n",
    "        total_needed = len(prompt_tokens) + len(completion_tokens) + 2  # BOS/EOS\n",
    "        \n",
    "        if total_needed <= max_length:\n",
    "            return sample  # ì‹¤ì œë¡œëŠ” ë¬¸ì œì—†ìŒ\n",
    "        \n",
    "        # í”„ë¡¬í”„íŠ¸:ì™„ì„± = 7:3 ë¹„ìœ¨ë¡œ í• ë‹¹\n",
    "        prompt_quota = int(max_length * 0.7)\n",
    "        completion_quota = max_length - prompt_quota - 2\n",
    "        \n",
    "        # í”„ë¡¬í”„íŠ¸ ìë¥´ê¸° (ë’¤ìª½ ìœ ì§€)\n",
    "        if len(prompt_tokens) > prompt_quota:\n",
    "            truncated_prompt_tokens = prompt_tokens[-prompt_quota:]\n",
    "            truncated_prompt = tokenizer.decode(truncated_prompt_tokens, skip_special_tokens=True)\n",
    "        else:\n",
    "            truncated_prompt = prompt\n",
    "            completion_quota = max_length - len(prompt_tokens) - 2\n",
    "        \n",
    "        # ì™„ì„± ìë¥´ê¸° (ì•ìª½ ìœ ì§€)\n",
    "        if len(completion_tokens) > completion_quota:\n",
    "            truncated_completion_tokens = completion_tokens[:completion_quota]\n",
    "            truncated_completion = tokenizer.decode(truncated_completion_tokens, skip_special_tokens=True)\n",
    "        else:\n",
    "            truncated_completion = completion\n",
    "        \n",
    "        # # EOS í† í° ë³´ì¥\n",
    "        # if not truncated_completion.endswith('<|im_end|>'):\n",
    "        #     truncated_completion += '<|im_end|>'\n",
    "        \n",
    "        return {\"prompt\": truncated_prompt, \"completion\": truncated_completion}\n",
    "    \n",
    "    return dataset.map(truncate_sample, num_proc=1)\n",
    "\n",
    "\n",
    "# ëª¨ë“  ìƒ˜í”Œ ë³´ì¡´í•˜ë©° ìë¥´ê¸°\n",
    "final_train_ds = truncate_keep_all_samples(train_ds)\n",
    "final_eval_ds = truncate_keep_all_samples(eval_ds)\n",
    "\n",
    "print(f\"âœ… ì›ë³¸ í›ˆë ¨ ìƒ˜í”Œ: {len(train_ds):,}ê°œ\")\n",
    "print(f\"âœ… ìµœì¢… í›ˆë ¨ ìƒ˜í”Œ: {len(final_train_ds):,}ê°œ\")\n",
    "print(f\"âœ… ì œê±°ëœ ìƒ˜í”Œ: 0ê°œ (ëª¨ë“  ìƒ˜í”Œ ë³´ì¡´!)\")\n",
    "\n",
    "# ê¸¸ì´ ê²€ì¦\n",
    "max_lens = []\n",
    "for i in range(min(100, len(final_train_ds))):\n",
    "    sample = final_train_ds[i]\n",
    "    tokens = tokenizer.encode(sample[\"prompt\"] + sample[\"completion\"], add_special_tokens=True)\n",
    "    max_lens.append(len(tokens))\n",
    "\n",
    "print(f\"âœ… ìµœëŒ€ í† í° ìˆ˜: {max(max_lens)}\")\n",
    "print(f\"âœ… í‰ê·  í† í° ìˆ˜: {sum(max_lens)/len(max_lens):.1f}\")\n",
    "\n",
    "# ë³€ìˆ˜ ì—…ë°ì´íŠ¸\n",
    "train_ds = final_train_ds\n",
    "eval_ds = final_eval_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b911ac66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a9c39ee98e64380bf1f1cdc6fe143cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/9548 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "328af5b6b61a44f68ef9575d7989a545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/503 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize ì˜¤ë¥˜ í•´ê²°\n",
    "\n",
    "import unicodedata, re\n",
    "\n",
    "ZWS = \"\\u200b\"          # zero-width space\n",
    "NBSP = \"\\xa0\"           # non-breaking space\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = s.replace(\"\\r\\n\",\"\\n\").replace(\"\\r\",\"\\n\")   # ê°œí–‰ í†µì¼\n",
    "    s = s.replace(NBSP,\" \").replace(ZWS,\"\")         # ì´ìƒ ê³µë°± ì œê±°\n",
    "    s = unicodedata.normalize(\"NFKC\", s)            # ìœ ë‹ˆì½”ë“œ ì •ê·œí™”\n",
    "    # ë¬¸ìì—´ì— ì§ì ‘ ë°•ì•„ë‘” íŠ¹ìˆ˜í† í°ì€ ì œê±°(í† í¬ë‚˜ì´ì €ì— ë§¡ê¹€)\n",
    "    for tok in (\"<s>\",\"</s>\",\"<bos>\",\"</eos>\",\"<<SYS>>\",\"<<USER>>\",\"<<ASSISTANT>>\", \"<|im_end|>\"):\n",
    "        s = s.replace(tok,\"\")\n",
    "    return s\n",
    "\n",
    "def normalize_pair(ex):\n",
    "    p = clean_text(ex[\"prompt\"]).rstrip()           # ë’¤ ê³µë°±/ê°œí–‰ ì œê±°\n",
    "    c = clean_text(ex[\"completion\"]).lstrip()       # ì• ê³µë°± ì œê±°\n",
    "    if not p.endswith(\"\\n\"):                        # ê²½ê³„ ê°œí–‰ 1ê°œ ê°•ì œ\n",
    "        p += \"\\n\"\n",
    "    return {\"prompt\": p, \"completion\": c}\n",
    "\n",
    "train_ds = train_ds.map(normalize_pair, num_proc=4)\n",
    "eval_ds  = eval_ds.map(normalize_pair,  num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ff1ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'completion'],\n",
      "    num_rows: 9548\n",
      "}) Dataset({\n",
      "    features: ['prompt', 'completion'],\n",
      "    num_rows: 503\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_ds, eval_ds)\n",
    "print(train_ds[0])\n",
    "print(train_ds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82cb8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ë¼ë²¨ ìœ íš¨ì„± ì ê²€ ì…€ ===\n",
    "def _count_supervised_tokens(example):\n",
    "    # labelsì— -100ì´ ì•„ë‹Œ í† í°ì´ ìµœì†Œ 1ê°œëŠ” ìˆì–´ì•¼ í•™ìŠµ ê°€ëŠ¥\n",
    "    return {\"supervised_tokens\": int(sum(1 for t in example[\"labels\"] if t != -100))}\n",
    "\n",
    "tmp = train_ds.map(_count_supervised_tokens)\n",
    "num_bad = sum(1 for x in tmp[\"supervised_tokens\"] if x == 0)\n",
    "print(f\"[check] supervised tokens == 0 ì¸ ìƒ˜í”Œ ìˆ˜: {num_bad}\")\n",
    "\n",
    "assert num_bad == 0, \"labelsê°€ ì „ë¶€ -100ì¸ ìƒ˜í”Œì´ ìˆìŠµë‹ˆë‹¤. answer_start ì¸ë±ì‹±/ë¼ë²¨ ë§ˆìŠ¤í‚¹ì„ ì ê²€í•˜ì„¸ìš”.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecd1677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b96dd88cf1e421c9c0102265e5fc898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/9548 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "894ad12377f8400094537959244632cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/9548 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a877028a96486ba0f3be7566d7bb93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/9548 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2872ccaf03bb4356942d0d83230048ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/503 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9274a7b8b8404f83abcd7f8dae29feea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/503 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea94cadab1d2465d9526da98ae7c01a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/503 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "\n",
    "@dataclass\n",
    "class CompletionOnlyCollator:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    answer_marker: str = \"ë‹µë³€:\\n\"\n",
    "    max_length: int = 128\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        # 1) prompt + completion + EOS ë¡œ í•©ì¹˜ê¸°\n",
    "        eos = self.tokenizer.eos_token or \"\"\n",
    "        texts = [ex[\"prompt\"] + ex[\"completion\"] + eos for ex in features]\n",
    "\n",
    "        batch = self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=True,\n",
    "        )\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        # 2) \"prompt + answer_marker\" ê¸¸ì´ë§Œí¼ -100 ë§ˆìŠ¤í‚¹\n",
    "        for i, ex in enumerate(features):\n",
    "            prefix = ex[\"prompt\"] + self.answer_marker\n",
    "            prefix_ids = self.tokenizer(\n",
    "                prefix,\n",
    "                add_special_tokens=True,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\",\n",
    "            )[\"input_ids\"][0]\n",
    "\n",
    "            cut = min(len(prefix_ids), labels.size(1))\n",
    "            labels[i, :cut] = -100\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "# âš ï¸ ìƒì„±ì€ \"í‚¤ì›Œë“œ ì¸ì\"ë¡œ\n",
    "collator = CompletionOnlyCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    answer_marker=\"ë‹µë³€:\\n\",\n",
    "    max_length=tokenizer.model_max_length or 128\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    args=cfg,\n",
    "    data_collator=collator,\n",
    "    # data_collator=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfeb6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Callback\n",
    "\n",
    "from transformers import TrainerCallback\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import math\n",
    "\n",
    "class CustomCallback(TrainerCallback):\n",
    "    tz_seoul = pytz.timezone(\"Asia/Seoul\")\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and \"loss\" in logs:\n",
    "            now = datetime.now(self.tz_seoul).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            print(f\"[{now}] step {state.global_step}\\tloss {logs['loss']:.4f}\", flush=True)\n",
    "\n",
    "class ManualGradNormCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs.get(\"model\")\n",
    "        if model is None: return\n",
    "\n",
    "        total_sq = 0.0\n",
    "        for _, p in model.named_parameters():\n",
    "            if p.grad is not None:\n",
    "                g = p.grad.detach()\n",
    "                total_sq += float(g.norm(2)**2)\n",
    "        manual = math.sqrt(total_sq)\n",
    "\n",
    "        try:\n",
    "            import wandb\n",
    "            wandb.log({\"train/manual_grad_norm\": manual}, step=state.global_step)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "trainer.add_callback(CustomCallback())\n",
    "trainer.add_callback(ManualGradNormCallback())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c1fa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === í•œ ë°°ì¹˜ grad ì¡´ì¬ í™•ì¸ ì…€ ===\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "dl = DataLoader(train_ds, batch_size=1, shuffle=True, collate_fn=default_data_collator)\n",
    "batch = next(iter(dl))\n",
    "batch = {k: torch.tensor(v).to(model.device) for k, v in batch.items()}  # ì•ˆì „í•˜ê²Œ í…ì„œí™” + ë””ë°”ì´ìŠ¤ ì´ë™\n",
    "\n",
    "model.zero_grad(set_to_none=True)\n",
    "out = model(**batch)\n",
    "loss = out.loss\n",
    "print(\"[check] single-batch loss:\", float(loss))\n",
    "loss.backward()\n",
    "\n",
    "first = None\n",
    "for n, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        first = (n, p)\n",
    "        break\n",
    "\n",
    "if first is None:\n",
    "    raise RuntimeError(\"í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ê°€ ì—†ìŠµë‹ˆë‹¤. LoRA ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "\n",
    "n, p = first\n",
    "print(f\"[grad-check] first trainable: {n}\")\n",
    "print(\"[grad-check] grad is None? ->\", p.grad is None)\n",
    "if p.grad is not None:\n",
    "    print(\"[grad-check] grad norm ->\", float(p.grad.norm()))\n",
    "\n",
    "# ìˆ˜ë™ ì „ì²´ grad normë„ ê³„ì‚°\n",
    "total_norm_sq = 0.0\n",
    "for _, p in model.named_parameters():\n",
    "    if p.grad is not None:\n",
    "        g = p.grad.detach()\n",
    "        total_norm_sq += float(g.norm(2)**2)\n",
    "manual_grad_norm = total_norm_sq ** 0.5\n",
    "print(\"[grad-check] manual total grad_norm =\", manual_grad_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5596d587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ í›ˆë ¨ ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhc5754\u001b[0m (\u001b[33mbhc5754-hyalobio\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/fske/note/wandb/run-20250810_080040-tu5651gd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhc5754-hyalobio/huggingface/runs/tu5651gd' target=\"_blank\">ax-4.0-light-7b_qlora_0810_0759</a></strong> to <a href='https://wandb.ai/bhc5754-hyalobio/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhc5754-hyalobio/huggingface' target=\"_blank\">https://wandb.ai/bhc5754-hyalobio/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhc5754-hyalobio/huggingface/runs/tu5651gd' target=\"_blank\">https://wandb.ai/bhc5754-hyalobio/huggingface/runs/tu5651gd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='267' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [267/375 55:07 < 22:27, 0.08 it/s, Epoch 3.55/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>9.696300</td>\n",
       "      <td>10.047052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>9.162000</td>\n",
       "      <td>10.047052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-10 17:04:00] step 16\tloss 9.7146\n",
      "[2025-08-10 17:07:18] step 32\tloss 9.3173\n",
      "[2025-08-10 17:10:36] step 48\tloss 9.0676\n",
      "[2025-08-10 17:13:55] step 64\tloss 9.2231\n",
      "[2025-08-10 17:17:08] step 80\tloss 8.9972\n",
      "[2025-08-10 17:20:27] step 96\tloss 9.5488\n",
      "[2025-08-10 17:23:45] step 112\tloss 9.2084\n",
      "[2025-08-10 17:27:03] step 128\tloss 9.6963\n",
      "[2025-08-10 17:30:40] step 144\tloss 8.9933\n",
      "[2025-08-10 17:33:54] step 160\tloss 9.4604\n",
      "[2025-08-10 17:37:12] step 176\tloss 8.9456\n",
      "[2025-08-10 17:40:30] step 192\tloss 9.3939\n",
      "[2025-08-10 17:43:48] step 208\tloss 9.5084\n",
      "[2025-08-10 17:47:07] step 224\tloss 9.1312\n",
      "[2025-08-10 17:50:21] step 240\tloss 9.5661\n",
      "[2025-08-10 17:53:39] step 256\tloss 9.1620\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ¯ í›ˆë ¨ ì‹œì‘...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# ì–´ëŒ‘í„° ì €ì¥\u001b[39;00m\n\u001b[1;32m      5\u001b[0m adapter_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mselected_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_qlora/adapter\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2238\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2236\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2239\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2582\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2575\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2576\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2577\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2578\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2579\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2580\u001b[0m )\n\u001b[1;32m   2581\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2582\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2585\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2586\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2587\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2588\u001b[0m ):\n\u001b[1;32m   2589\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2590\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:904\u001b[0m, in \u001b[0;36mSFTTrainer.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaybe_activation_offload_context:\n\u001b[0;32m--> 904\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3845\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3842\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   3843\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 3845\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3847\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:2578\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2576\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2577\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2578\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._post_run_cell_hook of <wandb.sdk.wandb_init._WandbInit object at 0x727849cb26b0>> (for post_run_cell), with arguments args (<ExecutionResult object at 72784a3db250, execution_count=28 error_before_exec=None error_in_exec= info=<ExecutionInfo object at 72784a3db130, raw_cell=\"print(\"ğŸ¯ í›ˆë ¨ ì‹œì‘...\")\n",
      "trainer.train()\n",
      "\n",
      "# ì–´ëŒ‘í„° ì €ì¥\n",
      "adap..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226465762d706f64227d/workspace/fske/note/finetune_qlora.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py:593\u001b[0m, in \u001b[0;36m_WandbInit._post_run_cell_hook\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresuming backend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 593\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface.py:787\u001b[0m, in \u001b[0;36mInterfaceBase.publish_resume\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpublish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    786\u001b[0m     resume \u001b[38;5;241m=\u001b[39m pb\u001b[38;5;241m.\u001b[39mResumeRequest()\n\u001b[0;32m--> 787\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_shared.py:294\u001b[0m, in \u001b[0;36mInterfaceShared._publish_resume\u001b[0;34m(self, resume)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m, resume: pb\u001b[38;5;241m.\u001b[39mResumeRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    293\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(resume\u001b[38;5;241m=\u001b[39mresume)\n\u001b[0;32m--> 294\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_sock.py:39\u001b[0m, in \u001b[0;36mInterfaceSock._publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m, local: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign(record)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_record_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py:166\u001b[0m, in \u001b[0;36mSockClient.send_record_publish\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    164\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrequest_id \u001b[38;5;241m=\u001b[39m record\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mmailbox_slot\n\u001b[1;32m    165\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrecord_publish\u001b[38;5;241m.\u001b[39mCopyFrom(record)\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_server_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_req\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py:146\u001b[0m, in \u001b[0;36mSockClient.send_server_request\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_server_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg: spb\u001b[38;5;241m.\u001b[39mServerRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py:143\u001b[0m, in \u001b[0;36mSockClient._send_message\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    141\u001b[0m header \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<BI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m), raw_size)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendall_with_error_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py:122\u001b[0m, in \u001b[0;36mSockClient._sendall_with_error_handle\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    120\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 122\u001b[0m     sent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;66;03m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sent \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ¯ í›ˆë ¨ ì‹œì‘...\")\n",
    "trainer.train()\n",
    "\n",
    "# ì–´ëŒ‘í„° ì €ì¥\n",
    "adapter_dir = f\"../../models/{selected_model}_qlora/adapter\"\n",
    "os.makedirs(adapter_dir, exist_ok=True)\n",
    "\n",
    "trainer.model.save_pretrained(adapter_dir)\n",
    "tokenizer.save_pretrained(adapter_dir)\n",
    "\n",
    "print(f\"âœ… ì–´ëŒ‘í„° ì €ì¥ ì™„ë£Œ: {adapter_dir}\")\n",
    "print(f\"í›ˆë ¨ ì™„ë£Œ í›„ GPU ë©”ëª¨ë¦¬: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9958c663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í›ˆë ¨ ì™„ë£Œ í›„ ì–´ëŒ‘í„° ë³‘í•© (ì„ íƒì‚¬í•­)\n",
    "# í›ˆë ¨ì´ ì™„ë£Œë˜ë©´ ì´ ì…€ì„ ì‹¤í–‰í•˜ì—¬ ìµœì¢… ëª¨ë¸ì„ ìƒì„±í•˜ì„¸ìš”\n",
    "\n",
    "def merge_adapter():\n",
    "    \"\"\"ì–´ëŒ‘í„°ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ê³¼ ë³‘í•©í•˜ì—¬ ìµœì¢… ëª¨ë¸ ìƒì„±\"\"\"\n",
    "    \n",
    "    adapter_dir = f\"../../models/{selected_model}_qlora/adapter\"\n",
    "    merged_dir = f\"../../models/{selected_model}_qlora/merged\"\n",
    "    \n",
    "    print(\"ğŸ”„ ì–´ëŒ‘í„° ë³‘í•© ì‹œì‘...\")\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    import gc\n",
    "    if 'model' in globals(): del model\n",
    "    if 'base_model' in globals(): del base_model  \n",
    "    if 'trainer' in globals(): del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # ë² ì´ìŠ¤ ëª¨ë¸ ì¬ë¡œë”© (í’€í”„ë¦¬ì‹œì „)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=\"eager\"\n",
    "    )\n",
    "    \n",
    "    # ì–´ëŒ‘í„° ë³‘í•©\n",
    "    model = PeftModel.from_pretrained(model, adapter_dir)\n",
    "    model = model.merge_and_unload()\n",
    "    \n",
    "    # ì €ì¥\n",
    "    model.save_pretrained(merged_dir)\n",
    "    tokenizer.save_pretrained(merged_dir)\n",
    "    \n",
    "    print(f\"âœ… ìµœì¢… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {merged_dir}\")\n",
    "    return merged_dir\n",
    "\n",
    "# ì‚¬ìš©ë²•: merge_adapter() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ì‹¤í–‰\n",
    "\n",
    "merge_adapter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28135989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
