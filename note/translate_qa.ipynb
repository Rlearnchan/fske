{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b3ce014",
   "metadata": {},
   "source": [
    "# 한국어 금융보안 데이터셋 번역 작업\n",
    "\n",
    "## 작업 개요\n",
    "이 노트북은 영어/중국어로 작성된 금융보안 관련 QA 데이터셋을 한국어로 번역하는 작업을 수행합니다.\n",
    "\n",
    "## 대상 파일들\n",
    "1. **CyberMetric/mcqa_org.csv** - 영어 전용 MCQA 데이터\n",
    "2. **SecBench/mcqa_org.csv** - 영어/중국어 혼용 MCQA 데이터  \n",
    "3. **SecBench/qa_org.csv** - 영어/중국어 혼용 QA 데이터\n",
    "\n",
    "## 작업 흐름\n",
    "1. 각 `_org.csv` 파일을 읽어서 데이터 구조 및 언어 분포 확인\n",
    "2. **언어 자동 감지**: SecBench 데이터의 영어/중국어 혼용 상황 처리\n",
    "3. 로컬 LLM을 사용하여 한국어로 번역\n",
    "4. 번역된 결과를 원래 폴더에 한국어 버전으로 저장\n",
    "5. 번역 품질 검증\n",
    "\n",
    "## 주요 개선사항\n",
    "- **언어 자동 감지 기능**: SecBench 데이터의 영어/중국어 혼용 문제 해결\n",
    "- **개별 항목별 언어 판단**: 질문과 선택지마다 독립적으로 언어 감지\n",
    "- **이미 번역된 내용 건너뛰기**: 한국어 텍스트는 재번역하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "202d565b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라이브러리 설치 완료!\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 설치 및 설정\n",
    "\n",
    "# !pip install -q pandas tqdm \n",
    "# !pip install -q transformers==4.55.0 # llm requires >=4.46.0\n",
    "# !pip install -q safetensors==0.4.3 # downgrade for torch 2.1.0\n",
    "# !pip install -q bitsandbytes==0.43.2 accelerate==1.9.0 # quantization\n",
    "\n",
    "print(\"라이브러리 설치 완료!\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6556cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM 모델 로딩 중...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e649ec3232d43749162937f6af0294f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 '/workspace/models/exaone-4.0-32b' 로드 완료!\n"
     ]
    }
   ],
   "source": [
    "# LLM 모델 로드 및 번역 함수 정의\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "import unicodedata\n",
    "\n",
    "def load_translation_model(model_name=\"microsoft/DialoGPT-medium\"):\n",
    "    \"\"\"번역용 LLM 모델을 로드합니다.\"\"\"\n",
    "    try:\n",
    "        # 양자화 설정 (메모리 절약)\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\"\n",
    "        )\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            padding_side=\"left\"  # Decoder-only 모델용 left padding\n",
    "        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,\n",
    "            attn_implementation=\"eager\"  # SDPA 요구사항 우회\n",
    "        )\n",
    "        \n",
    "        # pad_token 설정\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "        print(f\"모델 '{model_name}' 로드 완료!\")\n",
    "        return tokenizer, model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"모델 로드 실패: {e}\")\n",
    "        print(\"대안: OpenAI API 또는 다른 번역 서비스를 사용하세요.\")\n",
    "        return None, None\n",
    "\n",
    "def detect_language(text: str) -> str:\n",
    "    \"\"\"텍스트의 언어를 자동 감지합니다.\"\"\"\n",
    "    \n",
    "    # 한국어는 이미 번역 완료된 것으로 간주\n",
    "    korean_chars = sum(1 for char in text if '\\uac00' <= char <= '\\ud7af')\n",
    "    if korean_chars > len(text) * 0.1:  # 10% 이상이 한글이면 한국어\n",
    "        return \"ko\"\n",
    "    \n",
    "    # 중국어 문자 감지 (간체/번체 포함)\n",
    "    chinese_chars = sum(1 for char in text if '\\u4e00' <= char <= '\\u9fff')\n",
    "    \n",
    "    # 영어 문자 감지 (알파벳)\n",
    "    english_chars = sum(1 for char in text if char.isascii() and char.isalpha())\n",
    "    \n",
    "    # 전체 텍스트 길이\n",
    "    total_chars = len(text.replace(' ', '').replace('\\n', ''))\n",
    "    \n",
    "    if total_chars == 0:\n",
    "        return \"unknown\"\n",
    "    \n",
    "    # 중국어 비율이 높으면 중국어\n",
    "    if chinese_chars > total_chars * 0.3:\n",
    "        return \"zh\"\n",
    "    \n",
    "    # 영어 비율이 높으면 영어\n",
    "    if english_chars > total_chars * 0.5:\n",
    "        return \"en\"\n",
    "    \n",
    "    # 혼용되어 있는 경우 더 많은 쪽으로 판단\n",
    "    if chinese_chars > english_chars:\n",
    "        return \"zh\"\n",
    "    elif english_chars > chinese_chars:\n",
    "        return \"en\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "\n",
    "def clean_translation_result(text: str) -> str:\n",
    "    \"\"\"번역 결과에서 불필요한 요소들을 제거합니다.\"\"\"\n",
    "    # 프롬프트 잔여물 및 메타 설명 제거\n",
    "    unwanted_patterns = [\n",
    "        r\"English:\\s*\", r\"Korean:\\s*\", r\"Chinese:\\s*\", r\"Korean translation:\\s*\",\n",
    "        r\"---+\", r\"\\*\\*.*?\\*\\*\", r\"Final Answer?:?\",\n",
    "        r\"번역:\\s*\", r\"답변:\\s*\", r\"결과:\\s*\",\n",
    "        r\"The original.*?requested\\.\", r\"However.*?ask!\",\n",
    "        r\"I will follow.*?format\\.\", r\"The translation.*?style\\.\",\n",
    "        r\"The provided text.*?terminology\\.\", r\"If you need.*?ask!\",\n",
    "        r\"Translate.*?Korean\", r\"Translation.*?format\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in unwanted_patterns:\n",
    "        text = re.sub(pattern, \"\", text, flags=re.IGNORECASE | re.DOTALL)\n",
    "    \n",
    "    # 음성 번역 패턴 제거 (한국어 + 괄호 안 영어 발음)\n",
    "    # 예: \"정보 보증 (Jeongbo bojung)\" → \"정보 보증\"\n",
    "    text = re.sub(r'\\s*\\([A-Za-z\\s-]+\\)', '', text)\n",
    "    \n",
    "    # 영어 설명문 제거 (긴 영어 문장들)\n",
    "    sentences = text.split('.')\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if sentence:\n",
    "            # 영어 비율이 높은 문장 제거\n",
    "            english_chars = sum(1 for char in sentence if char.isascii() and char.isalpha())\n",
    "            total_chars = len(sentence.replace(' ', ''))\n",
    "            if total_chars > 0 and english_chars / total_chars < 0.7:  # 영어 비율 70% 미만만 유지\n",
    "                cleaned_sentences.append(sentence)\n",
    "    \n",
    "    text = '. '.join(cleaned_sentences)\n",
    "    \n",
    "    # 연속된 공백과 줄바꿈 정리\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n', text)  # 빈 줄 제거\n",
    "    text = re.sub(r'\\s+', ' ', text)  # 연속 공백을 하나로\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def translate_text(text: str, tokenizer, model, source_lang: str = \"auto\", target_lang: str = \"ko\") -> str:\n",
    "    \"\"\"텍스트를 한국어로 번역합니다. source_lang이 'auto'면 자동 감지합니다.\"\"\"\n",
    "    \n",
    "    # 언어 자동 감지\n",
    "    if source_lang == \"auto\":\n",
    "        detected_lang = detect_language(text)\n",
    "        if detected_lang == \"ko\":\n",
    "            return text  # 이미 한국어면 번역하지 않음\n",
    "        elif detected_lang == \"unknown\":\n",
    "            # 알 수 없는 경우 영어로 가정\n",
    "            detected_lang = \"en\"\n",
    "        source_lang = detected_lang\n",
    "    \n",
    "    # 의미 번역을 강조하는 프롬프트 (음성 번역 방지)\n",
    "    if source_lang == \"en\":\n",
    "        prompt = f\"Translate this English text to Korean with proper meaning (not pronunciation). Use standard Korean terminology:\\n\\n{text}\\n\\nKorean translation:\"\n",
    "    elif source_lang == \"zh\":\n",
    "        prompt = f\"Translate this Chinese text to Korean with proper meaning (not pronunciation). Use standard Korean terminology:\\n\\n{text}\\n\\nKorean translation:\"\n",
    "    else:\n",
    "        prompt = f\"Translate to Korean with proper meaning (not pronunciation):\\n\\n{text}\\n\\nKorean translation:\"\n",
    "    \n",
    "    try:\n",
    "        # 토큰화 (attention_mask 포함)\n",
    "        inputs = tokenizer(\n",
    "            prompt, \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True, \n",
    "            max_length=512,\n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        # 입력 텐서를 모델과 같은 디바이스로 이동\n",
    "        device = next(model.parameters()).device\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        \n",
    "        # 생성 (GPU 최적화된 파라미터)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,  # input_ids와 attention_mask 모두 전달\n",
    "                max_new_tokens=100,  # 더 짧게 (속도 향상)\n",
    "                num_return_sequences=1,\n",
    "                temperature=0.2,  # 더 결정적으로 (속도 향상)\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.1,  # 반복 방지\n",
    "                use_cache=True  # 캐시 사용으로 속도 향상\n",
    "            )\n",
    "        \n",
    "        # 디코딩\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # 번역 결과 추출 (개선된 로직)\n",
    "        if \"Korean translation:\" in generated_text:\n",
    "            translated = generated_text.split(\"Korean translation:\")[-1].strip()\n",
    "        elif \"Korean:\" in generated_text:\n",
    "            translated = generated_text.split(\"Korean:\")[-1].strip()\n",
    "        else:\n",
    "            # 프롬프트 제거\n",
    "            translated = generated_text[len(prompt):].strip()\n",
    "        \n",
    "        # 결과 정제\n",
    "        translated = clean_translation_result(translated)\n",
    "        \n",
    "        # 영어 비율이 너무 높으면 원문 반환 (번역 실패로 간주)\n",
    "        if translated:\n",
    "            english_chars = sum(1 for char in translated if char.isascii() and char.isalpha())\n",
    "            korean_chars = sum(1 for char in translated if '\\uac00' <= char <= '\\ud7af')\n",
    "            total_chars = len(translated.replace(' ', '').replace('\\n', ''))\n",
    "            \n",
    "            if total_chars > 0:\n",
    "                english_ratio = english_chars / total_chars\n",
    "                korean_ratio = korean_chars / total_chars\n",
    "                \n",
    "                # 영어 비율이 50% 이상이거나 한국어가 10% 미만이면 번역 실패로 간주\n",
    "                if english_ratio > 0.5 or korean_ratio < 0.1:\n",
    "                    print(f\"번역 실패 (영어:{english_ratio:.1%}, 한국어:{korean_ratio:.1%}): {translated[:50]}...\")\n",
    "                    return text  # 원문 반환\n",
    "        \n",
    "        # 빈 결과나 너무 짧은 결과 방지\n",
    "        if not translated or len(translated.strip()) < 3:\n",
    "            return text  # 원문 반환\n",
    "            \n",
    "        return translated\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"번역 오류: {e}\")\n",
    "        return text  # 오류시 원문 반환\n",
    "\n",
    "# 모델 로드 (실제 사용시 적절한 모델명으로 변경)\n",
    "print(\"LLM 모델 로딩 중...\")\n",
    "tokenizer, model = load_translation_model(\"/workspace/models/exaone-4.0-32b\")  # 예시 모델명\n",
    "# print(\"실제 사용시 적절한 모델명을 설정하세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70ce2dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CyberMetric MCQA 데이터 ===\n",
      "데이터 크기: (10180, 2)\n",
      "컬럼: ['Question', 'Answer']\n",
      "\n",
      "샘플 데이터:\n",
      "                                            Question Answer\n",
      "0  Which of the following refers to the secrecy o...  답변: 4\n",
      "1  Which type of authentication uses multiple fac...  답변: 3\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== SecBench MCQA 데이터 ===\n",
      "데이터 크기: (2730, 2)\n",
      "컬럼: ['Question', 'Answer']\n",
      "\n",
      "샘플 데이터:\n",
      "                                            Question Answer\n",
      "0  以下哪种措施是企业为了遵守数据保护法规，通常会实施的？\\n1. 定期进行网络渗透测试\\n2....  답변: 2\n",
      "1  关于个人数据的权益，根据GDPR，数据主体有权：\\n1. 随时要求访问、更正或删除他们的个人...  답변: 1\n",
      "\n",
      "언어 분포 분석:\n",
      "샘플 100개 중 언어 분포: {'zh': 92, 'en': 8}\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== SecBench QA 데이터 ===\n",
      "데이터 크기: (270, 2)\n",
      "컬럼: ['Question', 'Answer']\n",
      "\n",
      "샘플 데이터:\n",
      "                   Question                                             Answer\n",
      "0  为什么现代操作系统的IP ID通常是随机产生的？                               为了提高安全性，防止某些类型的网络攻击。\n",
      "1   ECA技术如何帮助企业保护其网络免受内部威胁？  ECA技术通过分析内部网络流量的特征，可以识别异常行为和潜在的内部威胁，从而帮助企业加强内部...\n",
      "\n",
      "언어 분포 분석:\n",
      "QA 샘플 100개 중 언어 분포: {'zh': 97, 'en': 3}\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드 및 구조 확인\n",
    "\n",
    "# 1. CyberMetric MCQA (영어 전용)\n",
    "print(\"=== CyberMetric MCQA 데이터 ===\")\n",
    "cyber_mcqa = pd.read_csv('../data/CyberMetric/mcqa_org.csv')\n",
    "print(f\"데이터 크기: {cyber_mcqa.shape}\")\n",
    "print(f\"컬럼: {cyber_mcqa.columns.tolist()}\")\n",
    "print(\"\\n샘플 데이터:\")\n",
    "print(cyber_mcqa.head(2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# 2. SecBench MCQA (영어/중국어 혼용)\n",
    "print(\"=== SecBench MCQA 데이터 ===\")\n",
    "sec_mcqa = pd.read_csv('../data/SecBench/mcqa_org.csv')\n",
    "print(f\"데이터 크기: {sec_mcqa.shape}\")\n",
    "print(f\"컬럼: {sec_mcqa.columns.tolist()}\")\n",
    "print(\"\\n샘플 데이터:\")\n",
    "print(sec_mcqa.head(2))\n",
    "\n",
    "# 언어 분포 확인\n",
    "print(\"\\n언어 분포 분석:\")\n",
    "languages = []\n",
    "for idx in range(min(100, len(sec_mcqa))):  # 처음 100개만 샘플링\n",
    "    question = sec_mcqa.iloc[idx]['Question']\n",
    "    lang = detect_language(question.split('\\n')[0])  # 첫 번째 줄만 검사\n",
    "    languages.append(lang)\n",
    "\n",
    "from collections import Counter\n",
    "lang_count = Counter(languages)\n",
    "print(f\"샘플 100개 중 언어 분포: {dict(lang_count)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# 3. SecBench QA (영어/중국어 혼용)\n",
    "print(\"=== SecBench QA 데이터 ===\")\n",
    "sec_qa = pd.read_csv('../data/SecBench/qa_org.csv')\n",
    "print(f\"데이터 크기: {sec_qa.shape}\")\n",
    "print(f\"컬럼: {sec_qa.columns.tolist()}\")\n",
    "print(\"\\n샘플 데이터:\")\n",
    "print(sec_qa.head(2))\n",
    "\n",
    "# 언어 분포 확인\n",
    "print(\"\\n언어 분포 분석:\")\n",
    "qa_languages = []\n",
    "for idx in range(min(100, len(sec_qa))):  # 처음 100개만 샘플링\n",
    "    question = sec_qa.iloc[idx]['Question']\n",
    "    lang = detect_language(question)\n",
    "    qa_languages.append(lang)\n",
    "\n",
    "qa_lang_count = Counter(qa_languages)\n",
    "print(f\"QA 샘플 100개 중 언어 분포: {dict(qa_lang_count)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ca553bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "번역 함수 정의 완료!\n"
     ]
    }
   ],
   "source": [
    "# MCQA 데이터 번역 함수\n",
    "\n",
    "def parse_mcqa_question(question_text: str) -> Dict:\n",
    "    \"\"\"MCQA 질문을 파싱하여 문제와 선택지를 분리합니다.\"\"\"\n",
    "    lines = question_text.strip().split('\\n')\n",
    "    \n",
    "    # 첫 번째 줄은 문제 \n",
    "    question = lines[0]\n",
    "    \n",
    "    # 나머지는 선택지\n",
    "    choices = []\n",
    "    for line in lines[1:]:\n",
    "        line = line.strip()\n",
    "        if line and (line.startswith(('1.', '2.', '3.', '4.', '5.', '6.', '7.'))):\n",
    "            choices.append(line)\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'choices': choices\n",
    "    }\n",
    "\n",
    "def translate_mcqa_entry(question_text: str, answer: str, tokenizer, model, source_lang: str = \"auto\") -> Tuple[str, str]:\n",
    "    \"\"\"MCQA 엔트리를 번역합니다. 언어 자동 감지 지원.\"\"\"\n",
    "    \n",
    "    # 질문 파싱\n",
    "    parsed = parse_mcqa_question(question_text)\n",
    "    \n",
    "    # 질문 번역 (언어 자동 감지)\n",
    "    if source_lang == \"auto\":\n",
    "        detected_lang = detect_language(parsed['question'])\n",
    "        # print(f\"질문 언어 감지: {detected_lang}\")  # 로그 줄임\n",
    "    else:\n",
    "        detected_lang = source_lang\n",
    "    \n",
    "    # 질문 번역\n",
    "    translated_question = translate_text(parsed['question'], tokenizer, model, detected_lang)\n",
    "    \n",
    "    # 각 선택지 번역\n",
    "    translated_choices = []\n",
    "    for choice in parsed['choices']:\n",
    "        # 선택지 번호와 내용 분리\n",
    "        if '. ' in choice:\n",
    "            num_part, content = choice.split('. ', 1)\n",
    "            # 각 선택지마다 언어 감지 (혼용 가능성)\n",
    "            choice_lang = detect_language(content) if source_lang == \"auto\" else detected_lang\n",
    "            translated_content = translate_text(content, tokenizer, model, choice_lang)\n",
    "            # 번역 결과 정제\n",
    "            translated_content = clean_translation_result(translated_content)\n",
    "            translated_choices.append(f\"{num_part}. {translated_content}\")\n",
    "        else:\n",
    "            choice_lang = detect_language(choice) if source_lang == \"auto\" else detected_lang\n",
    "            translated_choice = translate_text(choice, tokenizer, model, choice_lang)\n",
    "            translated_choice = clean_translation_result(translated_choice)\n",
    "            translated_choices.append(translated_choice)\n",
    "    \n",
    "    # 번역된 질문과 선택지 결합 (형식 개선)\n",
    "    if translated_choices:\n",
    "        translated_full = translated_question + '\\n' + '\\n'.join(translated_choices)\n",
    "    else:\n",
    "        translated_full = translated_question\n",
    "    \n",
    "    # 답변은 그대로 유지 (번호나 간단한 형태)\n",
    "    translated_answer = answer\n",
    "    \n",
    "    return translated_full, translated_answer\n",
    "\n",
    "def translate_qa_entry(question: str, answer: str, tokenizer, model, source_lang: str = \"auto\") -> Tuple[str, str]:\n",
    "    \"\"\"QA 엔트리를 번역합니다. 언어 자동 감지 지원.\"\"\"\n",
    "    \n",
    "    # 질문과 답변 각각 언어 감지\n",
    "    if source_lang == \"auto\":\n",
    "        q_lang = detect_language(question)\n",
    "        a_lang = detect_language(answer)\n",
    "        # print(f\"질문 언어: {q_lang}, 답변 언어: {a_lang}\")  # 로그 줄임\n",
    "    else:\n",
    "        q_lang = a_lang = source_lang\n",
    "    \n",
    "    # 번역 및 정제\n",
    "    translated_question = translate_text(question, tokenizer, model, q_lang)\n",
    "    translated_question = clean_translation_result(translated_question)\n",
    "    \n",
    "    translated_answer = translate_text(answer, tokenizer, model, a_lang)\n",
    "    translated_answer = clean_translation_result(translated_answer)\n",
    "    \n",
    "    return translated_question, translated_answer\n",
    "\n",
    "print(\"번역 함수 정의 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb68a5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 고속 번역 함수 로드 완료!\n"
     ]
    }
   ],
   "source": [
    "# 고속 배치 번역 함수들\n",
    "\n",
    "def translate_batch_texts(texts: List[str], tokenizer, model, source_lang: str = \"en\", batch_size: int = 8, save_interval: int = 1000, checkpoint_prefix: str = \"checkpoint\"):\n",
    "    \"\"\"여러 텍스트를 배치로 번역합니다 (중간 저장 기능 포함).\"\"\"\n",
    "    results = []\n",
    "    checkpoint_file = f\"/workspace/fske/data/{checkpoint_prefix}_{source_lang}_progress.json\"\n",
    "    \n",
    "    # 기존 체크포인트 로드\n",
    "    start_idx = 0\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        try:\n",
    "            with open(checkpoint_file, 'r', encoding='utf-8') as f:\n",
    "                checkpoint = json.load(f)\n",
    "            results = checkpoint.get('results', [])\n",
    "            start_idx = len(results)\n",
    "            print(f\"📂 체크포인트 발견! {start_idx}개 항목부터 재시작합니다.\")\n",
    "        except Exception as e:\n",
    "            print(f\"체크포인트 로드 오류: {e}, 처음부터 시작합니다.\")\n",
    "    \n",
    "    print(f\"📦 {len(texts)}개 텍스트를 {batch_size}개씩 배치 번역 중... (시작: {start_idx})\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in tqdm(range(start_idx, len(texts), batch_size), desc=\"🚀 고속 배치 번역\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        batch_results = []\n",
    "        \n",
    "        for text in batch_texts:\n",
    "            try:\n",
    "                translated = translate_text(text, tokenizer, model, source_lang)\n",
    "                batch_results.append(translated)\n",
    "            except Exception as e:\n",
    "                print(f\"배치 번역 오류: {e}\")\n",
    "                batch_results.append(text)  # 오류시 원문 반환\n",
    "        \n",
    "        results.extend(batch_results)\n",
    "        \n",
    "        # 중간 저장 (save_interval마다)\n",
    "        if len(results) % save_interval == 0 or len(results) == len(texts):\n",
    "            checkpoint_data = {\n",
    "                'results': results,\n",
    "                'total': len(texts),\n",
    "                'completed': len(results),\n",
    "                'timestamp': time.time(),\n",
    "                'source_lang': source_lang\n",
    "            }\n",
    "            try:\n",
    "                with open(checkpoint_file, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)\n",
    "                print(f\"💾 체크포인트 저장: {len(results)}/{len(texts)}개 완료\")\n",
    "            except Exception as e:\n",
    "                print(f\"체크포인트 저장 오류: {e}\")\n",
    "        \n",
    "        # 진행률 및 속도 표시\n",
    "        if (i + batch_size) % (save_interval // 10) == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            speed = (len(results) - start_idx) / elapsed if elapsed > 0 else 0\n",
    "            remaining = (len(texts) - len(results)) / speed if speed > 0 else 0\n",
    "            print(f\"⚡ 진행률: {len(results)}/{len(texts)} ({len(results)/len(texts)*100:.1f}%) | \"\n",
    "                  f\"속도: {speed:.1f}개/초 | 남은시간: {remaining/60:.1f}분\")\n",
    "        \n",
    "        # GPU 메모리 정리\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # 완료 후 체크포인트 파일 삭제\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        os.remove(checkpoint_file)\n",
    "        print(\"🗑️ 체크포인트 파일 정리 완료\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"🎉 배치 번역 완료! 총 시간: {total_time/60:.1f}분, 평균 속도: {(len(results)-start_idx)/total_time:.1f}개/초\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def translate_cybermetric_mcqa_fast(df: pd.DataFrame, tokenizer, model, batch_size: int = 16):\n",
    "    \"\"\"CyberMetric MCQA 데이터를 고속으로 번역합니다.\"\"\"\n",
    "    \n",
    "    print(f\"🚀 CyberMetric MCQA 총 {len(df)}개 항목 고속 번역 시작...\")\n",
    "    print(f\"💡 배치 크기: {batch_size}, GPU 메모리: {torch.cuda.get_device_properties(0).total_memory/1024**3:.1f}GB\")\n",
    "    \n",
    "    # 1단계: 데이터 전처리 - 질문과 선택지 분리\n",
    "    all_questions = []\n",
    "    all_choices = []\n",
    "    choice_mapping = []  # 어떤 질문에 속하는지 매핑\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), desc=\"📋 데이터 전처리\", total=len(df)):\n",
    "        question_text = row['Question']\n",
    "        parsed = parse_mcqa_question(question_text)\n",
    "        \n",
    "        # 질문 본문 추가\n",
    "        all_questions.append(parsed['question'])\n",
    "        \n",
    "        # 각 선택지 추가 및 매핑 저장\n",
    "        choice_start_idx = len(all_choices)\n",
    "        for choice in parsed['choices']:\n",
    "            if '. ' in choice:\n",
    "                num_part, content = choice.split('. ', 1)\n",
    "                all_choices.append(content)\n",
    "            else:\n",
    "                all_choices.append(choice)\n",
    "        \n",
    "        choice_mapping.append({\n",
    "            'start_idx': choice_start_idx,\n",
    "            'count': len(parsed['choices']),\n",
    "            'original_choices': parsed['choices']\n",
    "        })\n",
    "    \n",
    "    # 2단계: 배치 번역 실행 (중간 저장 포함)\n",
    "    print(\"📝 질문 배치 번역 중...\")\n",
    "    translated_questions = translate_batch_texts(all_questions, tokenizer, model, \"en\", batch_size, 500, \"cybermetric_questions\")\n",
    "    \n",
    "    print(\"📋 선택지 배치 번역 중...\")\n",
    "    translated_choices = translate_batch_texts(all_choices, tokenizer, model, \"en\", batch_size, 1000, \"cybermetric_choices\")\n",
    "    \n",
    "    # 3단계: 결과 재조립\n",
    "    print(\"🔧 결과 조립 중...\")\n",
    "    translated_data = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), desc=\"🔧 결과 조립\", total=len(df)):\n",
    "        try:\n",
    "            # 번역된 질문 가져오기\n",
    "            translated_question = translated_questions[idx]\n",
    "            \n",
    "            # 번역된 선택지 조립\n",
    "            mapping = choice_mapping[idx]\n",
    "            translated_choice_list = []\n",
    "            \n",
    "            for i, original_choice in enumerate(mapping['original_choices']):\n",
    "                choice_idx = mapping['start_idx'] + i\n",
    "                \n",
    "                if '. ' in original_choice:\n",
    "                    num_part, content = original_choice.split('. ', 1)\n",
    "                    translated_content = translated_choices[choice_idx]\n",
    "                    translated_choice_list.append(f\"{num_part}. {translated_content}\")\n",
    "                else:\n",
    "                    translated_choice_list.append(translated_choices[choice_idx])\n",
    "            \n",
    "            # 최종 조립\n",
    "            if translated_choice_list:\n",
    "                full_question = translated_question + '\\\\n' + '\\\\n'.join(translated_choice_list)\n",
    "            else:\n",
    "                full_question = translated_question\n",
    "            \n",
    "            translated_data.append({\n",
    "                'Question': full_question,\n",
    "                'Answer': row['Answer']\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"조립 오류 (행 {idx}): {e}\")\n",
    "            translated_data.append({\n",
    "                'Question': row['Question'], \n",
    "                'Answer': row['Answer']\n",
    "            })\n",
    "    \n",
    "    # 최종 결과 중간 저장\n",
    "    final_checkpoint = {\n",
    "        'data': translated_data,\n",
    "        'completed_at': time.time(),\n",
    "        'total_items': len(df)\n",
    "    }\n",
    "    try:\n",
    "        with open(f\"/workspace/fske/data/cybermetric_mcqa_final.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(final_checkpoint, f, ensure_ascii=False, indent=2)\n",
    "        print(\"💾 최종 결과 저장 완료!\")\n",
    "    except Exception as e:\n",
    "        print(f\"최종 저장 오류: {e}\")\n",
    "    \n",
    "    print(\"✅ 고속 번역 완료!\")\n",
    "    return pd.DataFrame(translated_data)\n",
    "\n",
    "def translate_secbench_mcqa_fast(df: pd.DataFrame, tokenizer, model, batch_size: int = 16):\n",
    "    \"\"\"SecBench MCQA 데이터를 고속으로 번역합니다 (영어/중국어 혼용).\"\"\"\n",
    "    \n",
    "    print(f\"🚀 SecBench MCQA 총 {len(df)}개 항목 고속 번역 시작...\")\n",
    "    print(f\"💡 배치 크기: {batch_size}, 언어 자동 감지 활성화\")\n",
    "    \n",
    "    # 1단계: 데이터 전처리 - 질문과 선택지 분리 + 언어 감지\n",
    "    all_questions = []\n",
    "    all_choices = []\n",
    "    choice_mapping = []\n",
    "    question_langs = []\n",
    "    choice_langs = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), desc=\"📋 데이터 전처리 및 언어 감지\", total=len(df)):\n",
    "        question_text = row['Question']\n",
    "        parsed = parse_mcqa_question(question_text)\n",
    "        \n",
    "        # 질문 언어 감지 및 추가\n",
    "        q_lang = detect_language(parsed['question'])\n",
    "        all_questions.append(parsed['question'])\n",
    "        question_langs.append(q_lang)\n",
    "        \n",
    "        # 각 선택지 언어 감지 및 추가\n",
    "        choice_start_idx = len(all_choices)\n",
    "        for choice in parsed['choices']:\n",
    "            if '. ' in choice:\n",
    "                num_part, content = choice.split('. ', 1)\n",
    "                c_lang = detect_language(content)\n",
    "                all_choices.append(content)\n",
    "                choice_langs.append(c_lang)\n",
    "            else:\n",
    "                c_lang = detect_language(choice)\n",
    "                all_choices.append(choice)\n",
    "                choice_langs.append(c_lang)\n",
    "        \n",
    "        choice_mapping.append({\n",
    "            'start_idx': choice_start_idx,\n",
    "            'count': len(parsed['choices']),\n",
    "            'original_choices': parsed['choices']\n",
    "        })\n",
    "    \n",
    "    # 2단계: 언어별로 그룹화하여 배치 번역\n",
    "    print(\"📝 질문 언어별 배치 번역 중...\")\n",
    "    \n",
    "    # 영어 질문들\n",
    "    en_questions = [(i, q) for i, (q, lang) in enumerate(zip(all_questions, question_langs)) if lang == 'en']\n",
    "    zh_questions = [(i, q) for i, (q, lang) in enumerate(zip(all_questions, question_langs)) if lang == 'zh']\n",
    "    ko_questions = [(i, q) for i, (q, lang) in enumerate(zip(all_questions, question_langs)) if lang == 'ko']\n",
    "    \n",
    "    translated_questions = [''] * len(all_questions)\n",
    "    \n",
    "    if en_questions:\n",
    "        print(f\"  🇺🇸 영어 질문 {len(en_questions)}개 번역 중...\")\n",
    "        en_texts = [q for _, q in en_questions]\n",
    "        en_translated = translate_batch_texts(en_texts, tokenizer, model, \"en\", batch_size)\n",
    "        for (idx, _), trans in zip(en_questions, en_translated):\n",
    "            translated_questions[idx] = trans\n",
    "    \n",
    "    if zh_questions:\n",
    "        print(f\"  🇨🇳 중국어 질문 {len(zh_questions)}개 번역 중...\")\n",
    "        zh_texts = [q for _, q in zh_questions]\n",
    "        zh_translated = translate_batch_texts(zh_texts, tokenizer, model, \"zh\", batch_size)\n",
    "        for (idx, _), trans in zip(zh_questions, zh_translated):\n",
    "            translated_questions[idx] = trans\n",
    "    \n",
    "    if ko_questions:\n",
    "        print(f\"  🇰🇷 한국어 질문 {len(ko_questions)}개 (번역 생략)...\")\n",
    "        for idx, q in ko_questions:\n",
    "            translated_questions[idx] = q\n",
    "    \n",
    "    # 선택지도 동일하게 처리\n",
    "    print(\"📋 선택지 언어별 배치 번역 중...\")\n",
    "    \n",
    "    en_choices = [(i, c) for i, (c, lang) in enumerate(zip(all_choices, choice_langs)) if lang == 'en']\n",
    "    zh_choices = [(i, c) for i, (c, lang) in enumerate(zip(all_choices, choice_langs)) if lang == 'zh']\n",
    "    ko_choices = [(i, c) for i, (c, lang) in enumerate(zip(all_choices, choice_langs)) if lang == 'ko']\n",
    "    \n",
    "    translated_choices = [''] * len(all_choices)\n",
    "    \n",
    "    if en_choices:\n",
    "        print(f\"  🇺🇸 영어 선택지 {len(en_choices)}개 번역 중...\")\n",
    "        en_texts = [c for _, c in en_choices]\n",
    "        en_translated = translate_batch_texts(en_texts, tokenizer, model, \"en\", batch_size)\n",
    "        for (idx, _), trans in zip(en_choices, en_translated):\n",
    "            translated_choices[idx] = trans\n",
    "    \n",
    "    if zh_choices:\n",
    "        print(f\"  🇨🇳 중국어 선택지 {len(zh_choices)}개 번역 중...\")\n",
    "        zh_texts = [c for _, c in zh_choices]\n",
    "        zh_translated = translate_batch_texts(zh_texts, tokenizer, model, \"zh\", batch_size)\n",
    "        for (idx, _), trans in zip(zh_choices, zh_translated):\n",
    "            translated_choices[idx] = trans\n",
    "    \n",
    "    if ko_choices:\n",
    "        print(f\"  🇰🇷 한국어 선택지 {len(ko_choices)}개 (번역 생략)...\")\n",
    "        for idx, c in ko_choices:\n",
    "            translated_choices[idx] = c\n",
    "    \n",
    "    # 3단계: 결과 재조립\n",
    "    print(\"🔧 결과 조립 중...\")\n",
    "    translated_data = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), desc=\"🔧 결과 조립\", total=len(df)):\n",
    "        try:\n",
    "            # 번역된 질문 가져오기\n",
    "            translated_question = translated_questions[idx]\n",
    "            \n",
    "            # 번역된 선택지 조립\n",
    "            mapping = choice_mapping[idx]\n",
    "            translated_choice_list = []\n",
    "            \n",
    "            for i, original_choice in enumerate(mapping['original_choices']):\n",
    "                choice_idx = mapping['start_idx'] + i\n",
    "                \n",
    "                if '. ' in original_choice:\n",
    "                    num_part, content = original_choice.split('. ', 1)\n",
    "                    translated_content = translated_choices[choice_idx]\n",
    "                    translated_choice_list.append(f\"{num_part}. {translated_content}\")\n",
    "                else:\n",
    "                    translated_choice_list.append(translated_choices[choice_idx])\n",
    "            \n",
    "            # 최종 조립\n",
    "            if translated_choice_list:\n",
    "                full_question = translated_question + '\\\\n' + '\\\\n'.join(translated_choice_list)\n",
    "            else:\n",
    "                full_question = translated_question\n",
    "            \n",
    "            translated_data.append({\n",
    "                'Question': full_question,\n",
    "                'Answer': row['Answer']\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"조립 오류 (행 {idx}): {e}\")\n",
    "            translated_data.append({\n",
    "                'Question': row['Question'], \n",
    "                'Answer': row['Answer']\n",
    "            })\n",
    "    \n",
    "    print(\"✅ SecBench MCQA 고속 번역 완료!\")\n",
    "    return pd.DataFrame(translated_data)\n",
    "\n",
    "def translate_secbench_qa_fast(df: pd.DataFrame, tokenizer, model, batch_size: int = 16):\n",
    "    \"\"\"SecBench QA 데이터를 고속으로 번역합니다 (영어/중국어 혼용).\"\"\"\n",
    "    \n",
    "    print(f\"🚀 SecBench QA 총 {len(df)}개 항목 고속 번역 시작...\")\n",
    "    print(f\"💡 배치 크기: {batch_size}, 언어 자동 감지 활성화\")\n",
    "    \n",
    "    # 1단계: 언어 감지 및 분류\n",
    "    questions = df['Question'].tolist()\n",
    "    answers = df['Answer'].tolist()\n",
    "    \n",
    "    question_langs = [detect_language(q) for q in tqdm(questions, desc=\"📋 질문 언어 감지\")]\n",
    "    answer_langs = [detect_language(a) for a in tqdm(answers, desc=\"📋 답변 언어 감지\")]\n",
    "    \n",
    "    # 2단계: 언어별 배치 번역\n",
    "    print(\"📝 질문 언어별 배치 번역 중...\")\n",
    "    \n",
    "    # 질문 번역\n",
    "    en_questions = [(i, q) for i, (q, lang) in enumerate(zip(questions, question_langs)) if lang == 'en']\n",
    "    zh_questions = [(i, q) for i, (q, lang) in enumerate(zip(questions, question_langs)) if lang == 'zh']\n",
    "    ko_questions = [(i, q) for i, (q, lang) in enumerate(zip(questions, question_langs)) if lang == 'ko']\n",
    "    \n",
    "    translated_questions = [''] * len(questions)\n",
    "    \n",
    "    if en_questions:\n",
    "        print(f\"  🇺🇸 영어 질문 {len(en_questions)}개 번역 중...\")\n",
    "        en_texts = [q for _, q in en_questions]\n",
    "        en_translated = translate_batch_texts(en_texts, tokenizer, model, \"en\", batch_size)\n",
    "        for (idx, _), trans in zip(en_questions, en_translated):\n",
    "            translated_questions[idx] = trans\n",
    "    \n",
    "    if zh_questions:\n",
    "        print(f\"  🇨🇳 중국어 질문 {len(zh_questions)}개 번역 중...\")\n",
    "        zh_texts = [q for _, q in zh_questions]\n",
    "        zh_translated = translate_batch_texts(zh_texts, tokenizer, model, \"zh\", batch_size)\n",
    "        for (idx, _), trans in zip(zh_questions, zh_translated):\n",
    "            translated_questions[idx] = trans\n",
    "    \n",
    "    if ko_questions:\n",
    "        print(f\"  🇰🇷 한국어 질문 {len(ko_questions)}개 (번역 생략)...\")\n",
    "        for idx, q in ko_questions:\n",
    "            translated_questions[idx] = q\n",
    "    \n",
    "    # 답변 번역\n",
    "    print(\"📋 답변 언어별 배치 번역 중...\")\n",
    "    \n",
    "    en_answers = [(i, a) for i, (a, lang) in enumerate(zip(answers, answer_langs)) if lang == 'en']\n",
    "    zh_answers = [(i, a) for i, (a, lang) in enumerate(zip(answers, answer_langs)) if lang == 'zh']\n",
    "    ko_answers = [(i, a) for i, (a, lang) in enumerate(zip(answers, answer_langs)) if lang == 'ko']\n",
    "    \n",
    "    translated_answers = [''] * len(answers)\n",
    "    \n",
    "    if en_answers:\n",
    "        print(f\"  🇺🇸 영어 답변 {len(en_answers)}개 번역 중...\")\n",
    "        en_texts = [a for _, a in en_answers]\n",
    "        en_translated = translate_batch_texts(en_texts, tokenizer, model, \"en\", batch_size)\n",
    "        for (idx, _), trans in zip(en_answers, en_translated):\n",
    "            translated_answers[idx] = trans\n",
    "    \n",
    "    if zh_answers:\n",
    "        print(f\"  🇨🇳 중국어 답변 {len(zh_answers)}개 번역 중...\")\n",
    "        zh_texts = [a for _, a in zh_answers]\n",
    "        zh_translated = translate_batch_texts(zh_texts, tokenizer, model, \"zh\", batch_size)\n",
    "        for (idx, _), trans in zip(zh_answers, zh_translated):\n",
    "            translated_answers[idx] = trans\n",
    "    \n",
    "    if ko_answers:\n",
    "        print(f\"  🇰🇷 한국어 답변 {len(ko_answers)}개 (번역 생략)...\")\n",
    "        for idx, a in ko_answers:\n",
    "            translated_answers[idx] = a\n",
    "    \n",
    "    # 3단계: 결과 조립\n",
    "    print(\"🔧 결과 조립 중...\")\n",
    "    translated_data = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        translated_data.append({\n",
    "            'Question': translated_questions[i],\n",
    "            'Answer': translated_answers[i]\n",
    "        })\n",
    "    \n",
    "    print(\"✅ SecBench QA 고속 번역 완료!\")\n",
    "    return pd.DataFrame(translated_data)\n",
    "\n",
    "print(\"🚀 SecBench 고속 번역 함수 로드 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b102d9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 CyberMetric MCQA 총 10180개 항목 고속 번역 시작...\n",
      "💡 배치 크기: 32, GPU 메모리: 23.6GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📋 데이터 전처리: 100%|██████████| 10180/10180 [00:00<00:00, 19742.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 질문 배치 번역 중...\n",
      "📦 10180개 텍스트를 32개씩 배치 번역 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 고속 배치 번역:   0%|          | 0/319 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "번역 실패 (영어:65.0%, 한국어:25.0%): Maybe \"이용 가능성\" captures...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 고속 배치 번역:   0%|          | 1/319 [04:21<23:06:24, 261.59s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# 실제 번역 실행 (모델이 로드된 경우)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m() \u001b[38;5;129;01mand\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m     cyber_mcqa_ko \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate_cybermetric_mcqa_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcyber_mcqa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 고속 번역\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCyberMetric MCQA 번역 완료!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(cyber_mcqa_ko\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m2\u001b[39m))\n",
      "Cell \u001b[0;32mIn[5], line 76\u001b[0m, in \u001b[0;36mtranslate_cybermetric_mcqa_fast\u001b[0;34m(df, tokenizer, model, batch_size)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# 2단계: 배치 번역 실행\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📝 질문 배치 번역 중...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 76\u001b[0m translated_questions \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate_batch_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_questions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📋 선택지 배치 번역 중...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     79\u001b[0m translated_choices \u001b[38;5;241m=\u001b[39m translate_batch_texts(all_choices, tokenizer, model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_size)\n",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m, in \u001b[0;36mtranslate_batch_texts\u001b[0;34m(texts, tokenizer, model, source_lang, batch_size, save_interval)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m batch_texts:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 16\u001b[0m         translated \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_lang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m         batch_results\u001b[38;5;241m.\u001b[39mappend(translated)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[2], line 156\u001b[0m, in \u001b[0;36mtranslate_text\u001b[0;34m(text, tokenizer, model, source_lang, target_lang)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# 생성 (GPU 최적화된 파라미터)\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 156\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# input_ids와 attention_mask 모두 전달\u001b[39;49;00m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 더 짧게 (속도 향상)\u001b[39;49;00m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 더 결정적으로 (속도 향상)\u001b[39;49;00m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 반복 방지\u001b[39;49;00m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 캐시 사용으로 속도 향상\u001b[39;49;00m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# 디코딩\u001b[39;00m\n\u001b[1;32m    170\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2634\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2626\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2627\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2628\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2629\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2630\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2631\u001b[0m     )\n\u001b[1;32m   2633\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2634\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2635\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2639\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2641\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2642\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2644\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2645\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2646\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2647\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2648\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2649\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2650\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2651\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:3618\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3616\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3617\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3618\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3620\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3621\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3622\u001b[0m     outputs,\n\u001b[1;32m   3623\u001b[0m     model_kwargs,\n\u001b[1;32m   3624\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3625\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:959\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[0;32m--> 959\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    961\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/exaone4/modeling_exaone4.py:491\u001b[0m, in \u001b[0;36mExaone4ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[TransformersKwargs],\n\u001b[1;32m    458\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CausalLMOutputWithPast:\n\u001b[1;32m    459\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;124;03m    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;124;03m        Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    489\u001b[0m \n\u001b[1;32m    490\u001b[0m \u001b[38;5;124;03m    NOTE: `EXAONE-4.0-Instruct` is a placeholder model ID. The exact model ID will be updated in the future.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 491\u001b[0m     outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:1083\u001b[0m, in \u001b[0;36mcheck_model_inputs.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m                 module\u001b[38;5;241m.\u001b[39mforward \u001b[38;5;241m=\u001b[39m make_capture_wrapper(module, original_forward, key, specs\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   1081\u001b[0m                 monkey_patched_layers\u001b[38;5;241m.\u001b[39mappend((module, original_forward))\n\u001b[0;32m-> 1083\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;66;03m# Restore original forward methods\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module, original_forward \u001b[38;5;129;01min\u001b[39;00m monkey_patched_layers:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/exaone4/modeling_exaone4.py:404\u001b[0m, in \u001b[0;36mExaone4Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[1;32m    403\u001b[0m     layer_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayer_types[i]\n\u001b[0;32m--> 404\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(hidden_states)\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[1;32m    418\u001b[0m     last_hidden_state\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    419\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    420\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/exaone4/modeling_exaone4.py:291\u001b[0m, in \u001b[0;36mExaone4DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    281\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[TransformersKwargs],\n\u001b[1;32m    289\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mFloatTensor, Optional[\u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mFloatTensor, torch\u001b[38;5;241m.\u001b[39mFloatTensor]]]:\n\u001b[1;32m    290\u001b[0m     residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 291\u001b[0m     hidden_states, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m    302\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/exaone4/modeling_exaone4.py:230\u001b[0m, in \u001b[0;36mExaone4Attention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     cache_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_position,\n\u001b[1;32m    229\u001b[0m     }\n\u001b[0;32m--> 230\u001b[0m     key_states, value_states \u001b[38;5;241m=\u001b[39m \u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m attention_interface: Callable \u001b[38;5;241m=\u001b[39m eager_attention_forward\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/cache_utils.py:992\u001b[0m, in \u001b[0;36mapply_processors.<locals>._wrapped_update\u001b[0;34m(self, key_states, value_states, layer_idx, cache_kwargs)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    988\u001b[0m     key_states, value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_processor\u001b[38;5;241m.\u001b[39mpre_update(\n\u001b[1;32m    989\u001b[0m         \u001b[38;5;28mself\u001b[39m, key_states, value_states, layer_idx, cache_kwargs\n\u001b[1;32m    990\u001b[0m     )\n\u001b[0;32m--> 992\u001b[0m key_tensors, value_tensors \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    995\u001b[0m     key_tensors, value_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_processor\u001b[38;5;241m.\u001b[39mpost_update(\n\u001b[1;32m    996\u001b[0m         \u001b[38;5;28mself\u001b[39m, key_tensors, value_tensors, layer_idx, cache_kwargs\n\u001b[1;32m    997\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/cache_utils.py:1201\u001b[0m, in \u001b[0;36mCache.update\u001b[0;34m(self, key_states, value_states, layer_idx, cache_kwargs)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;124;03mUpdates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;124;03m    A tuple containing the updated key and value states.\u001b[39;00m\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend_new_layers(layer_idx)\n\u001b[0;32m-> 1201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/cache_utils.py:377\u001b[0m, in \u001b[0;36mSlidingWindowLayer.update\u001b[0;34m(self, key_states, value_states, cache_kwargs)\u001b[0m\n\u001b[1;32m    375\u001b[0m current_seq_len \u001b[38;5;241m=\u001b[39m cache_position[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Use last position to determine current length\u001b[39;00m\n\u001b[1;32m    376\u001b[0m to_shift \u001b[38;5;241m=\u001b[39m current_seq_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_cache_len\n\u001b[0;32m--> 377\u001b[0m indices \u001b[38;5;241m=\u001b[39m (slicing \u001b[38;5;241m+\u001b[39m \u001b[43mto_shift\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_cache_len\n\u001b[1;32m    379\u001b[0m k_out_shifted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys[:, :, indices]\n\u001b[1;32m    380\u001b[0m v_out_shifted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues[:, :, indices]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1. CyberMetric MCQA 번역 (영어 → 한국어)\n",
    "\n",
    "def translate_cybermetric_mcqa(df: pd.DataFrame, tokenizer, model, batch_size: int = 5):\n",
    "    \"\"\"CyberMetric MCQA 데이터를 번역합니다.\"\"\"\n",
    "    \n",
    "    translated_data = []\n",
    "    \n",
    "    print(f\"CyberMetric MCQA 총 {len(df)}개 항목 번역 시작...\")\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "            question = row['Question']\n",
    "            answer = row['Answer']\n",
    "            \n",
    "            # 번역 수행 (CyberMetric은 영어만)\n",
    "            translated_question, translated_answer = translate_mcqa_entry(\n",
    "                question, answer, tokenizer, model, source_lang=\"en\"\n",
    "            )\n",
    "            \n",
    "            translated_data.append({\n",
    "                'Question': translated_question,\n",
    "                'Answer': translated_answer\n",
    "            })\n",
    "            \n",
    "            # 배치마다 잠시 대기 (메모리 관리)\n",
    "            if (idx + 1) % batch_size == 0:\n",
    "                time.sleep(1)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"번역 오류 (행 {idx}): {e}\")\n",
    "            # 오류시 원본 데이터 사용\n",
    "            translated_data.append({\n",
    "                'Question': row['Question'], \n",
    "                'Answer': row['Answer']\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(translated_data)\n",
    "\n",
    "# 실제 번역 실행 (모델이 로드된 경우)\n",
    "if 'tokenizer' in locals() and 'model' in locals() and tokenizer is not None:\n",
    "    cyber_mcqa_ko = translate_cybermetric_mcqa_fast(cyber_mcqa, tokenizer, model, batch_size=32)  # 고속 번역\n",
    "    print(\"CyberMetric MCQA 번역 완료!\")\n",
    "    print(cyber_mcqa_ko.head(2))\n",
    "else:\n",
    "    print(\"모델이 로드되지 않았습니다. 실제 실행시 모델을 먼저 로드하세요.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb61b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. SecBench MCQA 번역 (중국어 → 한국어)\n",
    "\n",
    "def translate_secbench_mcqa(df: pd.DataFrame, tokenizer, model, batch_size: int = 5):\n",
    "    \"\"\"SecBench MCQA 데이터를 번역합니다.\"\"\"\n",
    "    \n",
    "    translated_data = []\n",
    "    \n",
    "    print(f\"SecBench MCQA 총 {len(df)}개 항목 번역 시작...\")\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "            question = row['Question']\n",
    "            answer = row['Answer']\n",
    "            \n",
    "            # 번역 수행 (SecBench는 영어/중국어 혼용, 자동 감지)\n",
    "            translated_question, translated_answer = translate_mcqa_entry(\n",
    "                question, answer, tokenizer, model, source_lang=\"auto\"\n",
    "            )\n",
    "            \n",
    "            translated_data.append({\n",
    "                'Question': translated_question,\n",
    "                'Answer': translated_answer\n",
    "            })\n",
    "            \n",
    "            # 배치마다 잠시 대기\n",
    "            if (idx + 1) % batch_size == 0:\n",
    "                time.sleep(1)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"번역 오류 (행 {idx}): {e}\")\n",
    "            translated_data.append({\n",
    "                'Question': row['Question'], \n",
    "                'Answer': row['Answer']\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(translated_data)\n",
    "\n",
    "# 실제 번역 실행\n",
    "if 'tokenizer' in locals() and 'model' in locals() and tokenizer is not None:\n",
    "    sec_mcqa_ko = translate_secbench_mcqa_fast(sec_mcqa, tokenizer, model, batch_size=32)  # 고속 번역\n",
    "    print(\"SecBench MCQA 번역 완료!\")\n",
    "    print(sec_mcqa_ko.head(2))\n",
    "else:\n",
    "    print(\"모델이 로드되지 않았습니다. 실제 실행시 모델을 먼저 로드하세요.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a04ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. SecBench QA 번역 (중국어 → 한국어)\n",
    "\n",
    "def translate_secbench_qa(df: pd.DataFrame, tokenizer, model, batch_size: int = 5):\n",
    "    \"\"\"SecBench QA 데이터를 번역합니다.\"\"\"\n",
    "    \n",
    "    translated_data = []\n",
    "    \n",
    "    print(f\"SecBench QA 총 {len(df)}개 항목 번역 시작...\")\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "            question = row['Question']\n",
    "            answer = row['Answer']\n",
    "            \n",
    "            # 번역 수행 (SecBench는 영어/중국어 혼용, 자동 감지)\n",
    "            translated_question, translated_answer = translate_qa_entry(\n",
    "                question, answer, tokenizer, model, source_lang=\"auto\"\n",
    "            )\n",
    "            \n",
    "            translated_data.append({\n",
    "                'Question': translated_question,\n",
    "                'Answer': translated_answer\n",
    "            })\n",
    "            \n",
    "            # 배치마다 잠시 대기\n",
    "            if (idx + 1) % batch_size == 0:\n",
    "                time.sleep(1)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"번역 오류 (행 {idx}): {e}\")\n",
    "            translated_data.append({\n",
    "                'Question': row['Question'], \n",
    "                'Answer': row['Answer']\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(translated_data)\n",
    "\n",
    "# 실제 번역 실행\n",
    "if 'tokenizer' in locals() and 'model' in locals() and tokenizer is not None:\n",
    "    sec_qa_ko = translate_secbench_qa_fast(sec_qa, tokenizer, model, batch_size=32)  # 고속 번역\n",
    "    print(\"SecBench QA 번역 완료!\")\n",
    "    print(sec_qa_ko.head(2))\n",
    "else:\n",
    "    print(\"모델이 로드되지 않았습니다. 실제 실행시 모델을 먼저 로드하세요.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64f129b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "번역 결과 저장 함수 준비 완료!\n",
      "실제 번역 완료 후 save_translated_data() 함수를 호출하세요.\n",
      "✅ CyberMetric MCQA (영어→한국어) 저장 완료: ../data/CyberMetric/mcqa.csv (10개 항목)\n",
      "❌ SecBench MCQA (중국어→한국어): 변수 'sec_mcqa_ko'를 찾을 수 없습니다.\n",
      "❌ SecBench QA (중국어→한국어): 변수 'sec_qa_ko'를 찾을 수 없습니다.\n"
     ]
    }
   ],
   "source": [
    "# 번역 결과 저장\n",
    "\n",
    "def save_translated_data():\n",
    "    \"\"\"번역된 데이터를 CSV 파일로 저장합니다.\"\"\"\n",
    "    \n",
    "    # 저장할 데이터와 경로 정의\n",
    "    datasets_to_save = [\n",
    "        {\n",
    "            'data': 'cyber_mcqa_ko',\n",
    "            'path': '../data/CyberMetric/mcqa.csv',\n",
    "            'description': 'CyberMetric MCQA (영어→한국어)'\n",
    "        },\n",
    "        {\n",
    "            'data': 'sec_mcqa_ko', \n",
    "            'path': '../data/SecBench/mcqa.csv',\n",
    "            'description': 'SecBench MCQA (중국어→한국어)'\n",
    "        },\n",
    "        {\n",
    "            'data': 'sec_qa_ko',\n",
    "            'path': '../data/SecBench/qa.csv', \n",
    "            'description': 'SecBench QA (중국어→한국어)'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for dataset_info in datasets_to_save:\n",
    "        data_name = dataset_info['data']\n",
    "        file_path = dataset_info['path']\n",
    "        description = dataset_info['description']\n",
    "        \n",
    "        try:\n",
    "            # 변수가 존재하는지 확인\n",
    "            if data_name in locals() or data_name in globals():\n",
    "                data = locals().get(data_name) or globals().get(data_name)\n",
    "                \n",
    "                if data is not None and len(data) > 0:\n",
    "                    # CSV로 저장\n",
    "                    data.to_csv(file_path, index=False, encoding='utf-8')\n",
    "                    print(f\"✅ {description} 저장 완료: {file_path} ({len(data)}개 항목)\")\n",
    "                else:\n",
    "                    print(f\"❌ {description}: 데이터가 비어있습니다.\")\n",
    "            else:\n",
    "                print(f\"❌ {description}: 변수 '{data_name}'를 찾을 수 없습니다.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ {description} 저장 실패: {e}\")\n",
    "\n",
    "# 실제 번역이 완료된 경우에만 저장\n",
    "print(\"번역 결과 저장 함수 준비 완료!\")\n",
    "print(\"실제 번역 완료 후 save_translated_data() 함수를 호출하세요.\")\n",
    "\n",
    "save_translated_data()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
