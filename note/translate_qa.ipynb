{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b3ce014",
   "metadata": {},
   "source": [
    "# í•œêµ­ì–´ ê¸ˆìœµë³´ì•ˆ ë°ì´í„°ì…‹ ë²ˆì—­ ì‘ì—…\n",
    "\n",
    "## ì‘ì—… ê°œìš”\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ì˜ì–´/ì¤‘êµ­ì–´ë¡œ ì‘ì„±ëœ ê¸ˆìœµë³´ì•ˆ ê´€ë ¨ QA ë°ì´í„°ì…‹ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ëŠ” ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ëŒ€ìƒ íŒŒì¼ë“¤\n",
    "1. **CyberMetric/mcqa_org.csv** - ì˜ì–´ ì „ìš© MCQA ë°ì´í„°\n",
    "2. **SecBench/mcqa_org.csv** - ì˜ì–´/ì¤‘êµ­ì–´ í˜¼ìš© MCQA ë°ì´í„°  \n",
    "3. **SecBench/qa_org.csv** - ì˜ì–´/ì¤‘êµ­ì–´ í˜¼ìš© QA ë°ì´í„°\n",
    "\n",
    "## ì‘ì—… íë¦„\n",
    "1. ê° `_org.csv` íŒŒì¼ì„ ì½ì–´ì„œ ë°ì´í„° êµ¬ì¡° ë° ì–¸ì–´ ë¶„í¬ í™•ì¸\n",
    "2. **ì–¸ì–´ ìë™ ê°ì§€**: SecBench ë°ì´í„°ì˜ ì˜ì–´/ì¤‘êµ­ì–´ í˜¼ìš© ìƒí™© ì²˜ë¦¬\n",
    "3. ë¡œì»¬ LLMì„ ì‚¬ìš©í•˜ì—¬ í•œêµ­ì–´ë¡œ ë²ˆì—­\n",
    "4. ë²ˆì—­ëœ ê²°ê³¼ë¥¼ ì›ë˜ í´ë”ì— í•œêµ­ì–´ ë²„ì „ìœ¼ë¡œ ì €ì¥\n",
    "5. ë²ˆì—­ í’ˆì§ˆ ê²€ì¦\n",
    "\n",
    "## ì£¼ìš” ê°œì„ ì‚¬í•­\n",
    "- **ì–¸ì–´ ìë™ ê°ì§€ ê¸°ëŠ¥**: SecBench ë°ì´í„°ì˜ ì˜ì–´/ì¤‘êµ­ì–´ í˜¼ìš© ë¬¸ì œ í•´ê²°\n",
    "- **ê°œë³„ í•­ëª©ë³„ ì–¸ì–´ íŒë‹¨**: ì§ˆë¬¸ê³¼ ì„ íƒì§€ë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ ì–¸ì–´ ê°ì§€\n",
    "- **ì´ë¯¸ ë²ˆì—­ëœ ë‚´ìš© ê±´ë„ˆë›°ê¸°**: í•œêµ­ì–´ í…ìŠ¤íŠ¸ëŠ” ì¬ë²ˆì—­í•˜ì§€ ì•ŠìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "202d565b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ì„¤ì •\n",
    "\n",
    "# !pip install -q pandas tqdm \n",
    "# !pip install -q transformers==4.55.0 # llm requires >=4.46.0\n",
    "# !pip install -q safetensors==0.4.3 # downgrade for torch 2.1.0\n",
    "# !pip install -q bitsandbytes==0.43.2 accelerate==1.9.0 # quantization\n",
    "\n",
    "print(\"ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ!\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6556cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM ëª¨ë¸ ë¡œë”© ì¤‘...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e649ec3232d43749162937f6af0294f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë¸ '/workspace/models/exaone-4.0-32b' ë¡œë“œ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# LLM ëª¨ë¸ ë¡œë“œ ë° ë²ˆì—­ í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "import unicodedata\n",
    "\n",
    "def load_translation_model(model_name=\"microsoft/DialoGPT-medium\"):\n",
    "    \"\"\"ë²ˆì—­ìš© LLM ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.\"\"\"\n",
    "    try:\n",
    "        # ì–‘ìí™” ì„¤ì • (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\"\n",
    "        )\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            padding_side=\"left\"  # Decoder-only ëª¨ë¸ìš© left padding\n",
    "        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,\n",
    "            attn_implementation=\"eager\"  # SDPA ìš”êµ¬ì‚¬í•­ ìš°íšŒ\n",
    "        )\n",
    "        \n",
    "        # pad_token ì„¤ì •\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "        print(f\"ëª¨ë¸ '{model_name}' ë¡œë“œ ì™„ë£Œ!\")\n",
    "        return tokenizer, model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "        print(\"ëŒ€ì•ˆ: OpenAI API ë˜ëŠ” ë‹¤ë¥¸ ë²ˆì—­ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\")\n",
    "        return None, None\n",
    "\n",
    "def detect_language(text: str) -> str:\n",
    "    \"\"\"í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ìë™ ê°ì§€í•©ë‹ˆë‹¤.\"\"\"\n",
    "    \n",
    "    # í•œêµ­ì–´ëŠ” ì´ë¯¸ ë²ˆì—­ ì™„ë£Œëœ ê²ƒìœ¼ë¡œ ê°„ì£¼\n",
    "    korean_chars = sum(1 for char in text if '\\uac00' <= char <= '\\ud7af')\n",
    "    if korean_chars > len(text) * 0.1:  # 10% ì´ìƒì´ í•œê¸€ì´ë©´ í•œêµ­ì–´\n",
    "        return \"ko\"\n",
    "    \n",
    "    # ì¤‘êµ­ì–´ ë¬¸ì ê°ì§€ (ê°„ì²´/ë²ˆì²´ í¬í•¨)\n",
    "    chinese_chars = sum(1 for char in text if '\\u4e00' <= char <= '\\u9fff')\n",
    "    \n",
    "    # ì˜ì–´ ë¬¸ì ê°ì§€ (ì•ŒíŒŒë²³)\n",
    "    english_chars = sum(1 for char in text if char.isascii() and char.isalpha())\n",
    "    \n",
    "    # ì „ì²´ í…ìŠ¤íŠ¸ ê¸¸ì´\n",
    "    total_chars = len(text.replace(' ', '').replace('\\n', ''))\n",
    "    \n",
    "    if total_chars == 0:\n",
    "        return \"unknown\"\n",
    "    \n",
    "    # ì¤‘êµ­ì–´ ë¹„ìœ¨ì´ ë†’ìœ¼ë©´ ì¤‘êµ­ì–´\n",
    "    if chinese_chars > total_chars * 0.3:\n",
    "        return \"zh\"\n",
    "    \n",
    "    # ì˜ì–´ ë¹„ìœ¨ì´ ë†’ìœ¼ë©´ ì˜ì–´\n",
    "    if english_chars > total_chars * 0.5:\n",
    "        return \"en\"\n",
    "    \n",
    "    # í˜¼ìš©ë˜ì–´ ìˆëŠ” ê²½ìš° ë” ë§ì€ ìª½ìœ¼ë¡œ íŒë‹¨\n",
    "    if chinese_chars > english_chars:\n",
    "        return \"zh\"\n",
    "    elif english_chars > chinese_chars:\n",
    "        return \"en\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "\n",
    "def clean_translation_result(text: str) -> str:\n",
    "    \"\"\"ë²ˆì—­ ê²°ê³¼ì—ì„œ ë¶ˆí•„ìš”í•œ ìš”ì†Œë“¤ì„ ì œê±°í•©ë‹ˆë‹¤.\"\"\"\n",
    "    # í”„ë¡¬í”„íŠ¸ ì”ì—¬ë¬¼ ë° ë©”íƒ€ ì„¤ëª… ì œê±°\n",
    "    unwanted_patterns = [\n",
    "        r\"English:\\s*\", r\"Korean:\\s*\", r\"Chinese:\\s*\", r\"Korean translation:\\s*\",\n",
    "        r\"---+\", r\"\\*\\*.*?\\*\\*\", r\"Final Answer?:?\",\n",
    "        r\"ë²ˆì—­:\\s*\", r\"ë‹µë³€:\\s*\", r\"ê²°ê³¼:\\s*\",\n",
    "        r\"The original.*?requested\\.\", r\"However.*?ask!\",\n",
    "        r\"I will follow.*?format\\.\", r\"The translation.*?style\\.\",\n",
    "        r\"The provided text.*?terminology\\.\", r\"If you need.*?ask!\",\n",
    "        r\"Translate.*?Korean\", r\"Translation.*?format\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in unwanted_patterns:\n",
    "        text = re.sub(pattern, \"\", text, flags=re.IGNORECASE | re.DOTALL)\n",
    "    \n",
    "    # ìŒì„± ë²ˆì—­ íŒ¨í„´ ì œê±° (í•œêµ­ì–´ + ê´„í˜¸ ì•ˆ ì˜ì–´ ë°œìŒ)\n",
    "    # ì˜ˆ: \"ì •ë³´ ë³´ì¦ (Jeongbo bojung)\" â†’ \"ì •ë³´ ë³´ì¦\"\n",
    "    text = re.sub(r'\\s*\\([A-Za-z\\s-]+\\)', '', text)\n",
    "    \n",
    "    # ì˜ì–´ ì„¤ëª…ë¬¸ ì œê±° (ê¸´ ì˜ì–´ ë¬¸ì¥ë“¤)\n",
    "    sentences = text.split('.')\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if sentence:\n",
    "            # ì˜ì–´ ë¹„ìœ¨ì´ ë†’ì€ ë¬¸ì¥ ì œê±°\n",
    "            english_chars = sum(1 for char in sentence if char.isascii() and char.isalpha())\n",
    "            total_chars = len(sentence.replace(' ', ''))\n",
    "            if total_chars > 0 and english_chars / total_chars < 0.7:  # ì˜ì–´ ë¹„ìœ¨ 70% ë¯¸ë§Œë§Œ ìœ ì§€\n",
    "                cleaned_sentences.append(sentence)\n",
    "    \n",
    "    text = '. '.join(cleaned_sentences)\n",
    "    \n",
    "    # ì—°ì†ëœ ê³µë°±ê³¼ ì¤„ë°”ê¿ˆ ì •ë¦¬\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n', text)  # ë¹ˆ ì¤„ ì œê±°\n",
    "    text = re.sub(r'\\s+', ' ', text)  # ì—°ì† ê³µë°±ì„ í•˜ë‚˜ë¡œ\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def translate_text(text: str, tokenizer, model, source_lang: str = \"auto\", target_lang: str = \"ko\") -> str:\n",
    "    \"\"\"í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤. source_langì´ 'auto'ë©´ ìë™ ê°ì§€í•©ë‹ˆë‹¤.\"\"\"\n",
    "    \n",
    "    # ì–¸ì–´ ìë™ ê°ì§€\n",
    "    if source_lang == \"auto\":\n",
    "        detected_lang = detect_language(text)\n",
    "        if detected_lang == \"ko\":\n",
    "            return text  # ì´ë¯¸ í•œêµ­ì–´ë©´ ë²ˆì—­í•˜ì§€ ì•ŠìŒ\n",
    "        elif detected_lang == \"unknown\":\n",
    "            # ì•Œ ìˆ˜ ì—†ëŠ” ê²½ìš° ì˜ì–´ë¡œ ê°€ì •\n",
    "            detected_lang = \"en\"\n",
    "        source_lang = detected_lang\n",
    "    \n",
    "    # ì˜ë¯¸ ë²ˆì—­ì„ ê°•ì¡°í•˜ëŠ” í”„ë¡¬í”„íŠ¸ (ìŒì„± ë²ˆì—­ ë°©ì§€)\n",
    "    if source_lang == \"en\":\n",
    "        prompt = f\"Translate this English text to Korean with proper meaning (not pronunciation). Use standard Korean terminology:\\n\\n{text}\\n\\nKorean translation:\"\n",
    "    elif source_lang == \"zh\":\n",
    "        prompt = f\"Translate this Chinese text to Korean with proper meaning (not pronunciation). Use standard Korean terminology:\\n\\n{text}\\n\\nKorean translation:\"\n",
    "    else:\n",
    "        prompt = f\"Translate to Korean with proper meaning (not pronunciation):\\n\\n{text}\\n\\nKorean translation:\"\n",
    "    \n",
    "    try:\n",
    "        # í† í°í™” (attention_mask í¬í•¨)\n",
    "        inputs = tokenizer(\n",
    "            prompt, \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True, \n",
    "            max_length=512,\n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        # ì…ë ¥ í…ì„œë¥¼ ëª¨ë¸ê³¼ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™\n",
    "        device = next(model.parameters()).device\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        \n",
    "        # ìƒì„± (GPU ìµœì í™”ëœ íŒŒë¼ë¯¸í„°)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,  # input_idsì™€ attention_mask ëª¨ë‘ ì „ë‹¬\n",
    "                max_new_tokens=100,  # ë” ì§§ê²Œ (ì†ë„ í–¥ìƒ)\n",
    "                num_return_sequences=1,\n",
    "                temperature=0.2,  # ë” ê²°ì •ì ìœ¼ë¡œ (ì†ë„ í–¥ìƒ)\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.1,  # ë°˜ë³µ ë°©ì§€\n",
    "                use_cache=True  # ìºì‹œ ì‚¬ìš©ìœ¼ë¡œ ì†ë„ í–¥ìƒ\n",
    "            )\n",
    "        \n",
    "        # ë””ì½”ë”©\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # ë²ˆì—­ ê²°ê³¼ ì¶”ì¶œ (ê°œì„ ëœ ë¡œì§)\n",
    "        if \"Korean translation:\" in generated_text:\n",
    "            translated = generated_text.split(\"Korean translation:\")[-1].strip()\n",
    "        elif \"Korean:\" in generated_text:\n",
    "            translated = generated_text.split(\"Korean:\")[-1].strip()\n",
    "        else:\n",
    "            # í”„ë¡¬í”„íŠ¸ ì œê±°\n",
    "            translated = generated_text[len(prompt):].strip()\n",
    "        \n",
    "        # ê²°ê³¼ ì •ì œ\n",
    "        translated = clean_translation_result(translated)\n",
    "        \n",
    "        # ì˜ì–´ ë¹„ìœ¨ì´ ë„ˆë¬´ ë†’ìœ¼ë©´ ì›ë¬¸ ë°˜í™˜ (ë²ˆì—­ ì‹¤íŒ¨ë¡œ ê°„ì£¼)\n",
    "        if translated:\n",
    "            english_chars = sum(1 for char in translated if char.isascii() and char.isalpha())\n",
    "            korean_chars = sum(1 for char in translated if '\\uac00' <= char <= '\\ud7af')\n",
    "            total_chars = len(translated.replace(' ', '').replace('\\n', ''))\n",
    "            \n",
    "            if total_chars > 0:\n",
    "                english_ratio = english_chars / total_chars\n",
    "                korean_ratio = korean_chars / total_chars\n",
    "                \n",
    "                # ì˜ì–´ ë¹„ìœ¨ì´ 50% ì´ìƒì´ê±°ë‚˜ í•œêµ­ì–´ê°€ 10% ë¯¸ë§Œì´ë©´ ë²ˆì—­ ì‹¤íŒ¨ë¡œ ê°„ì£¼\n",
    "                if english_ratio > 0.5 or korean_ratio < 0.1:\n",
    "                    print(f\"ë²ˆì—­ ì‹¤íŒ¨ (ì˜ì–´:{english_ratio:.1%}, í•œêµ­ì–´:{korean_ratio:.1%}): {translated[:50]}...\")\n",
    "                    return text  # ì›ë¬¸ ë°˜í™˜\n",
    "        \n",
    "        # ë¹ˆ ê²°ê³¼ë‚˜ ë„ˆë¬´ ì§§ì€ ê²°ê³¼ ë°©ì§€\n",
    "        if not translated or len(translated.strip()) < 3:\n",
    "            return text  # ì›ë¬¸ ë°˜í™˜\n",
    "            \n",
    "        return translated\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ë²ˆì—­ ì˜¤ë¥˜: {e}\")\n",
    "        return text  # ì˜¤ë¥˜ì‹œ ì›ë¬¸ ë°˜í™˜\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ (ì‹¤ì œ ì‚¬ìš©ì‹œ ì ì ˆí•œ ëª¨ë¸ëª…ìœ¼ë¡œ ë³€ê²½)\n",
    "print(\"LLM ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "tokenizer, model = load_translation_model(\"/workspace/models/exaone-4.0-32b\")  # ì˜ˆì‹œ ëª¨ë¸ëª…\n",
    "# print(\"ì‹¤ì œ ì‚¬ìš©ì‹œ ì ì ˆí•œ ëª¨ë¸ëª…ì„ ì„¤ì •í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70ce2dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CyberMetric MCQA ë°ì´í„° ===\n",
      "ë°ì´í„° í¬ê¸°: (10180, 2)\n",
      "ì»¬ëŸ¼: ['Question', 'Answer']\n",
      "\n",
      "ìƒ˜í”Œ ë°ì´í„°:\n",
      "                                            Question Answer\n",
      "0  Which of the following refers to the secrecy o...  ë‹µë³€: 4\n",
      "1  Which type of authentication uses multiple fac...  ë‹µë³€: 3\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== SecBench MCQA ë°ì´í„° ===\n",
      "ë°ì´í„° í¬ê¸°: (2730, 2)\n",
      "ì»¬ëŸ¼: ['Question', 'Answer']\n",
      "\n",
      "ìƒ˜í”Œ ë°ì´í„°:\n",
      "                                            Question Answer\n",
      "0  ä»¥ä¸‹å“ªç§æªæ–½æ˜¯ä¼ä¸šä¸ºäº†éµå®ˆæ•°æ®ä¿æŠ¤æ³•è§„ï¼Œé€šå¸¸ä¼šå®æ–½çš„ï¼Ÿ\\n1. å®šæœŸè¿›è¡Œç½‘ç»œæ¸—é€æµ‹è¯•\\n2....  ë‹µë³€: 2\n",
      "1  å…³äºä¸ªäººæ•°æ®çš„æƒç›Šï¼Œæ ¹æ®GDPRï¼Œæ•°æ®ä¸»ä½“æœ‰æƒï¼š\\n1. éšæ—¶è¦æ±‚è®¿é—®ã€æ›´æ­£æˆ–åˆ é™¤ä»–ä»¬çš„ä¸ªäºº...  ë‹µë³€: 1\n",
      "\n",
      "ì–¸ì–´ ë¶„í¬ ë¶„ì„:\n",
      "ìƒ˜í”Œ 100ê°œ ì¤‘ ì–¸ì–´ ë¶„í¬: {'zh': 92, 'en': 8}\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== SecBench QA ë°ì´í„° ===\n",
      "ë°ì´í„° í¬ê¸°: (270, 2)\n",
      "ì»¬ëŸ¼: ['Question', 'Answer']\n",
      "\n",
      "ìƒ˜í”Œ ë°ì´í„°:\n",
      "                   Question                                             Answer\n",
      "0  ä¸ºä»€ä¹ˆç°ä»£æ“ä½œç³»ç»Ÿçš„IP IDé€šå¸¸æ˜¯éšæœºäº§ç”Ÿçš„ï¼Ÿ                               ä¸ºäº†æé«˜å®‰å…¨æ€§ï¼Œé˜²æ­¢æŸäº›ç±»å‹çš„ç½‘ç»œæ”»å‡»ã€‚\n",
      "1   ECAæŠ€æœ¯å¦‚ä½•å¸®åŠ©ä¼ä¸šä¿æŠ¤å…¶ç½‘ç»œå…å—å†…éƒ¨å¨èƒï¼Ÿ  ECAæŠ€æœ¯é€šè¿‡åˆ†æå†…éƒ¨ç½‘ç»œæµé‡çš„ç‰¹å¾ï¼Œå¯ä»¥è¯†åˆ«å¼‚å¸¸è¡Œä¸ºå’Œæ½œåœ¨çš„å†…éƒ¨å¨èƒï¼Œä»è€Œå¸®åŠ©ä¼ä¸šåŠ å¼ºå†…éƒ¨...\n",
      "\n",
      "ì–¸ì–´ ë¶„í¬ ë¶„ì„:\n",
      "QA ìƒ˜í”Œ 100ê°œ ì¤‘ ì–¸ì–´ ë¶„í¬: {'zh': 97, 'en': 3}\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„° ë¡œë“œ ë° êµ¬ì¡° í™•ì¸\n",
    "\n",
    "# 1. CyberMetric MCQA (ì˜ì–´ ì „ìš©)\n",
    "print(\"=== CyberMetric MCQA ë°ì´í„° ===\")\n",
    "cyber_mcqa = pd.read_csv('../data/CyberMetric/mcqa_org.csv')\n",
    "print(f\"ë°ì´í„° í¬ê¸°: {cyber_mcqa.shape}\")\n",
    "print(f\"ì»¬ëŸ¼: {cyber_mcqa.columns.tolist()}\")\n",
    "print(\"\\nìƒ˜í”Œ ë°ì´í„°:\")\n",
    "print(cyber_mcqa.head(2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# 2. SecBench MCQA (ì˜ì–´/ì¤‘êµ­ì–´ í˜¼ìš©)\n",
    "print(\"=== SecBench MCQA ë°ì´í„° ===\")\n",
    "sec_mcqa = pd.read_csv('../data/SecBench/mcqa_org.csv')\n",
    "print(f\"ë°ì´í„° í¬ê¸°: {sec_mcqa.shape}\")\n",
    "print(f\"ì»¬ëŸ¼: {sec_mcqa.columns.tolist()}\")\n",
    "print(\"\\nìƒ˜í”Œ ë°ì´í„°:\")\n",
    "print(sec_mcqa.head(2))\n",
    "\n",
    "# ì–¸ì–´ ë¶„í¬ í™•ì¸\n",
    "print(\"\\nì–¸ì–´ ë¶„í¬ ë¶„ì„:\")\n",
    "languages = []\n",
    "for idx in range(min(100, len(sec_mcqa))):  # ì²˜ìŒ 100ê°œë§Œ ìƒ˜í”Œë§\n",
    "    question = sec_mcqa.iloc[idx]['Question']\n",
    "    lang = detect_language(question.split('\\n')[0])  # ì²« ë²ˆì§¸ ì¤„ë§Œ ê²€ì‚¬\n",
    "    languages.append(lang)\n",
    "\n",
    "from collections import Counter\n",
    "lang_count = Counter(languages)\n",
    "print(f\"ìƒ˜í”Œ 100ê°œ ì¤‘ ì–¸ì–´ ë¶„í¬: {dict(lang_count)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# 3. SecBench QA (ì˜ì–´/ì¤‘êµ­ì–´ í˜¼ìš©)\n",
    "print(\"=== SecBench QA ë°ì´í„° ===\")\n",
    "sec_qa = pd.read_csv('../data/SecBench/qa_org.csv')\n",
    "print(f\"ë°ì´í„° í¬ê¸°: {sec_qa.shape}\")\n",
    "print(f\"ì»¬ëŸ¼: {sec_qa.columns.tolist()}\")\n",
    "print(\"\\nìƒ˜í”Œ ë°ì´í„°:\")\n",
    "print(sec_qa.head(2))\n",
    "\n",
    "# ì–¸ì–´ ë¶„í¬ í™•ì¸\n",
    "print(\"\\nì–¸ì–´ ë¶„í¬ ë¶„ì„:\")\n",
    "qa_languages = []\n",
    "for idx in range(min(100, len(sec_qa))):  # ì²˜ìŒ 100ê°œë§Œ ìƒ˜í”Œë§\n",
    "    question = sec_qa.iloc[idx]['Question']\n",
    "    lang = detect_language(question)\n",
    "    qa_languages.append(lang)\n",
    "\n",
    "qa_lang_count = Counter(qa_languages)\n",
    "print(f\"QA ìƒ˜í”Œ 100ê°œ ì¤‘ ì–¸ì–´ ë¶„í¬: {dict(qa_lang_count)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ca553bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë²ˆì—­ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# MCQA ë°ì´í„° ë²ˆì—­ í•¨ìˆ˜\n",
    "\n",
    "def parse_mcqa_question(question_text: str) -> Dict:\n",
    "    \"\"\"MCQA ì§ˆë¬¸ì„ íŒŒì‹±í•˜ì—¬ ë¬¸ì œì™€ ì„ íƒì§€ë¥¼ ë¶„ë¦¬í•©ë‹ˆë‹¤.\"\"\"\n",
    "    lines = question_text.strip().split('\\n')\n",
    "    \n",
    "    # ì²« ë²ˆì§¸ ì¤„ì€ ë¬¸ì œ \n",
    "    question = lines[0]\n",
    "    \n",
    "    # ë‚˜ë¨¸ì§€ëŠ” ì„ íƒì§€\n",
    "    choices = []\n",
    "    for line in lines[1:]:\n",
    "        line = line.strip()\n",
    "        if line and (line.startswith(('1.', '2.', '3.', '4.', '5.', '6.', '7.'))):\n",
    "            choices.append(line)\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'choices': choices\n",
    "    }\n",
    "\n",
    "def translate_mcqa_entry(question_text: str, answer: str, tokenizer, model, source_lang: str = \"auto\") -> Tuple[str, str]:\n",
    "    \"\"\"MCQA ì—”íŠ¸ë¦¬ë¥¼ ë²ˆì—­í•©ë‹ˆë‹¤. ì–¸ì–´ ìë™ ê°ì§€ ì§€ì›.\"\"\"\n",
    "    \n",
    "    # ì§ˆë¬¸ íŒŒì‹±\n",
    "    parsed = parse_mcqa_question(question_text)\n",
    "    \n",
    "    # ì§ˆë¬¸ ë²ˆì—­ (ì–¸ì–´ ìë™ ê°ì§€)\n",
    "    if source_lang == \"auto\":\n",
    "        detected_lang = detect_language(parsed['question'])\n",
    "        # print(f\"ì§ˆë¬¸ ì–¸ì–´ ê°ì§€: {detected_lang}\")  # ë¡œê·¸ ì¤„ì„\n",
    "    else:\n",
    "        detected_lang = source_lang\n",
    "    \n",
    "    # ì§ˆë¬¸ ë²ˆì—­\n",
    "    translated_question = translate_text(parsed['question'], tokenizer, model, detected_lang)\n",
    "    \n",
    "    # ê° ì„ íƒì§€ ë²ˆì—­\n",
    "    translated_choices = []\n",
    "    for choice in parsed['choices']:\n",
    "        # ì„ íƒì§€ ë²ˆí˜¸ì™€ ë‚´ìš© ë¶„ë¦¬\n",
    "        if '. ' in choice:\n",
    "            num_part, content = choice.split('. ', 1)\n",
    "            # ê° ì„ íƒì§€ë§ˆë‹¤ ì–¸ì–´ ê°ì§€ (í˜¼ìš© ê°€ëŠ¥ì„±)\n",
    "            choice_lang = detect_language(content) if source_lang == \"auto\" else detected_lang\n",
    "            translated_content = translate_text(content, tokenizer, model, choice_lang)\n",
    "            # ë²ˆì—­ ê²°ê³¼ ì •ì œ\n",
    "            translated_content = clean_translation_result(translated_content)\n",
    "            translated_choices.append(f\"{num_part}. {translated_content}\")\n",
    "        else:\n",
    "            choice_lang = detect_language(choice) if source_lang == \"auto\" else detected_lang\n",
    "            translated_choice = translate_text(choice, tokenizer, model, choice_lang)\n",
    "            translated_choice = clean_translation_result(translated_choice)\n",
    "            translated_choices.append(translated_choice)\n",
    "    \n",
    "    # ë²ˆì—­ëœ ì§ˆë¬¸ê³¼ ì„ íƒì§€ ê²°í•© (í˜•ì‹ ê°œì„ )\n",
    "    if translated_choices:\n",
    "        translated_full = translated_question + '\\n' + '\\n'.join(translated_choices)\n",
    "    else:\n",
    "        translated_full = translated_question\n",
    "    \n",
    "    # ë‹µë³€ì€ ê·¸ëŒ€ë¡œ ìœ ì§€ (ë²ˆí˜¸ë‚˜ ê°„ë‹¨í•œ í˜•íƒœ)\n",
    "    translated_answer = answer\n",
    "    \n",
    "    return translated_full, translated_answer\n",
    "\n",
    "def translate_qa_entry(question: str, answer: str, tokenizer, model, source_lang: str = \"auto\") -> Tuple[str, str]:\n",
    "    \"\"\"QA ì—”íŠ¸ë¦¬ë¥¼ ë²ˆì—­í•©ë‹ˆë‹¤. ì–¸ì–´ ìë™ ê°ì§€ ì§€ì›.\"\"\"\n",
    "    \n",
    "    # ì§ˆë¬¸ê³¼ ë‹µë³€ ê°ê° ì–¸ì–´ ê°ì§€\n",
    "    if source_lang == \"auto\":\n",
    "        q_lang = detect_language(question)\n",
    "        a_lang = detect_language(answer)\n",
    "        # print(f\"ì§ˆë¬¸ ì–¸ì–´: {q_lang}, ë‹µë³€ ì–¸ì–´: {a_lang}\")  # ë¡œê·¸ ì¤„ì„\n",
    "    else:\n",
    "        q_lang = a_lang = source_lang\n",
    "    \n",
    "    # ë²ˆì—­ ë° ì •ì œ\n",
    "    translated_question = translate_text(question, tokenizer, model, q_lang)\n",
    "    translated_question = clean_translation_result(translated_question)\n",
    "    \n",
    "    translated_answer = translate_text(answer, tokenizer, model, a_lang)\n",
    "    translated_answer = clean_translation_result(translated_answer)\n",
    "    \n",
    "    return translated_question, translated_answer\n",
    "\n",
    "print(\"ë²ˆì—­ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb68a5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ê³ ì† ë²ˆì—­ í•¨ìˆ˜ ë¡œë“œ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# ê³ ì† ë°°ì¹˜ ë²ˆì—­ í•¨ìˆ˜ë“¤\n",
    "\n",
    "def translate_batch_texts(texts: List[str], tokenizer, model, source_lang: str = \"en\", batch_size: int = 8, save_interval: int = 1000, checkpoint_prefix: str = \"checkpoint\"):\n",
    "    \"\"\"ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤ (ì¤‘ê°„ ì €ì¥ ê¸°ëŠ¥ í¬í•¨).\"\"\"\n",
    "    results = []\n",
    "    checkpoint_file = f\"/workspace/fske/data/{checkpoint_prefix}_{source_lang}_progress.json\"\n",
    "    \n",
    "    # ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ\n",
    "    start_idx = 0\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        try:\n",
    "            with open(checkpoint_file, 'r', encoding='utf-8') as f:\n",
    "                checkpoint = json.load(f)\n",
    "            results = checkpoint.get('results', [])\n",
    "            start_idx = len(results)\n",
    "            print(f\"ğŸ“‚ ì²´í¬í¬ì¸íŠ¸ ë°œê²¬! {start_idx}ê°œ í•­ëª©ë¶€í„° ì¬ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì˜¤ë¥˜: {e}, ì²˜ìŒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    print(f\"ğŸ“¦ {len(texts)}ê°œ í…ìŠ¤íŠ¸ë¥¼ {batch_size}ê°œì”© ë°°ì¹˜ ë²ˆì—­ ì¤‘... (ì‹œì‘: {start_idx})\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in tqdm(range(start_idx, len(texts), batch_size), desc=\"ğŸš€ ê³ ì† ë°°ì¹˜ ë²ˆì—­\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        batch_results = []\n",
    "        \n",
    "        for text in batch_texts:\n",
    "            try:\n",
    "                translated = translate_text(text, tokenizer, model, source_lang)\n",
    "                batch_results.append(translated)\n",
    "            except Exception as e:\n",
    "                print(f\"ë°°ì¹˜ ë²ˆì—­ ì˜¤ë¥˜: {e}\")\n",
    "                batch_results.append(text)  # ì˜¤ë¥˜ì‹œ ì›ë¬¸ ë°˜í™˜\n",
    "        \n",
    "        results.extend(batch_results)\n",
    "        \n",
    "        # ì¤‘ê°„ ì €ì¥ (save_intervalë§ˆë‹¤)\n",
    "        if len(results) % save_interval == 0 or len(results) == len(texts):\n",
    "            checkpoint_data = {\n",
    "                'results': results,\n",
    "                'total': len(texts),\n",
    "                'completed': len(results),\n",
    "                'timestamp': time.time(),\n",
    "                'source_lang': source_lang\n",
    "            }\n",
    "            try:\n",
    "                with open(checkpoint_file, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)\n",
    "                print(f\"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {len(results)}/{len(texts)}ê°œ ì™„ë£Œ\")\n",
    "            except Exception as e:\n",
    "                print(f\"ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì˜¤ë¥˜: {e}\")\n",
    "        \n",
    "        # ì§„í–‰ë¥  ë° ì†ë„ í‘œì‹œ\n",
    "        if (i + batch_size) % (save_interval // 10) == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            speed = (len(results) - start_idx) / elapsed if elapsed > 0 else 0\n",
    "            remaining = (len(texts) - len(results)) / speed if speed > 0 else 0\n",
    "            print(f\"âš¡ ì§„í–‰ë¥ : {len(results)}/{len(texts)} ({len(results)/len(texts)*100:.1f}%) | \"\n",
    "                  f\"ì†ë„: {speed:.1f}ê°œ/ì´ˆ | ë‚¨ì€ì‹œê°„: {remaining/60:.1f}ë¶„\")\n",
    "        \n",
    "        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # ì™„ë£Œ í›„ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì‚­ì œ\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        os.remove(checkpoint_file)\n",
    "        print(\"ğŸ—‘ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"ğŸ‰ ë°°ì¹˜ ë²ˆì—­ ì™„ë£Œ! ì´ ì‹œê°„: {total_time/60:.1f}ë¶„, í‰ê·  ì†ë„: {(len(results)-start_idx)/total_time:.1f}ê°œ/ì´ˆ\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def translate_cybermetric_mcqa_fast(df: pd.DataFrame, tokenizer, model, batch_size: int = 16):\n",
    "    \"\"\"CyberMetric MCQA ë°ì´í„°ë¥¼ ê³ ì†ìœ¼ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤.\"\"\"\n",
    "    \n",
    "    print(f\"ğŸš€ CyberMetric MCQA ì´ {len(df)}ê°œ í•­ëª© ê³ ì† ë²ˆì—­ ì‹œì‘...\")\n",
    "    print(f\"ğŸ’¡ ë°°ì¹˜ í¬ê¸°: {batch_size}, GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory/1024**3:.1f}GB\")\n",
    "    \n",
    "    # 1ë‹¨ê³„: ë°ì´í„° ì „ì²˜ë¦¬ - ì§ˆë¬¸ê³¼ ì„ íƒì§€ ë¶„ë¦¬\n",
    "    all_questions = []\n",
    "    all_choices = []\n",
    "    choice_mapping = []  # ì–´ë–¤ ì§ˆë¬¸ì— ì†í•˜ëŠ”ì§€ ë§¤í•‘\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), desc=\"ğŸ“‹ ë°ì´í„° ì „ì²˜ë¦¬\", total=len(df)):\n",
    "        question_text = row['Question']\n",
    "        parsed = parse_mcqa_question(question_text)\n",
    "        \n",
    "        # ì§ˆë¬¸ ë³¸ë¬¸ ì¶”ê°€\n",
    "        all_questions.append(parsed['question'])\n",
    "        \n",
    "        # ê° ì„ íƒì§€ ì¶”ê°€ ë° ë§¤í•‘ ì €ì¥\n",
    "        choice_start_idx = len(all_choices)\n",
    "        for choice in parsed['choices']:\n",
    "            if '. ' in choice:\n",
    "                num_part, content = choice.split('. ', 1)\n",
    "                all_choices.append(content)\n",
    "            else:\n",
    "                all_choices.append(choice)\n",
    "        \n",
    "        choice_mapping.append({\n",
    "            'start_idx': choice_start_idx,\n",
    "            'count': len(parsed['choices']),\n",
    "            'original_choices': parsed['choices']\n",
    "        })\n",
    "    \n",
    "    # 2ë‹¨ê³„: ë°°ì¹˜ ë²ˆì—­ ì‹¤í–‰ (ì¤‘ê°„ ì €ì¥ í¬í•¨)\n",
    "    print(\"ğŸ“ ì§ˆë¬¸ ë°°ì¹˜ ë²ˆì—­ ì¤‘...\")\n",
    "    translated_questions = translate_batch_texts(all_questions, tokenizer, model, \"en\", batch_size, 500, \"cybermetric_questions\")\n",
    "    \n",
    "    print(\"ğŸ“‹ ì„ íƒì§€ ë°°ì¹˜ ë²ˆì—­ ì¤‘...\")\n",
    "    translated_choices = translate_batch_texts(all_choices, tokenizer, model, \"en\", batch_size, 1000, \"cybermetric_choices\")\n",
    "    \n",
    "    # 3ë‹¨ê³„: ê²°ê³¼ ì¬ì¡°ë¦½\n",
    "    print(\"ğŸ”§ ê²°ê³¼ ì¡°ë¦½ ì¤‘...\")\n",
    "    translated_data = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), desc=\"ğŸ”§ ê²°ê³¼ ì¡°ë¦½\", total=len(df)):\n",
    "        try:\n",
    "            # ë²ˆì—­ëœ ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°\n",
    "            translated_question = translated_questions[idx]\n",
    "            \n",
    "            # ë²ˆì—­ëœ ì„ íƒì§€ ì¡°ë¦½\n",
    "            mapping = choice_mapping[idx]\n",
    "            translated_choice_list = []\n",
    "            \n",
    "            for i, original_choice in enumerate(mapping['original_choices']):\n",
    "                choice_idx = mapping['start_idx'] + i\n",
    "                \n",
    "                if '. ' in original_choice:\n",
    "                    num_part, content = original_choice.split('. ', 1)\n",
    "                    translated_content = translated_choices[choice_idx]\n",
    "                    translated_choice_list.append(f\"{num_part}. {translated_content}\")\n",
    "                else:\n",
    "                    translated_choice_list.append(translated_choices[choice_idx])\n",
    "            \n",
    "            # ìµœì¢… ì¡°ë¦½\n",
    "            if translated_choice_list:\n",
    "                full_question = translated_question + '\\\\n' + '\\\\n'.join(translated_choice_list)\n",
    "            else:\n",
    "                full_question = translated_question\n",
    "            \n",
    "            translated_data.append({\n",
    "                'Question': full_question,\n",
    "                'Answer': row['Answer']\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ì¡°ë¦½ ì˜¤ë¥˜ (í–‰ {idx}): {e}\")\n",
    "            translated_data.append({\n",
    "                'Question': row['Question'], \n",
    "                'Answer': row['Answer']\n",
    "            })\n",
    "    \n",
    "    # ìµœì¢… ê²°ê³¼ ì¤‘ê°„ ì €ì¥\n",
    "    final_checkpoint = {\n",
    "        'data': translated_data,\n",
    "        'completed_at': time.time(),\n",
    "        'total_items': len(df)\n",
    "    }\n",
    "    try:\n",
    "        with open(f\"/workspace/fske/data/cybermetric_mcqa_final.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(final_checkpoint, f, ensure_ascii=False, indent=2)\n",
    "        print(\"ğŸ’¾ ìµœì¢… ê²°ê³¼ ì €ì¥ ì™„ë£Œ!\")\n",
    "    except Exception as e:\n",
    "        print(f\"ìµœì¢… ì €ì¥ ì˜¤ë¥˜: {e}\")\n",
    "    \n",
    "    print(\"âœ… ê³ ì† ë²ˆì—­ ì™„ë£Œ!\")\n",
    "    return pd.DataFrame(translated_data)\n",
    "\n",
    "def translate_secbench_mcqa_fast(df: pd.DataFrame, tokenizer, model, batch_size: int = 16):\n",
    "    \"\"\"SecBench MCQA ë°ì´í„°ë¥¼ ê³ ì†ìœ¼ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤ (ì˜ì–´/ì¤‘êµ­ì–´ í˜¼ìš©).\"\"\"\n",
    "    \n",
    "    print(f\"ğŸš€ SecBench MCQA ì´ {len(df)}ê°œ í•­ëª© ê³ ì† ë²ˆì—­ ì‹œì‘...\")\n",
    "    print(f\"ğŸ’¡ ë°°ì¹˜ í¬ê¸°: {batch_size}, ì–¸ì–´ ìë™ ê°ì§€ í™œì„±í™”\")\n",
    "    \n",
    "    # 1ë‹¨ê³„: ë°ì´í„° ì „ì²˜ë¦¬ - ì§ˆë¬¸ê³¼ ì„ íƒì§€ ë¶„ë¦¬ + ì–¸ì–´ ê°ì§€\n",
    "    all_questions = []\n",
    "    all_choices = []\n",
    "    choice_mapping = []\n",
    "    question_langs = []\n",
    "    choice_langs = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), desc=\"ğŸ“‹ ë°ì´í„° ì „ì²˜ë¦¬ ë° ì–¸ì–´ ê°ì§€\", total=len(df)):\n",
    "        question_text = row['Question']\n",
    "        parsed = parse_mcqa_question(question_text)\n",
    "        \n",
    "        # ì§ˆë¬¸ ì–¸ì–´ ê°ì§€ ë° ì¶”ê°€\n",
    "        q_lang = detect_language(parsed['question'])\n",
    "        all_questions.append(parsed['question'])\n",
    "        question_langs.append(q_lang)\n",
    "        \n",
    "        # ê° ì„ íƒì§€ ì–¸ì–´ ê°ì§€ ë° ì¶”ê°€\n",
    "        choice_start_idx = len(all_choices)\n",
    "        for choice in parsed['choices']:\n",
    "            if '. ' in choice:\n",
    "                num_part, content = choice.split('. ', 1)\n",
    "                c_lang = detect_language(content)\n",
    "                all_choices.append(content)\n",
    "                choice_langs.append(c_lang)\n",
    "            else:\n",
    "                c_lang = detect_language(choice)\n",
    "                all_choices.append(choice)\n",
    "                choice_langs.append(c_lang)\n",
    "        \n",
    "        choice_mapping.append({\n",
    "            'start_idx': choice_start_idx,\n",
    "            'count': len(parsed['choices']),\n",
    "            'original_choices': parsed['choices']\n",
    "        })\n",
    "    \n",
    "    # 2ë‹¨ê³„: ì–¸ì–´ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ë°°ì¹˜ ë²ˆì—­\n",
    "    print(\"ğŸ“ ì§ˆë¬¸ ì–¸ì–´ë³„ ë°°ì¹˜ ë²ˆì—­ ì¤‘...\")\n",
    "    \n",
    "    # ì˜ì–´ ì§ˆë¬¸ë“¤\n",
    "    en_questions = [(i, q) for i, (q, lang) in enumerate(zip(all_questions, question_langs)) if lang == 'en']\n",
    "    zh_questions = [(i, q) for i, (q, lang) in enumerate(zip(all_questions, question_langs)) if lang == 'zh']\n",
    "    ko_questions = [(i, q) for i, (q, lang) in enumerate(zip(all_questions, question_langs)) if lang == 'ko']\n",
    "    \n",
    "    translated_questions = [''] * len(all_questions)\n",
    "    \n",
    "    if en_questions:\n",
    "        print(f\"  ğŸ‡ºğŸ‡¸ ì˜ì–´ ì§ˆë¬¸ {len(en_questions)}ê°œ ë²ˆì—­ ì¤‘...\")\n",
    "        en_texts = [q for _, q in en_questions]\n",
    "        en_translated = translate_batch_texts(en_texts, tokenizer, model, \"en\", batch_size)\n",
    "        for (idx, _), trans in zip(en_questions, en_translated):\n",
    "            translated_questions[idx] = trans\n",
    "    \n",
    "    if zh_questions:\n",
    "        print(f\"  ğŸ‡¨ğŸ‡³ ì¤‘êµ­ì–´ ì§ˆë¬¸ {len(zh_questions)}ê°œ ë²ˆì—­ ì¤‘...\")\n",
    "        zh_texts = [q for _, q in zh_questions]\n",
    "        zh_translated = translate_batch_texts(zh_texts, tokenizer, model, \"zh\", batch_size)\n",
    "        for (idx, _), trans in zip(zh_questions, zh_translated):\n",
    "            translated_questions[idx] = trans\n",
    "    \n",
    "    if ko_questions:\n",
    "        print(f\"  ğŸ‡°ğŸ‡· í•œêµ­ì–´ ì§ˆë¬¸ {len(ko_questions)}ê°œ (ë²ˆì—­ ìƒëµ)...\")\n",
    "        for idx, q in ko_questions:\n",
    "            translated_questions[idx] = q\n",
    "    \n",
    "    # ì„ íƒì§€ë„ ë™ì¼í•˜ê²Œ ì²˜ë¦¬\n",
    "    print(\"ğŸ“‹ ì„ íƒì§€ ì–¸ì–´ë³„ ë°°ì¹˜ ë²ˆì—­ ì¤‘...\")\n",
    "    \n",
    "    en_choices = [(i, c) for i, (c, lang) in enumerate(zip(all_choices, choice_langs)) if lang == 'en']\n",
    "    zh_choices = [(i, c) for i, (c, lang) in enumerate(zip(all_choices, choice_langs)) if lang == 'zh']\n",
    "    ko_choices = [(i, c) for i, (c, lang) in enumerate(zip(all_choices, choice_langs)) if lang == 'ko']\n",
    "    \n",
    "    translated_choices = [''] * len(all_choices)\n",
    "    \n",
    "    if en_choices:\n",
    "        print(f\"  ğŸ‡ºğŸ‡¸ ì˜ì–´ ì„ íƒì§€ {len(en_choices)}ê°œ ë²ˆì—­ ì¤‘...\")\n",
    "        en_texts = [c for _, c in en_choices]\n",
    "        en_translated = translate_batch_texts(en_texts, tokenizer, model, \"en\", batch_size)\n",
    "        for (idx, _), trans in zip(en_choices, en_translated):\n",
    "            translated_choices[idx] = trans\n",
    "    \n",
    "    if zh_choices:\n",
    "        print(f\"  ğŸ‡¨ğŸ‡³ ì¤‘êµ­ì–´ ì„ íƒì§€ {len(zh_choices)}ê°œ ë²ˆì—­ ì¤‘...\")\n",
    "        zh_texts = [c for _, c in zh_choices]\n",
    "        zh_translated = translate_batch_texts(zh_texts, tokenizer, model, \"zh\", batch_size)\n",
    "        for (idx, _), trans in zip(zh_choices, zh_translated):\n",
    "            translated_choices[idx] = trans\n",
    "    \n",
    "    if ko_choices:\n",
    "        print(f\"  ğŸ‡°ğŸ‡· í•œêµ­ì–´ ì„ íƒì§€ {len(ko_choices)}ê°œ (ë²ˆì—­ ìƒëµ)...\")\n",
    "        for idx, c in ko_choices:\n",
    "            translated_choices[idx] = c\n",
    "    \n",
    "    # 3ë‹¨ê³„: ê²°ê³¼ ì¬ì¡°ë¦½\n",
    "    print(\"ğŸ”§ ê²°ê³¼ ì¡°ë¦½ ì¤‘...\")\n",
    "    translated_data = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), desc=\"ğŸ”§ ê²°ê³¼ ì¡°ë¦½\", total=len(df)):\n",
    "        try:\n",
    "            # ë²ˆì—­ëœ ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°\n",
    "            translated_question = translated_questions[idx]\n",
    "            \n",
    "            # ë²ˆì—­ëœ ì„ íƒì§€ ì¡°ë¦½\n",
    "            mapping = choice_mapping[idx]\n",
    "            translated_choice_list = []\n",
    "            \n",
    "            for i, original_choice in enumerate(mapping['original_choices']):\n",
    "                choice_idx = mapping['start_idx'] + i\n",
    "                \n",
    "                if '. ' in original_choice:\n",
    "                    num_part, content = original_choice.split('. ', 1)\n",
    "                    translated_content = translated_choices[choice_idx]\n",
    "                    translated_choice_list.append(f\"{num_part}. {translated_content}\")\n",
    "                else:\n",
    "                    translated_choice_list.append(translated_choices[choice_idx])\n",
    "            \n",
    "            # ìµœì¢… ì¡°ë¦½\n",
    "            if translated_choice_list:\n",
    "                full_question = translated_question + '\\\\n' + '\\\\n'.join(translated_choice_list)\n",
    "            else:\n",
    "                full_question = translated_question\n",
    "            \n",
    "            translated_data.append({\n",
    "                'Question': full_question,\n",
    "                'Answer': row['Answer']\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ì¡°ë¦½ ì˜¤ë¥˜ (í–‰ {idx}): {e}\")\n",
    "            translated_data.append({\n",
    "                'Question': row['Question'], \n",
    "                'Answer': row['Answer']\n",
    "            })\n",
    "    \n",
    "    print(\"âœ… SecBench MCQA ê³ ì† ë²ˆì—­ ì™„ë£Œ!\")\n",
    "    return pd.DataFrame(translated_data)\n",
    "\n",
    "def translate_secbench_qa_fast(df: pd.DataFrame, tokenizer, model, batch_size: int = 16):\n",
    "    \"\"\"SecBench QA ë°ì´í„°ë¥¼ ê³ ì†ìœ¼ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤ (ì˜ì–´/ì¤‘êµ­ì–´ í˜¼ìš©).\"\"\"\n",
    "    \n",
    "    print(f\"ğŸš€ SecBench QA ì´ {len(df)}ê°œ í•­ëª© ê³ ì† ë²ˆì—­ ì‹œì‘...\")\n",
    "    print(f\"ğŸ’¡ ë°°ì¹˜ í¬ê¸°: {batch_size}, ì–¸ì–´ ìë™ ê°ì§€ í™œì„±í™”\")\n",
    "    \n",
    "    # 1ë‹¨ê³„: ì–¸ì–´ ê°ì§€ ë° ë¶„ë¥˜\n",
    "    questions = df['Question'].tolist()\n",
    "    answers = df['Answer'].tolist()\n",
    "    \n",
    "    question_langs = [detect_language(q) for q in tqdm(questions, desc=\"ğŸ“‹ ì§ˆë¬¸ ì–¸ì–´ ê°ì§€\")]\n",
    "    answer_langs = [detect_language(a) for a in tqdm(answers, desc=\"ğŸ“‹ ë‹µë³€ ì–¸ì–´ ê°ì§€\")]\n",
    "    \n",
    "    # 2ë‹¨ê³„: ì–¸ì–´ë³„ ë°°ì¹˜ ë²ˆì—­\n",
    "    print(\"ğŸ“ ì§ˆë¬¸ ì–¸ì–´ë³„ ë°°ì¹˜ ë²ˆì—­ ì¤‘...\")\n",
    "    \n",
    "    # ì§ˆë¬¸ ë²ˆì—­\n",
    "    en_questions = [(i, q) for i, (q, lang) in enumerate(zip(questions, question_langs)) if lang == 'en']\n",
    "    zh_questions = [(i, q) for i, (q, lang) in enumerate(zip(questions, question_langs)) if lang == 'zh']\n",
    "    ko_questions = [(i, q) for i, (q, lang) in enumerate(zip(questions, question_langs)) if lang == 'ko']\n",
    "    \n",
    "    translated_questions = [''] * len(questions)\n",
    "    \n",
    "    if en_questions:\n",
    "        print(f\"  ğŸ‡ºğŸ‡¸ ì˜ì–´ ì§ˆë¬¸ {len(en_questions)}ê°œ ë²ˆì—­ ì¤‘...\")\n",
    "        en_texts = [q for _, q in en_questions]\n",
    "        en_translated = translate_batch_texts(en_texts, tokenizer, model, \"en\", batch_size)\n",
    "        for (idx, _), trans in zip(en_questions, en_translated):\n",
    "            translated_questions[idx] = trans\n",
    "    \n",
    "    if zh_questions:\n",
    "        print(f\"  ğŸ‡¨ğŸ‡³ ì¤‘êµ­ì–´ ì§ˆë¬¸ {len(zh_questions)}ê°œ ë²ˆì—­ ì¤‘...\")\n",
    "        zh_texts = [q for _, q in zh_questions]\n",
    "        zh_translated = translate_batch_texts(zh_texts, tokenizer, model, \"zh\", batch_size)\n",
    "        for (idx, _), trans in zip(zh_questions, zh_translated):\n",
    "            translated_questions[idx] = trans\n",
    "    \n",
    "    if ko_questions:\n",
    "        print(f\"  ğŸ‡°ğŸ‡· í•œêµ­ì–´ ì§ˆë¬¸ {len(ko_questions)}ê°œ (ë²ˆì—­ ìƒëµ)...\")\n",
    "        for idx, q in ko_questions:\n",
    "            translated_questions[idx] = q\n",
    "    \n",
    "    # ë‹µë³€ ë²ˆì—­\n",
    "    print(\"ğŸ“‹ ë‹µë³€ ì–¸ì–´ë³„ ë°°ì¹˜ ë²ˆì—­ ì¤‘...\")\n",
    "    \n",
    "    en_answers = [(i, a) for i, (a, lang) in enumerate(zip(answers, answer_langs)) if lang == 'en']\n",
    "    zh_answers = [(i, a) for i, (a, lang) in enumerate(zip(answers, answer_langs)) if lang == 'zh']\n",
    "    ko_answers = [(i, a) for i, (a, lang) in enumerate(zip(answers, answer_langs)) if lang == 'ko']\n",
    "    \n",
    "    translated_answers = [''] * len(answers)\n",
    "    \n",
    "    if en_answers:\n",
    "        print(f\"  ğŸ‡ºğŸ‡¸ ì˜ì–´ ë‹µë³€ {len(en_answers)}ê°œ ë²ˆì—­ ì¤‘...\")\n",
    "        en_texts = [a for _, a in en_answers]\n",
    "        en_translated = translate_batch_texts(en_texts, tokenizer, model, \"en\", batch_size)\n",
    "        for (idx, _), trans in zip(en_answers, en_translated):\n",
    "            translated_answers[idx] = trans\n",
    "    \n",
    "    if zh_answers:\n",
    "        print(f\"  ğŸ‡¨ğŸ‡³ ì¤‘êµ­ì–´ ë‹µë³€ {len(zh_answers)}ê°œ ë²ˆì—­ ì¤‘...\")\n",
    "        zh_texts = [a for _, a in zh_answers]\n",
    "        zh_translated = translate_batch_texts(zh_texts, tokenizer, model, \"zh\", batch_size)\n",
    "        for (idx, _), trans in zip(zh_answers, zh_translated):\n",
    "            translated_answers[idx] = trans\n",
    "    \n",
    "    if ko_answers:\n",
    "        print(f\"  ğŸ‡°ğŸ‡· í•œêµ­ì–´ ë‹µë³€ {len(ko_answers)}ê°œ (ë²ˆì—­ ìƒëµ)...\")\n",
    "        for idx, a in ko_answers:\n",
    "            translated_answers[idx] = a\n",
    "    \n",
    "    # 3ë‹¨ê³„: ê²°ê³¼ ì¡°ë¦½\n",
    "    print(\"ğŸ”§ ê²°ê³¼ ì¡°ë¦½ ì¤‘...\")\n",
    "    translated_data = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        translated_data.append({\n",
    "            'Question': translated_questions[i],\n",
    "            'Answer': translated_answers[i]\n",
    "        })\n",
    "    \n",
    "    print(\"âœ… SecBench QA ê³ ì† ë²ˆì—­ ì™„ë£Œ!\")\n",
    "    return pd.DataFrame(translated_data)\n",
    "\n",
    "print(\"ğŸš€ SecBench ê³ ì† ë²ˆì—­ í•¨ìˆ˜ ë¡œë“œ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b102d9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ CyberMetric MCQA ì´ 10180ê°œ í•­ëª© ê³ ì† ë²ˆì—­ ì‹œì‘...\n",
      "ğŸ’¡ ë°°ì¹˜ í¬ê¸°: 32, GPU ë©”ëª¨ë¦¬: 23.6GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ ë°ì´í„° ì „ì²˜ë¦¬: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10180/10180 [00:00<00:00, 19742.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ì§ˆë¬¸ ë°°ì¹˜ ë²ˆì—­ ì¤‘...\n",
      "ğŸ“¦ 10180ê°œ í…ìŠ¤íŠ¸ë¥¼ 32ê°œì”© ë°°ì¹˜ ë²ˆì—­ ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ ê³ ì† ë°°ì¹˜ ë²ˆì—­:   0%|          | 0/319 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë²ˆì—­ ì‹¤íŒ¨ (ì˜ì–´:65.0%, í•œêµ­ì–´:25.0%): Maybe \"ì´ìš© ê°€ëŠ¥ì„±\" captures...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ ê³ ì† ë°°ì¹˜ ë²ˆì—­:   0%|          | 1/319 [04:21<23:06:24, 261.59s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# ì‹¤ì œ ë²ˆì—­ ì‹¤í–‰ (ëª¨ë¸ì´ ë¡œë“œëœ ê²½ìš°)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m() \u001b[38;5;129;01mand\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m     cyber_mcqa_ko \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate_cybermetric_mcqa_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcyber_mcqa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# ê³ ì† ë²ˆì—­\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCyberMetric MCQA ë²ˆì—­ ì™„ë£Œ!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(cyber_mcqa_ko\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m2\u001b[39m))\n",
      "Cell \u001b[0;32mIn[5], line 76\u001b[0m, in \u001b[0;36mtranslate_cybermetric_mcqa_fast\u001b[0;34m(df, tokenizer, model, batch_size)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# 2ë‹¨ê³„: ë°°ì¹˜ ë²ˆì—­ ì‹¤í–‰\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ“ ì§ˆë¬¸ ë°°ì¹˜ ë²ˆì—­ ì¤‘...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 76\u001b[0m translated_questions \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate_batch_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_questions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ“‹ ì„ íƒì§€ ë°°ì¹˜ ë²ˆì—­ ì¤‘...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     79\u001b[0m translated_choices \u001b[38;5;241m=\u001b[39m translate_batch_texts(all_choices, tokenizer, model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_size)\n",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m, in \u001b[0;36mtranslate_batch_texts\u001b[0;34m(texts, tokenizer, model, source_lang, batch_size, save_interval)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m batch_texts:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 16\u001b[0m         translated \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_lang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m         batch_results\u001b[38;5;241m.\u001b[39mappend(translated)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[2], line 156\u001b[0m, in \u001b[0;36mtranslate_text\u001b[0;34m(text, tokenizer, model, source_lang, target_lang)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# ìƒì„± (GPU ìµœì í™”ëœ íŒŒë¼ë¯¸í„°)\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 156\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# input_idsì™€ attention_mask ëª¨ë‘ ì „ë‹¬\u001b[39;49;00m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ë” ì§§ê²Œ (ì†ë„ í–¥ìƒ)\u001b[39;49;00m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ë” ê²°ì •ì ìœ¼ë¡œ (ì†ë„ í–¥ìƒ)\u001b[39;49;00m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ë°˜ë³µ ë°©ì§€\u001b[39;49;00m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ìºì‹œ ì‚¬ìš©ìœ¼ë¡œ ì†ë„ í–¥ìƒ\u001b[39;49;00m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# ë””ì½”ë”©\u001b[39;00m\n\u001b[1;32m    170\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2634\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2626\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2627\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2628\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2629\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2630\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2631\u001b[0m     )\n\u001b[1;32m   2633\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2634\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2635\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2639\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2641\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2642\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2644\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2645\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2646\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2647\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2648\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2649\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2650\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2651\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:3618\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3616\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3617\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3618\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3620\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3621\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3622\u001b[0m     outputs,\n\u001b[1;32m   3623\u001b[0m     model_kwargs,\n\u001b[1;32m   3624\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3625\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:959\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[0;32m--> 959\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    961\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/exaone4/modeling_exaone4.py:491\u001b[0m, in \u001b[0;36mExaone4ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[TransformersKwargs],\n\u001b[1;32m    458\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CausalLMOutputWithPast:\n\u001b[1;32m    459\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;124;03m    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;124;03m        Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    489\u001b[0m \n\u001b[1;32m    490\u001b[0m \u001b[38;5;124;03m    NOTE: `EXAONE-4.0-Instruct` is a placeholder model ID. The exact model ID will be updated in the future.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 491\u001b[0m     outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:1083\u001b[0m, in \u001b[0;36mcheck_model_inputs.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m                 module\u001b[38;5;241m.\u001b[39mforward \u001b[38;5;241m=\u001b[39m make_capture_wrapper(module, original_forward, key, specs\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   1081\u001b[0m                 monkey_patched_layers\u001b[38;5;241m.\u001b[39mappend((module, original_forward))\n\u001b[0;32m-> 1083\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;66;03m# Restore original forward methods\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module, original_forward \u001b[38;5;129;01min\u001b[39;00m monkey_patched_layers:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/exaone4/modeling_exaone4.py:404\u001b[0m, in \u001b[0;36mExaone4Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[1;32m    403\u001b[0m     layer_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayer_types[i]\n\u001b[0;32m--> 404\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(hidden_states)\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[1;32m    418\u001b[0m     last_hidden_state\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    419\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    420\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/exaone4/modeling_exaone4.py:291\u001b[0m, in \u001b[0;36mExaone4DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    281\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[TransformersKwargs],\n\u001b[1;32m    289\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mFloatTensor, Optional[\u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mFloatTensor, torch\u001b[38;5;241m.\u001b[39mFloatTensor]]]:\n\u001b[1;32m    290\u001b[0m     residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 291\u001b[0m     hidden_states, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m    302\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/exaone4/modeling_exaone4.py:230\u001b[0m, in \u001b[0;36mExaone4Attention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     cache_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_position,\n\u001b[1;32m    229\u001b[0m     }\n\u001b[0;32m--> 230\u001b[0m     key_states, value_states \u001b[38;5;241m=\u001b[39m \u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m attention_interface: Callable \u001b[38;5;241m=\u001b[39m eager_attention_forward\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/cache_utils.py:992\u001b[0m, in \u001b[0;36mapply_processors.<locals>._wrapped_update\u001b[0;34m(self, key_states, value_states, layer_idx, cache_kwargs)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    988\u001b[0m     key_states, value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_processor\u001b[38;5;241m.\u001b[39mpre_update(\n\u001b[1;32m    989\u001b[0m         \u001b[38;5;28mself\u001b[39m, key_states, value_states, layer_idx, cache_kwargs\n\u001b[1;32m    990\u001b[0m     )\n\u001b[0;32m--> 992\u001b[0m key_tensors, value_tensors \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    995\u001b[0m     key_tensors, value_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_processor\u001b[38;5;241m.\u001b[39mpost_update(\n\u001b[1;32m    996\u001b[0m         \u001b[38;5;28mself\u001b[39m, key_tensors, value_tensors, layer_idx, cache_kwargs\n\u001b[1;32m    997\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/cache_utils.py:1201\u001b[0m, in \u001b[0;36mCache.update\u001b[0;34m(self, key_states, value_states, layer_idx, cache_kwargs)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;124;03mUpdates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;124;03m    A tuple containing the updated key and value states.\u001b[39;00m\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend_new_layers(layer_idx)\n\u001b[0;32m-> 1201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/cache_utils.py:377\u001b[0m, in \u001b[0;36mSlidingWindowLayer.update\u001b[0;34m(self, key_states, value_states, cache_kwargs)\u001b[0m\n\u001b[1;32m    375\u001b[0m current_seq_len \u001b[38;5;241m=\u001b[39m cache_position[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Use last position to determine current length\u001b[39;00m\n\u001b[1;32m    376\u001b[0m to_shift \u001b[38;5;241m=\u001b[39m current_seq_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_cache_len\n\u001b[0;32m--> 377\u001b[0m indices \u001b[38;5;241m=\u001b[39m (slicing \u001b[38;5;241m+\u001b[39m \u001b[43mto_shift\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_cache_len\n\u001b[1;32m    379\u001b[0m k_out_shifted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys[:, :, indices]\n\u001b[1;32m    380\u001b[0m v_out_shifted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues[:, :, indices]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1. CyberMetric MCQA ë²ˆì—­ (ì˜ì–´ â†’ í•œêµ­ì–´)\n",
    "\n",
    "def translate_cybermetric_mcqa(df: pd.DataFrame, tokenizer, model, batch_size: int = 5):\n",
    "    \"\"\"CyberMetric MCQA ë°ì´í„°ë¥¼ ë²ˆì—­í•©ë‹ˆë‹¤.\"\"\"\n",
    "    \n",
    "    translated_data = []\n",
    "    \n",
    "    print(f\"CyberMetric MCQA ì´ {len(df)}ê°œ í•­ëª© ë²ˆì—­ ì‹œì‘...\")\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "            question = row['Question']\n",
    "            answer = row['Answer']\n",
    "            \n",
    "            # ë²ˆì—­ ìˆ˜í–‰ (CyberMetricì€ ì˜ì–´ë§Œ)\n",
    "            translated_question, translated_answer = translate_mcqa_entry(\n",
    "                question, answer, tokenizer, model, source_lang=\"en\"\n",
    "            )\n",
    "            \n",
    "            translated_data.append({\n",
    "                'Question': translated_question,\n",
    "                'Answer': translated_answer\n",
    "            })\n",
    "            \n",
    "            # ë°°ì¹˜ë§ˆë‹¤ ì ì‹œ ëŒ€ê¸° (ë©”ëª¨ë¦¬ ê´€ë¦¬)\n",
    "            if (idx + 1) % batch_size == 0:\n",
    "                time.sleep(1)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"ë²ˆì—­ ì˜¤ë¥˜ (í–‰ {idx}): {e}\")\n",
    "            # ì˜¤ë¥˜ì‹œ ì›ë³¸ ë°ì´í„° ì‚¬ìš©\n",
    "            translated_data.append({\n",
    "                'Question': row['Question'], \n",
    "                'Answer': row['Answer']\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(translated_data)\n",
    "\n",
    "# ì‹¤ì œ ë²ˆì—­ ì‹¤í–‰ (ëª¨ë¸ì´ ë¡œë“œëœ ê²½ìš°)\n",
    "if 'tokenizer' in locals() and 'model' in locals() and tokenizer is not None:\n",
    "    cyber_mcqa_ko = translate_cybermetric_mcqa_fast(cyber_mcqa, tokenizer, model, batch_size=32)  # ê³ ì† ë²ˆì—­\n",
    "    print(\"CyberMetric MCQA ë²ˆì—­ ì™„ë£Œ!\")\n",
    "    print(cyber_mcqa_ko.head(2))\n",
    "else:\n",
    "    print(\"ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‹¤ì œ ì‹¤í–‰ì‹œ ëª¨ë¸ì„ ë¨¼ì € ë¡œë“œí•˜ì„¸ìš”.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb61b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. SecBench MCQA ë²ˆì—­ (ì¤‘êµ­ì–´ â†’ í•œêµ­ì–´)\n",
    "\n",
    "def translate_secbench_mcqa(df: pd.DataFrame, tokenizer, model, batch_size: int = 5):\n",
    "    \"\"\"SecBench MCQA ë°ì´í„°ë¥¼ ë²ˆì—­í•©ë‹ˆë‹¤.\"\"\"\n",
    "    \n",
    "    translated_data = []\n",
    "    \n",
    "    print(f\"SecBench MCQA ì´ {len(df)}ê°œ í•­ëª© ë²ˆì—­ ì‹œì‘...\")\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "            question = row['Question']\n",
    "            answer = row['Answer']\n",
    "            \n",
    "            # ë²ˆì—­ ìˆ˜í–‰ (SecBenchëŠ” ì˜ì–´/ì¤‘êµ­ì–´ í˜¼ìš©, ìë™ ê°ì§€)\n",
    "            translated_question, translated_answer = translate_mcqa_entry(\n",
    "                question, answer, tokenizer, model, source_lang=\"auto\"\n",
    "            )\n",
    "            \n",
    "            translated_data.append({\n",
    "                'Question': translated_question,\n",
    "                'Answer': translated_answer\n",
    "            })\n",
    "            \n",
    "            # ë°°ì¹˜ë§ˆë‹¤ ì ì‹œ ëŒ€ê¸°\n",
    "            if (idx + 1) % batch_size == 0:\n",
    "                time.sleep(1)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"ë²ˆì—­ ì˜¤ë¥˜ (í–‰ {idx}): {e}\")\n",
    "            translated_data.append({\n",
    "                'Question': row['Question'], \n",
    "                'Answer': row['Answer']\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(translated_data)\n",
    "\n",
    "# ì‹¤ì œ ë²ˆì—­ ì‹¤í–‰\n",
    "if 'tokenizer' in locals() and 'model' in locals() and tokenizer is not None:\n",
    "    sec_mcqa_ko = translate_secbench_mcqa_fast(sec_mcqa, tokenizer, model, batch_size=32)  # ê³ ì† ë²ˆì—­\n",
    "    print(\"SecBench MCQA ë²ˆì—­ ì™„ë£Œ!\")\n",
    "    print(sec_mcqa_ko.head(2))\n",
    "else:\n",
    "    print(\"ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‹¤ì œ ì‹¤í–‰ì‹œ ëª¨ë¸ì„ ë¨¼ì € ë¡œë“œí•˜ì„¸ìš”.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a04ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. SecBench QA ë²ˆì—­ (ì¤‘êµ­ì–´ â†’ í•œêµ­ì–´)\n",
    "\n",
    "def translate_secbench_qa(df: pd.DataFrame, tokenizer, model, batch_size: int = 5):\n",
    "    \"\"\"SecBench QA ë°ì´í„°ë¥¼ ë²ˆì—­í•©ë‹ˆë‹¤.\"\"\"\n",
    "    \n",
    "    translated_data = []\n",
    "    \n",
    "    print(f\"SecBench QA ì´ {len(df)}ê°œ í•­ëª© ë²ˆì—­ ì‹œì‘...\")\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "            question = row['Question']\n",
    "            answer = row['Answer']\n",
    "            \n",
    "            # ë²ˆì—­ ìˆ˜í–‰ (SecBenchëŠ” ì˜ì–´/ì¤‘êµ­ì–´ í˜¼ìš©, ìë™ ê°ì§€)\n",
    "            translated_question, translated_answer = translate_qa_entry(\n",
    "                question, answer, tokenizer, model, source_lang=\"auto\"\n",
    "            )\n",
    "            \n",
    "            translated_data.append({\n",
    "                'Question': translated_question,\n",
    "                'Answer': translated_answer\n",
    "            })\n",
    "            \n",
    "            # ë°°ì¹˜ë§ˆë‹¤ ì ì‹œ ëŒ€ê¸°\n",
    "            if (idx + 1) % batch_size == 0:\n",
    "                time.sleep(1)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"ë²ˆì—­ ì˜¤ë¥˜ (í–‰ {idx}): {e}\")\n",
    "            translated_data.append({\n",
    "                'Question': row['Question'], \n",
    "                'Answer': row['Answer']\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(translated_data)\n",
    "\n",
    "# ì‹¤ì œ ë²ˆì—­ ì‹¤í–‰\n",
    "if 'tokenizer' in locals() and 'model' in locals() and tokenizer is not None:\n",
    "    sec_qa_ko = translate_secbench_qa_fast(sec_qa, tokenizer, model, batch_size=32)  # ê³ ì† ë²ˆì—­\n",
    "    print(\"SecBench QA ë²ˆì—­ ì™„ë£Œ!\")\n",
    "    print(sec_qa_ko.head(2))\n",
    "else:\n",
    "    print(\"ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‹¤ì œ ì‹¤í–‰ì‹œ ëª¨ë¸ì„ ë¨¼ì € ë¡œë“œí•˜ì„¸ìš”.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64f129b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë²ˆì—­ ê²°ê³¼ ì €ì¥ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ!\n",
      "ì‹¤ì œ ë²ˆì—­ ì™„ë£Œ í›„ save_translated_data() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”.\n",
      "âœ… CyberMetric MCQA (ì˜ì–´â†’í•œêµ­ì–´) ì €ì¥ ì™„ë£Œ: ../data/CyberMetric/mcqa.csv (10ê°œ í•­ëª©)\n",
      "âŒ SecBench MCQA (ì¤‘êµ­ì–´â†’í•œêµ­ì–´): ë³€ìˆ˜ 'sec_mcqa_ko'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n",
      "âŒ SecBench QA (ì¤‘êµ­ì–´â†’í•œêµ­ì–´): ë³€ìˆ˜ 'sec_qa_ko'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ë²ˆì—­ ê²°ê³¼ ì €ì¥\n",
    "\n",
    "def save_translated_data():\n",
    "    \"\"\"ë²ˆì—­ëœ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\"\"\"\n",
    "    \n",
    "    # ì €ì¥í•  ë°ì´í„°ì™€ ê²½ë¡œ ì •ì˜\n",
    "    datasets_to_save = [\n",
    "        {\n",
    "            'data': 'cyber_mcqa_ko',\n",
    "            'path': '../data/CyberMetric/mcqa.csv',\n",
    "            'description': 'CyberMetric MCQA (ì˜ì–´â†’í•œêµ­ì–´)'\n",
    "        },\n",
    "        {\n",
    "            'data': 'sec_mcqa_ko', \n",
    "            'path': '../data/SecBench/mcqa.csv',\n",
    "            'description': 'SecBench MCQA (ì¤‘êµ­ì–´â†’í•œêµ­ì–´)'\n",
    "        },\n",
    "        {\n",
    "            'data': 'sec_qa_ko',\n",
    "            'path': '../data/SecBench/qa.csv', \n",
    "            'description': 'SecBench QA (ì¤‘êµ­ì–´â†’í•œêµ­ì–´)'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for dataset_info in datasets_to_save:\n",
    "        data_name = dataset_info['data']\n",
    "        file_path = dataset_info['path']\n",
    "        description = dataset_info['description']\n",
    "        \n",
    "        try:\n",
    "            # ë³€ìˆ˜ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸\n",
    "            if data_name in locals() or data_name in globals():\n",
    "                data = locals().get(data_name) or globals().get(data_name)\n",
    "                \n",
    "                if data is not None and len(data) > 0:\n",
    "                    # CSVë¡œ ì €ì¥\n",
    "                    data.to_csv(file_path, index=False, encoding='utf-8')\n",
    "                    print(f\"âœ… {description} ì €ì¥ ì™„ë£Œ: {file_path} ({len(data)}ê°œ í•­ëª©)\")\n",
    "                else:\n",
    "                    print(f\"âŒ {description}: ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.\")\n",
    "            else:\n",
    "                print(f\"âŒ {description}: ë³€ìˆ˜ '{data_name}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ {description} ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "# ì‹¤ì œ ë²ˆì—­ì´ ì™„ë£Œëœ ê²½ìš°ì—ë§Œ ì €ì¥\n",
    "print(\"ë²ˆì—­ ê²°ê³¼ ì €ì¥ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "print(\"ì‹¤ì œ ë²ˆì—­ ì™„ë£Œ í›„ save_translated_data() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”.\")\n",
    "\n",
    "save_translated_data()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
