{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b3ce014",
   "metadata": {},
   "source": [
    "# í•œêµ­ì–´ ê¸ˆìœµë³´ì•ˆ ë°ì´í„°ì…‹ ë²ˆì—­ ì‘ì—…\n",
    "\n",
    "## ì‘ì—… ê°œìš”\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ì˜ì–´/ì¤‘êµ­ì–´ë¡œ ì‘ì„±ëœ ê¸ˆìœµë³´ì•ˆ ê´€ë ¨ QA ë°ì´í„°ì…‹ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ëŠ” ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ëŒ€ìƒ íŒŒì¼ë“¤\n",
    "1. **CyberMetric/mcqa_org.csv** - ì˜ì–´ ì „ìš© MCQA ë°ì´í„°\n",
    "2. **SecBench/mcqa_org.csv** - ì˜ì–´/ì¤‘êµ­ì–´ í˜¼ìš© MCQA ë°ì´í„°  \n",
    "3. **SecBench/qa_org.csv** - ì˜ì–´/ì¤‘êµ­ì–´ í˜¼ìš© QA ë°ì´í„°\n",
    "\n",
    "## ì‘ì—… íë¦„\n",
    "1. ê° `_org.csv` íŒŒì¼ì„ ì½ì–´ì„œ ë°ì´í„° êµ¬ì¡° ë° ì–¸ì–´ ë¶„í¬ í™•ì¸\n",
    "2. **ì–¸ì–´ ìë™ ê°ì§€**: SecBench ë°ì´í„°ì˜ ì˜ì–´/ì¤‘êµ­ì–´ í˜¼ìš© ìƒí™© ì²˜ë¦¬\n",
    "3. ë¡œì»¬ LLMì„ ì‚¬ìš©í•˜ì—¬ í•œêµ­ì–´ë¡œ ë²ˆì—­\n",
    "4. ë²ˆì—­ëœ ê²°ê³¼ë¥¼ ì›ë˜ í´ë”ì— í•œêµ­ì–´ ë²„ì „ìœ¼ë¡œ ì €ì¥\n",
    "5. ë²ˆì—­ í’ˆì§ˆ ê²€ì¦\n",
    "\n",
    "## ì£¼ìš” ê°œì„ ì‚¬í•­\n",
    "- **ì–¸ì–´ ìë™ ê°ì§€ ê¸°ëŠ¥**: SecBench ë°ì´í„°ì˜ ì˜ì–´/ì¤‘êµ­ì–´ í˜¼ìš© ë¬¸ì œ í•´ê²°\n",
    "- **ê°œë³„ í•­ëª©ë³„ ì–¸ì–´ íŒë‹¨**: ì§ˆë¬¸ê³¼ ì„ íƒì§€ë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ ì–¸ì–´ ê°ì§€\n",
    "- **ì´ë¯¸ ë²ˆì—­ëœ ë‚´ìš© ê±´ë„ˆë›°ê¸°**: í•œêµ­ì–´ í…ìŠ¤íŠ¸ëŠ” ì¬ë²ˆì—­í•˜ì§€ ì•ŠìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202d565b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ì„¤ì •\n",
    "\n",
    "%pip install -q transformers==4.55.0  # LLM ìš”êµ¬ì‚¬í•­: >=4.46.0\n",
    "%pip install -q safetensors==0.4.3    # torch 2.1.0 í˜¸í™˜ì„±\n",
    "%pip install -q bitsandbytes==0.43.2 accelerate==1.9.0  # ì–‘ìí™” ì§€ì›\n",
    "%pip install -q torch torchaudio\n",
    "\n",
    "print(\"ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ!\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6556cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM ëª¨ë¸ ë¡œë“œ ë° ë²ˆì—­ í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "import unicodedata\n",
    "\n",
    "def load_translation_model(model_name=\"microsoft/DialoGPT-medium\"):\n",
    "    \"\"\"ë²ˆì—­ìš© LLM ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.\"\"\"\n",
    "    try:\n",
    "        # ì–‘ìí™” ì„¤ì • (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\"\n",
    "        )\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        \n",
    "        # pad_token ì„¤ì •\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "        print(f\"ëª¨ë¸ '{model_name}' ë¡œë“œ ì™„ë£Œ!\")\n",
    "        return tokenizer, model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "        print(\"ëŒ€ì•ˆ: OpenAI API ë˜ëŠ” ë‹¤ë¥¸ ë²ˆì—­ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\")\n",
    "        return None, None\n",
    "\n",
    "def detect_language(text: str) -> str:\n",
    "    \"\"\"í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ìë™ ê°ì§€í•©ë‹ˆë‹¤.\"\"\"\n",
    "    \n",
    "    # í•œêµ­ì–´ëŠ” ì´ë¯¸ ë²ˆì—­ ì™„ë£Œëœ ê²ƒìœ¼ë¡œ ê°„ì£¼\n",
    "    korean_chars = sum(1 for char in text if '\\uac00' <= char <= '\\ud7af')\n",
    "    if korean_chars > len(text) * 0.1:  # 10% ì´ìƒì´ í•œê¸€ì´ë©´ í•œêµ­ì–´\n",
    "        return \"ko\"\n",
    "    \n",
    "    # ì¤‘êµ­ì–´ ë¬¸ì ê°ì§€ (ê°„ì²´/ë²ˆì²´ í¬í•¨)\n",
    "    chinese_chars = sum(1 for char in text if '\\u4e00' <= char <= '\\u9fff')\n",
    "    \n",
    "    # ì˜ì–´ ë¬¸ì ê°ì§€ (ì•ŒíŒŒë²³)\n",
    "    english_chars = sum(1 for char in text if char.isascii() and char.isalpha())\n",
    "    \n",
    "    # ì „ì²´ í…ìŠ¤íŠ¸ ê¸¸ì´\n",
    "    total_chars = len(text.replace(' ', '').replace('\\n', ''))\n",
    "    \n",
    "    if total_chars == 0:\n",
    "        return \"unknown\"\n",
    "    \n",
    "    # ì¤‘êµ­ì–´ ë¹„ìœ¨ì´ ë†’ìœ¼ë©´ ì¤‘êµ­ì–´\n",
    "    if chinese_chars > total_chars * 0.3:\n",
    "        return \"zh\"\n",
    "    \n",
    "    # ì˜ì–´ ë¹„ìœ¨ì´ ë†’ìœ¼ë©´ ì˜ì–´\n",
    "    if english_chars > total_chars * 0.5:\n",
    "        return \"en\"\n",
    "    \n",
    "    # í˜¼ìš©ë˜ì–´ ìˆëŠ” ê²½ìš° ë” ë§ì€ ìª½ìœ¼ë¡œ íŒë‹¨\n",
    "    if chinese_chars > english_chars:\n",
    "        return \"zh\"\n",
    "    elif english_chars > chinese_chars:\n",
    "        return \"en\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "\n",
    "def translate_text(text: str, tokenizer, model, source_lang: str = \"auto\", target_lang: str = \"ko\") -> str:\n",
    "    \"\"\"í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤. source_langì´ 'auto'ë©´ ìë™ ê°ì§€í•©ë‹ˆë‹¤.\"\"\"\n",
    "    \n",
    "    # ì–¸ì–´ ìë™ ê°ì§€\n",
    "    if source_lang == \"auto\":\n",
    "        detected_lang = detect_language(text)\n",
    "        if detected_lang == \"ko\":\n",
    "            return text  # ì´ë¯¸ í•œêµ­ì–´ë©´ ë²ˆì—­í•˜ì§€ ì•ŠìŒ\n",
    "        elif detected_lang == \"unknown\":\n",
    "            # ì•Œ ìˆ˜ ì—†ëŠ” ê²½ìš° ì˜ì–´ë¡œ ê°€ì •\n",
    "            detected_lang = \"en\"\n",
    "        source_lang = detected_lang\n",
    "    \n",
    "    # ë²ˆì—­ í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "    if source_lang == \"en\":\n",
    "        prompt = f\"Translate the following English text to Korean. Maintain the technical terminology and formatting:\\n\\nEnglish: {text}\\nKorean:\"\n",
    "    elif source_lang == \"zh\":\n",
    "        prompt = f\"Translate the following Chinese text to Korean. Maintain the technical terminology and formatting:\\n\\nChinese: {text}\\nKorean:\"\n",
    "    else:\n",
    "        prompt = f\"Translate to Korean: {text}\\nKorean:\"\n",
    "    \n",
    "    try:\n",
    "        # í† í°í™”\n",
    "        inputs = tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        \n",
    "        # ìƒì„±\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs,\n",
    "                max_new_tokens=200,\n",
    "                num_return_sequences=1,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # ë””ì½”ë”©\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # ë²ˆì—­ ê²°ê³¼ ì¶”ì¶œ\n",
    "        if \"Korean:\" in generated_text:\n",
    "            translated = generated_text.split(\"Korean:\")[-1].strip()\n",
    "        else:\n",
    "            translated = generated_text[len(prompt):].strip()\n",
    "            \n",
    "        return translated if translated else text  # ë²ˆì—­ ì‹¤íŒ¨ì‹œ ì›ë¬¸ ë°˜í™˜\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ë²ˆì—­ ì˜¤ë¥˜: {e}\")\n",
    "        return text  # ì˜¤ë¥˜ì‹œ ì›ë¬¸ ë°˜í™˜\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ (ì‹¤ì œ ì‚¬ìš©ì‹œ ì ì ˆí•œ ëª¨ë¸ëª…ìœ¼ë¡œ ë³€ê²½)\n",
    "print(\"LLM ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "# tokenizer, model = load_translation_model(\"beomi/gemma-ko-7b\")  # ì˜ˆì‹œ ëª¨ë¸ëª…\n",
    "print(\"ì‹¤ì œ ì‚¬ìš©ì‹œ ì ì ˆí•œ ëª¨ë¸ëª…ì„ ì„¤ì •í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3eb7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¡œë“œ ë° êµ¬ì¡° í™•ì¸\n",
    "\n",
    "# 1. CyberMetric MCQA (ì˜ì–´ ì „ìš©)\n",
    "print(\"=== CyberMetric MCQA ë°ì´í„° ===\")\n",
    "cyber_mcqa = pd.read_csv('../data/CyberMetric/mcqa_org.csv')\n",
    "print(f\"ë°ì´í„° í¬ê¸°: {cyber_mcqa.shape}\")\n",
    "print(f\"ì»¬ëŸ¼: {cyber_mcqa.columns.tolist()}\")\n",
    "print(\"\\nìƒ˜í”Œ ë°ì´í„°:\")\n",
    "print(cyber_mcqa.head(2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# 2. SecBench MCQA (ì˜ì–´/ì¤‘êµ­ì–´ í˜¼ìš©)\n",
    "print(\"=== SecBench MCQA ë°ì´í„° ===\")\n",
    "sec_mcqa = pd.read_csv('../data/SecBench/mcqa_org.csv')\n",
    "print(f\"ë°ì´í„° í¬ê¸°: {sec_mcqa.shape}\")\n",
    "print(f\"ì»¬ëŸ¼: {sec_mcqa.columns.tolist()}\")\n",
    "print(\"\\nìƒ˜í”Œ ë°ì´í„°:\")\n",
    "print(sec_mcqa.head(2))\n",
    "\n",
    "# ì–¸ì–´ ë¶„í¬ í™•ì¸\n",
    "print(\"\\nì–¸ì–´ ë¶„í¬ ë¶„ì„:\")\n",
    "languages = []\n",
    "for idx in range(min(100, len(sec_mcqa))):  # ì²˜ìŒ 100ê°œë§Œ ìƒ˜í”Œë§\n",
    "    question = sec_mcqa.iloc[idx]['Question']\n",
    "    lang = detect_language(question.split('\\n')[0])  # ì²« ë²ˆì§¸ ì¤„ë§Œ ê²€ì‚¬\n",
    "    languages.append(lang)\n",
    "\n",
    "from collections import Counter\n",
    "lang_count = Counter(languages)\n",
    "print(f\"ìƒ˜í”Œ 100ê°œ ì¤‘ ì–¸ì–´ ë¶„í¬: {dict(lang_count)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# 3. SecBench QA (ì˜ì–´/ì¤‘êµ­ì–´ í˜¼ìš©)\n",
    "print(\"=== SecBench QA ë°ì´í„° ===\")\n",
    "sec_qa = pd.read_csv('../data/SecBench/qa_org.csv')\n",
    "print(f\"ë°ì´í„° í¬ê¸°: {sec_qa.shape}\")\n",
    "print(f\"ì»¬ëŸ¼: {sec_qa.columns.tolist()}\")\n",
    "print(\"\\nìƒ˜í”Œ ë°ì´í„°:\")\n",
    "print(sec_qa.head(2))\n",
    "\n",
    "# ì–¸ì–´ ë¶„í¬ í™•ì¸\n",
    "print(\"\\nì–¸ì–´ ë¶„í¬ ë¶„ì„:\")\n",
    "qa_languages = []\n",
    "for idx in range(min(100, len(sec_qa))):  # ì²˜ìŒ 100ê°œë§Œ ìƒ˜í”Œë§\n",
    "    question = sec_qa.iloc[idx]['Question']\n",
    "    lang = detect_language(question)\n",
    "    qa_languages.append(lang)\n",
    "\n",
    "qa_lang_count = Counter(qa_languages)\n",
    "print(f\"QA ìƒ˜í”Œ 100ê°œ ì¤‘ ì–¸ì–´ ë¶„í¬: {dict(qa_lang_count)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca553bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCQA ë°ì´í„° ë²ˆì—­ í•¨ìˆ˜\n",
    "\n",
    "def parse_mcqa_question(question_text: str) -> Dict:\n",
    "    \"\"\"MCQA ì§ˆë¬¸ì„ íŒŒì‹±í•˜ì—¬ ë¬¸ì œì™€ ì„ íƒì§€ë¥¼ ë¶„ë¦¬í•©ë‹ˆë‹¤.\"\"\"\n",
    "    lines = question_text.strip().split('\\n')\n",
    "    \n",
    "    # ì²« ë²ˆì§¸ ì¤„ì€ ë¬¸ì œ \n",
    "    question = lines[0]\n",
    "    \n",
    "    # ë‚˜ë¨¸ì§€ëŠ” ì„ íƒì§€\n",
    "    choices = []\n",
    "    for line in lines[1:]:\n",
    "        line = line.strip()\n",
    "        if line and (line.startswith(('1.', '2.', '3.', '4.', '5.', '6.', '7.'))):\n",
    "            choices.append(line)\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'choices': choices\n",
    "    }\n",
    "\n",
    "def translate_mcqa_entry(question_text: str, answer: str, tokenizer, model, source_lang: str = \"auto\") -> Tuple[str, str]:\n",
    "    \"\"\"MCQA ì—”íŠ¸ë¦¬ë¥¼ ë²ˆì—­í•©ë‹ˆë‹¤. ì–¸ì–´ ìë™ ê°ì§€ ì§€ì›.\"\"\"\n",
    "    \n",
    "    # ì§ˆë¬¸ íŒŒì‹±\n",
    "    parsed = parse_mcqa_question(question_text)\n",
    "    \n",
    "    # ì§ˆë¬¸ ë²ˆì—­ (ì–¸ì–´ ìë™ ê°ì§€)\n",
    "    if source_lang == \"auto\":\n",
    "        detected_lang = detect_language(parsed['question'])\n",
    "        print(f\"ì§ˆë¬¸ ì–¸ì–´ ê°ì§€: {detected_lang}\")\n",
    "    else:\n",
    "        detected_lang = source_lang\n",
    "    \n",
    "    translated_question = translate_text(parsed['question'], tokenizer, model, detected_lang)\n",
    "    \n",
    "    # ê° ì„ íƒì§€ ë²ˆì—­\n",
    "    translated_choices = []\n",
    "    for choice in parsed['choices']:\n",
    "        # ì„ íƒì§€ ë²ˆí˜¸ì™€ ë‚´ìš© ë¶„ë¦¬\n",
    "        if '. ' in choice:\n",
    "            num_part, content = choice.split('. ', 1)\n",
    "            # ê° ì„ íƒì§€ë§ˆë‹¤ ì–¸ì–´ ê°ì§€ (í˜¼ìš© ê°€ëŠ¥ì„±)\n",
    "            choice_lang = detect_language(content) if source_lang == \"auto\" else detected_lang\n",
    "            translated_content = translate_text(content, tokenizer, model, choice_lang)\n",
    "            translated_choices.append(f\"{num_part}. {translated_content}\")\n",
    "        else:\n",
    "            choice_lang = detect_language(choice) if source_lang == \"auto\" else detected_lang\n",
    "            translated_choices.append(translate_text(choice, tokenizer, model, choice_lang))\n",
    "    \n",
    "    # ë²ˆì—­ëœ ì§ˆë¬¸ê³¼ ì„ íƒì§€ ê²°í•©\n",
    "    translated_full = translated_question + '\\n' + '\\n'.join(translated_choices)\n",
    "    \n",
    "    # ë‹µë³€ì€ ê·¸ëŒ€ë¡œ ìœ ì§€ (ë²ˆí˜¸ë‚˜ ê°„ë‹¨í•œ í˜•íƒœ)\n",
    "    translated_answer = answer\n",
    "    \n",
    "    return translated_full, translated_answer\n",
    "\n",
    "def translate_qa_entry(question: str, answer: str, tokenizer, model, source_lang: str = \"auto\") -> Tuple[str, str]:\n",
    "    \"\"\"QA ì—”íŠ¸ë¦¬ë¥¼ ë²ˆì—­í•©ë‹ˆë‹¤. ì–¸ì–´ ìë™ ê°ì§€ ì§€ì›.\"\"\"\n",
    "    \n",
    "    # ì§ˆë¬¸ê³¼ ë‹µë³€ ê°ê° ì–¸ì–´ ê°ì§€\n",
    "    if source_lang == \"auto\":\n",
    "        q_lang = detect_language(question)\n",
    "        a_lang = detect_language(answer)\n",
    "        print(f\"ì§ˆë¬¸ ì–¸ì–´: {q_lang}, ë‹µë³€ ì–¸ì–´: {a_lang}\")\n",
    "    else:\n",
    "        q_lang = a_lang = source_lang\n",
    "    \n",
    "    translated_question = translate_text(question, tokenizer, model, q_lang)\n",
    "    translated_answer = translate_text(answer, tokenizer, model, a_lang)\n",
    "    \n",
    "    return translated_question, translated_answer\n",
    "\n",
    "print(\"ë²ˆì—­ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b102d9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CyberMetric MCQA ë²ˆì—­ (ì˜ì–´ â†’ í•œêµ­ì–´)\n",
    "\n",
    "def translate_cybermetric_mcqa(df: pd.DataFrame, tokenizer, model, batch_size: int = 5):\n",
    "    \"\"\"CyberMetric MCQA ë°ì´í„°ë¥¼ ë²ˆì—­í•©ë‹ˆë‹¤.\"\"\"\n",
    "    \n",
    "    translated_data = []\n",
    "    \n",
    "    print(f\"CyberMetric MCQA ì´ {len(df)}ê°œ í•­ëª© ë²ˆì—­ ì‹œì‘...\")\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "            question = row['Question']\n",
    "            answer = row['Answer']\n",
    "            \n",
    "            # ë²ˆì—­ ìˆ˜í–‰ (CyberMetricì€ ì˜ì–´ë§Œ)\n",
    "            translated_question, translated_answer = translate_mcqa_entry(\n",
    "                question, answer, tokenizer, model, source_lang=\"en\"\n",
    "            )\n",
    "            \n",
    "            translated_data.append({\n",
    "                'Question': translated_question,\n",
    "                'Answer': translated_answer\n",
    "            })\n",
    "            \n",
    "            # ë°°ì¹˜ë§ˆë‹¤ ì ì‹œ ëŒ€ê¸° (ë©”ëª¨ë¦¬ ê´€ë¦¬)\n",
    "            if (idx + 1) % batch_size == 0:\n",
    "                time.sleep(1)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"ë²ˆì—­ ì˜¤ë¥˜ (í–‰ {idx}): {e}\")\n",
    "            # ì˜¤ë¥˜ì‹œ ì›ë³¸ ë°ì´í„° ì‚¬ìš©\n",
    "            translated_data.append({\n",
    "                'Question': row['Question'], \n",
    "                'Answer': row['Answer']\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(translated_data)\n",
    "\n",
    "# ì‹¤ì œ ë²ˆì—­ ì‹¤í–‰ (ëª¨ë¸ì´ ë¡œë“œëœ ê²½ìš°)\n",
    "if 'tokenizer' in locals() and 'model' in locals() and tokenizer is not None:\n",
    "    cyber_mcqa_ko = translate_cybermetric_mcqa(cyber_mcqa.head(10), tokenizer, model)  # í…ŒìŠ¤íŠ¸ìš© 10ê°œë§Œ\n",
    "    print(\"CyberMetric MCQA ë²ˆì—­ ì™„ë£Œ!\")\n",
    "    print(cyber_mcqa_ko.head(2))\n",
    "else:\n",
    "    print(\"ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‹¤ì œ ì‹¤í–‰ì‹œ ëª¨ë¸ì„ ë¨¼ì € ë¡œë“œí•˜ì„¸ìš”.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb61b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. SecBench MCQA ë²ˆì—­ (ì¤‘êµ­ì–´ â†’ í•œêµ­ì–´)\n",
    "\n",
    "def translate_secbench_mcqa(df: pd.DataFrame, tokenizer, model, batch_size: int = 5):\n",
    "    \"\"\"SecBench MCQA ë°ì´í„°ë¥¼ ë²ˆì—­í•©ë‹ˆë‹¤.\"\"\"\n",
    "    \n",
    "    translated_data = []\n",
    "    \n",
    "    print(f\"SecBench MCQA ì´ {len(df)}ê°œ í•­ëª© ë²ˆì—­ ì‹œì‘...\")\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "            question = row['Question']\n",
    "            answer = row['Answer']\n",
    "            \n",
    "            # ë²ˆì—­ ìˆ˜í–‰ (SecBenchëŠ” ì˜ì–´/ì¤‘êµ­ì–´ í˜¼ìš©, ìë™ ê°ì§€)\n",
    "            translated_question, translated_answer = translate_mcqa_entry(\n",
    "                question, answer, tokenizer, model, source_lang=\"auto\"\n",
    "            )\n",
    "            \n",
    "            translated_data.append({\n",
    "                'Question': translated_question,\n",
    "                'Answer': translated_answer\n",
    "            })\n",
    "            \n",
    "            # ë°°ì¹˜ë§ˆë‹¤ ì ì‹œ ëŒ€ê¸°\n",
    "            if (idx + 1) % batch_size == 0:\n",
    "                time.sleep(1)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"ë²ˆì—­ ì˜¤ë¥˜ (í–‰ {idx}): {e}\")\n",
    "            translated_data.append({\n",
    "                'Question': row['Question'], \n",
    "                'Answer': row['Answer']\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(translated_data)\n",
    "\n",
    "# ì‹¤ì œ ë²ˆì—­ ì‹¤í–‰\n",
    "if 'tokenizer' in locals() and 'model' in locals() and tokenizer is not None:\n",
    "    sec_mcqa_ko = translate_secbench_mcqa(sec_mcqa.head(10), tokenizer, model)  # í…ŒìŠ¤íŠ¸ìš© 10ê°œë§Œ\n",
    "    print(\"SecBench MCQA ë²ˆì—­ ì™„ë£Œ!\")\n",
    "    print(sec_mcqa_ko.head(2))\n",
    "else:\n",
    "    print(\"ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‹¤ì œ ì‹¤í–‰ì‹œ ëª¨ë¸ì„ ë¨¼ì € ë¡œë“œí•˜ì„¸ìš”.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a04ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. SecBench QA ë²ˆì—­ (ì¤‘êµ­ì–´ â†’ í•œêµ­ì–´)\n",
    "\n",
    "def translate_secbench_qa(df: pd.DataFrame, tokenizer, model, batch_size: int = 5):\n",
    "    \"\"\"SecBench QA ë°ì´í„°ë¥¼ ë²ˆì—­í•©ë‹ˆë‹¤.\"\"\"\n",
    "    \n",
    "    translated_data = []\n",
    "    \n",
    "    print(f\"SecBench QA ì´ {len(df)}ê°œ í•­ëª© ë²ˆì—­ ì‹œì‘...\")\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "            question = row['Question']\n",
    "            answer = row['Answer']\n",
    "            \n",
    "            # ë²ˆì—­ ìˆ˜í–‰ (SecBenchëŠ” ì˜ì–´/ì¤‘êµ­ì–´ í˜¼ìš©, ìë™ ê°ì§€)\n",
    "            translated_question, translated_answer = translate_qa_entry(\n",
    "                question, answer, tokenizer, model, source_lang=\"auto\"\n",
    "            )\n",
    "            \n",
    "            translated_data.append({\n",
    "                'Question': translated_question,\n",
    "                'Answer': translated_answer\n",
    "            })\n",
    "            \n",
    "            # ë°°ì¹˜ë§ˆë‹¤ ì ì‹œ ëŒ€ê¸°\n",
    "            if (idx + 1) % batch_size == 0:\n",
    "                time.sleep(1)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"ë²ˆì—­ ì˜¤ë¥˜ (í–‰ {idx}): {e}\")\n",
    "            translated_data.append({\n",
    "                'Question': row['Question'], \n",
    "                'Answer': row['Answer']\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(translated_data)\n",
    "\n",
    "# ì‹¤ì œ ë²ˆì—­ ì‹¤í–‰\n",
    "if 'tokenizer' in locals() and 'model' in locals() and tokenizer is not None:\n",
    "    sec_qa_ko = translate_secbench_qa(sec_qa.head(10), tokenizer, model)  # í…ŒìŠ¤íŠ¸ìš© 10ê°œë§Œ\n",
    "    print(\"SecBench QA ë²ˆì—­ ì™„ë£Œ!\")\n",
    "    print(sec_qa_ko.head(2))\n",
    "else:\n",
    "    print(\"ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‹¤ì œ ì‹¤í–‰ì‹œ ëª¨ë¸ì„ ë¨¼ì € ë¡œë“œí•˜ì„¸ìš”.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f129b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë²ˆì—­ ê²°ê³¼ ì €ì¥\n",
    "\n",
    "def save_translated_data():\n",
    "    \"\"\"ë²ˆì—­ëœ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\"\"\"\n",
    "    \n",
    "    # ì €ì¥í•  ë°ì´í„°ì™€ ê²½ë¡œ ì •ì˜\n",
    "    datasets_to_save = [\n",
    "        {\n",
    "            'data': 'cyber_mcqa_ko',\n",
    "            'path': '../data/CyberMetric/mcqa.csv',\n",
    "            'description': 'CyberMetric MCQA (ì˜ì–´â†’í•œêµ­ì–´)'\n",
    "        },\n",
    "        {\n",
    "            'data': 'sec_mcqa_ko', \n",
    "            'path': '../data/SecBench/mcqa.csv',\n",
    "            'description': 'SecBench MCQA (ì¤‘êµ­ì–´â†’í•œêµ­ì–´)'\n",
    "        },\n",
    "        {\n",
    "            'data': 'sec_qa_ko',\n",
    "            'path': '../data/SecBench/qa.csv', \n",
    "            'description': 'SecBench QA (ì¤‘êµ­ì–´â†’í•œêµ­ì–´)'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for dataset_info in datasets_to_save:\n",
    "        data_name = dataset_info['data']\n",
    "        file_path = dataset_info['path']\n",
    "        description = dataset_info['description']\n",
    "        \n",
    "        try:\n",
    "            # ë³€ìˆ˜ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸\n",
    "            if data_name in locals() or data_name in globals():\n",
    "                data = locals().get(data_name) or globals().get(data_name)\n",
    "                \n",
    "                if data is not None and len(data) > 0:\n",
    "                    # CSVë¡œ ì €ì¥\n",
    "                    data.to_csv(file_path, index=False, encoding='utf-8')\n",
    "                    print(f\"âœ… {description} ì €ì¥ ì™„ë£Œ: {file_path} ({len(data)}ê°œ í•­ëª©)\")\n",
    "                else:\n",
    "                    print(f\"âŒ {description}: ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.\")\n",
    "            else:\n",
    "                print(f\"âŒ {description}: ë³€ìˆ˜ '{data_name}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ {description} ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "# ì‹¤ì œ ë²ˆì—­ì´ ì™„ë£Œëœ ê²½ìš°ì—ë§Œ ì €ì¥\n",
    "print(\"ë²ˆì—­ ê²°ê³¼ ì €ì¥ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "print(\"ì‹¤ì œ ë²ˆì—­ ì™„ë£Œ í›„ save_translated_data() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bed320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë²ˆì—­ í’ˆì§ˆ ê²€ì¦ ë° ìƒ˜í”Œ í™•ì¸\n",
    "\n",
    "def validate_translation_quality(original_df: pd.DataFrame, translated_df: pd.DataFrame, \n",
    "                                dataset_name: str, sample_size: int = 5):\n",
    "    \"\"\"ë²ˆì—­ í’ˆì§ˆì„ ê²€ì¦í•˜ê³  ìƒ˜í”Œì„ í™•ì¸í•©ë‹ˆë‹¤.\"\"\"\n",
    "    \n",
    "    print(f\"\\n=== {dataset_name} ë²ˆì—­ í’ˆì§ˆ ê²€ì¦ ===\")\n",
    "    print(f\"ì›ë³¸ ë°ì´í„° í¬ê¸°: {len(original_df)}\")\n",
    "    print(f\"ë²ˆì—­ ë°ì´í„° í¬ê¸°: {len(translated_df)}\")\n",
    "    \n",
    "    if len(original_df) != len(translated_df):\n",
    "        print(\"âš ï¸ ê²½ê³ : ì›ë³¸ê³¼ ë²ˆì—­ë³¸ì˜ ë°ì´í„° í¬ê¸°ê°€ ë‹¤ë¦…ë‹ˆë‹¤!\")\n",
    "    \n",
    "    # ìƒ˜í”Œ ë¹„êµ\n",
    "    print(f\"\\nğŸ“ ìƒ˜í”Œ {sample_size}ê°œ ë¹„êµ:\")\n",
    "    for i in range(min(sample_size, len(original_df), len(translated_df))):\n",
    "        print(f\"\\n--- ìƒ˜í”Œ {i+1} ---\")\n",
    "        print(f\"ì›ë³¸ ì§ˆë¬¸: {original_df.iloc[i]['Question'][:100]}...\")\n",
    "        print(f\"ë²ˆì—­ ì§ˆë¬¸: {translated_df.iloc[i]['Question'][:100]}...\")\n",
    "        print(f\"ì›ë³¸ ë‹µë³€: {original_df.iloc[i]['Answer']}\")\n",
    "        print(f\"ë²ˆì—­ ë‹µë³€: {translated_df.iloc[i]['Answer']}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# ì‹¤ì œ ë²ˆì—­ ì™„ë£Œ í›„ í’ˆì§ˆ ê²€ì¦ ì‹¤í–‰\n",
    "def run_validation_after_translation():\n",
    "    \"\"\"ë²ˆì—­ ì™„ë£Œ í›„ í’ˆì§ˆ ê²€ì¦ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        if 'cyber_mcqa_ko' in locals() or 'cyber_mcqa_ko' in globals():\n",
    "            validate_translation_quality(cyber_mcqa, cyber_mcqa_ko, \"CyberMetric MCQA\")\n",
    "    except NameError:\n",
    "        print(\"CyberMetric MCQA ë²ˆì—­ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    try:\n",
    "        if 'sec_mcqa_ko' in locals() or 'sec_mcqa_ko' in globals():\n",
    "            validate_translation_quality(sec_mcqa, sec_mcqa_ko, \"SecBench MCQA\")\n",
    "    except NameError:\n",
    "        print(\"SecBench MCQA ë²ˆì—­ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    try:\n",
    "        if 'sec_qa_ko' in locals() or 'sec_qa_ko' in globals():\n",
    "            validate_translation_quality(sec_qa, sec_qa_ko, \"SecBench QA\")\n",
    "    except NameError:\n",
    "        print(\"SecBench QA ë²ˆì—­ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(\"ë²ˆì—­ í’ˆì§ˆ ê²€ì¦ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6f3be2",
   "metadata": {},
   "source": [
    "# ì „ì²´ ë²ˆì—­ ì‘ì—… ì‹¤í–‰ ê°€ì´ë“œ\n",
    "\n",
    "## ë‹¨ê³„ë³„ ì‹¤í–‰ ë°©ë²•\n",
    "\n",
    "### 1. í™˜ê²½ ì„¤ì •\n",
    "1. ì²« ë²ˆì§¸ ì…€ë¶€í„° \"LLM ëª¨ë¸ ë¡œë“œ\" ì…€ê¹Œì§€ ì‹¤í–‰\n",
    "2. ì ì ˆí•œ ë²ˆì—­ ëª¨ë¸ ì„ íƒ ë° ë¡œë“œ\n",
    "\n",
    "### 2. ëª¨ë¸ ì„ íƒ ì˜µì…˜\n",
    "- **ë¡œì»¬ ëª¨ë¸**: `beomi/gemma-ko-7b`, `microsoft/DialoGPT-medium` ë“±\n",
    "- **API ê¸°ë°˜**: OpenAI GPT, Google Translate API, Papago API\n",
    "- **ê¶Œì¥**: í•œêµ­ì–´ì— íŠ¹í™”ëœ ëª¨ë¸ ì‚¬ìš©\n",
    "\n",
    "### 3. ë²ˆì—­ ì‹¤í–‰\n",
    "```python\n",
    "# ëª¨ë¸ ë¡œë“œ (ì ì ˆí•œ ëª¨ë¸ëª…ìœ¼ë¡œ ë³€ê²½)\n",
    "tokenizer, model = load_translation_model(\"your-model-name\")\n",
    "\n",
    "# ì „ì²´ ë°ì´í„° ë²ˆì—­ (head(10) ì œê±°)\n",
    "cyber_mcqa_ko = translate_cybermetric_mcqa(cyber_mcqa, tokenizer, model)\n",
    "sec_mcqa_ko = translate_secbench_mcqa(sec_mcqa, tokenizer, model)  \n",
    "sec_qa_ko = translate_secbench_qa(sec_qa, tokenizer, model)\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "save_translated_data()\n",
    "\n",
    "# í’ˆì§ˆ ê²€ì¦\n",
    "run_validation_after_translation()\n",
    "```\n",
    "\n",
    "### 4. ì£¼ì˜ì‚¬í•­\n",
    "- ë©”ëª¨ë¦¬ ë¶€ì¡±ì‹œ `batch_size`ë¥¼ ì¤„ì´ì„¸ìš”\n",
    "- ë²ˆì—­ ì†ë„ê°€ ëŠë¦° ê²½ìš° ë” ì‘ì€ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì„¸ìš”\n",
    "- GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•œ ê²½ìš° 4bit ì–‘ìí™”ë¥¼ í™œìš©í•˜ì„¸ìš”\n",
    "\n",
    "### 5. ì¶œë ¥ íŒŒì¼\n",
    "- `../data/CyberMetric/mcqa.csv` - ì˜ì–´â†’í•œêµ­ì–´ MCQA\n",
    "- `../data/SecBench/mcqa.csv` - ì¤‘êµ­ì–´â†’í•œêµ­ì–´ MCQA  \n",
    "- `../data/SecBench/qa.csv` - ì¤‘êµ­ì–´â†’í•œêµ­ì–´ QA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fddf0a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
