{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a14cf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 작업 흐름\n",
    "\n",
    "# ../data 내에 소스별 qa.csv, mcqa.csv 파일 로드\n",
    "# ../../models/name_of_llm 폴더에 있는 모델 로드\n",
    "# 4bit 양자화 후 파인튜닝\n",
    "# 파인튜닝 완료 후 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db11f715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 패키지 설치\n",
    "%pip install -q pandas tqdm \n",
    "%pip install -q transformers==4.55.0 # llm requires >=4.46.0\n",
    "%pip install -q safetensors==0.4.3 # downgrade for torch 2.1.0\n",
    "%pip install -q bitsandbytes==0.43.2 accelerate==1.9.0 # quantization\n",
    "%pip install -q datasets peft scikit-learn # 추가 필요 패키지\n",
    "\n",
    "print(\"✅ 패키지 설치 완료!\")\n",
    "\n",
    "# QLoRA 파인튜닝을 위한 라이브러리 임포트\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU 메모리 확인\n",
    "print(f\"🔥 CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"🔥 GPU 개수: {torch.cuda.device_count()}\")\n",
    "    print(f\"🔥 현재 GPU: {torch.cuda.current_device()}\")\n",
    "    print(f\"🔥 GPU 메모리: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63deee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로딩 및 전처리\n",
    "def load_and_combine_datasets():\n",
    "    \"\"\"모든 소스의 qa.csv와 mcqa.csv 파일을 로딩하고 결합\"\"\"\n",
    "    \n",
    "    data_sources = [\n",
    "        'FinShibainu',\n",
    "        'SecBench', \n",
    "        'CyberMetric'\n",
    "    ]\n",
    "    \n",
    "    all_qa_data = []\n",
    "    all_mcqa_data = []\n",
    "    \n",
    "    print(\"📂 데이터 로딩 중...\")\n",
    "    \n",
    "    for source in data_sources:\n",
    "        # QA 데이터 로딩\n",
    "        qa_path = f\"../data/{source}/qa.csv\" if source != 'SecBench' else f\"../data/{source}/qa_org.csv\"\n",
    "        if os.path.exists(qa_path):\n",
    "            qa_df = pd.read_csv(qa_path, encoding='utf-8-sig')\n",
    "            qa_df['source'] = source\n",
    "            all_qa_data.append(qa_df)\n",
    "            print(f\"✅ {source} QA 데이터: {len(qa_df)}개\")\n",
    "        else:\n",
    "            print(f\"❌ {qa_path} 파일을 찾을 수 없습니다.\")\n",
    "            \n",
    "        # MCQA 데이터 로딩  \n",
    "        mcqa_path = f\"../data/{source}/mcqa.csv\" if source != 'SecBench' and source != 'CyberMetric' else f\"../data/{source}/mcqa_org.csv\"\n",
    "        if os.path.exists(mcqa_path):\n",
    "            mcqa_df = pd.read_csv(mcqa_path, encoding='utf-8-sig')\n",
    "            mcqa_df['source'] = source\n",
    "            all_mcqa_data.append(mcqa_df)\n",
    "            print(f\"✅ {source} MCQA 데이터: {len(mcqa_df)}개\")\n",
    "        else:\n",
    "            print(f\"❌ {mcqa_path} 파일을 찾을 수 없습니다.\")\n",
    "    \n",
    "    # 데이터 결합\n",
    "    combined_qa = pd.concat(all_qa_data, ignore_index=True) if all_qa_data else pd.DataFrame()\n",
    "    combined_mcqa = pd.concat(all_mcqa_data, ignore_index=True) if all_mcqa_data else pd.DataFrame()\n",
    "    \n",
    "    print(f\"\\n📊 전체 QA 데이터: {len(combined_qa)}개\")\n",
    "    print(f\"📊 전체 MCQA 데이터: {len(combined_mcqa)}개\")\n",
    "    \n",
    "    return combined_qa, combined_mcqa\n",
    "\n",
    "# 데이터 로딩\n",
    "qa_data, mcqa_data = load_and_combine_datasets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55041fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 및 포맷팅\n",
    "def format_data_for_training(qa_data, mcqa_data):\n",
    "    \"\"\"학습용 데이터 포맷팅 - 대화형 형태로 변환\"\"\"\n",
    "    \n",
    "    formatted_data = []\n",
    "    \n",
    "    # QA 데이터 처리\n",
    "    if not qa_data.empty:\n",
    "        print(\"🔄 QA 데이터 포맷팅 중...\")\n",
    "        for _, row in qa_data.iterrows():\n",
    "            question = row['Question']\n",
    "            answer = row['Answer']\n",
    "            \n",
    "            # 대화형 포맷으로 변환\n",
    "            text = f\"질문: {question}\\n답변: {answer}\"\n",
    "            formatted_data.append({\"text\": text, \"type\": \"qa\", \"source\": row.get('source', 'unknown')})\n",
    "    \n",
    "    # MCQA 데이터 처리  \n",
    "    if not mcqa_data.empty:\n",
    "        print(\"🔄 MCQA 데이터 포맷팅 중...\")\n",
    "        for _, row in mcqa_data.iterrows():\n",
    "            question = row['Question']\n",
    "            answer = row['Answer']\n",
    "            \n",
    "            # 대화형 포맷으로 변환\n",
    "            text = f\"질문: {question}\\n답변: {answer}\"\n",
    "            formatted_data.append({\"text\": text, \"type\": \"mcqa\", \"source\": row.get('source', 'unknown')})\n",
    "    \n",
    "    print(f\"📝 총 {len(formatted_data)}개의 학습 샘플 생성\")\n",
    "    \n",
    "    # 샘플 확인\n",
    "    if formatted_data:\n",
    "        print(\"\\n🔍 샘플 데이터:\")\n",
    "        print(formatted_data[0]['text'][:200] + \"...\")\n",
    "    \n",
    "    return formatted_data\n",
    "\n",
    "# 데이터 포맷팅\n",
    "formatted_data = format_data_for_training(qa_data, mcqa_data)\n",
    "\n",
    "# Hugging Face Dataset으로 변환\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if formatted_data:\n",
    "    # 훈련/검증 데이터 분할\n",
    "    train_data, val_data = train_test_split(formatted_data, test_size=0.1, random_state=42)\n",
    "    \n",
    "    train_dataset = Dataset.from_list(train_data)\n",
    "    val_dataset = Dataset.from_list(val_data)\n",
    "    \n",
    "    dataset = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'validation': val_dataset\n",
    "    })\n",
    "    \n",
    "    print(f\"🎯 훈련 데이터: {len(train_dataset)}개\")\n",
    "    print(f\"🎯 검증 데이터: {len(val_dataset)}개\")\n",
    "else:\n",
    "    print(\"❌ 포맷팅된 데이터가 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ac96fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 선택 및 설정\n",
    "AVAILABLE_MODELS = {\n",
    "    \"gemma-ko-7b\": \"beomi/gemma-ko-7b\",\n",
    "    \"ax-4.0-light-7b\": \"axolotl-ai-co/ax-4.0-light-7b\", \n",
    "    \"midm-2.0-11.5b\": \"microsoft/DialoGPT-medium-2.0-11.5b\"  # 실제 모델명으로 수정 필요\n",
    "}\n",
    "\n",
    "# 사용할 모델 선택 (여기서 변경하세요)\n",
    "SELECTED_MODEL = \"gemma-ko-7b\"  # \"gemma-ko-7b\", \"ax-4.0-light-7b\", \"midm-2.0-11.5b\" 중 선택\n",
    "\n",
    "MODEL_NAME = AVAILABLE_MODELS[SELECTED_MODEL]\n",
    "MODEL_PATH = f\"../../models/{SELECTED_MODEL}\"  # 로컬 모델 경로\n",
    "\n",
    "print(f\"🎯 선택된 모델: {SELECTED_MODEL}\")\n",
    "print(f\"🤖 모델명: {MODEL_NAME}\")\n",
    "print(f\"📁 로컬 경로: {MODEL_PATH}\")\n",
    "\n",
    "# 4bit 양자화 설정\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 토크나이저 로딩\n",
    "try:\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "        print(\"✅ 로컬 토크나이저 로딩 완료\")\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "        print(\"✅ 허깅페이스 토크나이저 로딩 완료\")\n",
    "        \n",
    "    # 패딩 토큰 설정\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ 토크나이저 로딩 실패: {e}\")\n",
    "\n",
    "# 모델 로딩\n",
    "try:\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_PATH,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "        print(\"✅ 로컬 모델 로딩 완료\")\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\", \n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "        print(\"✅ 허깅페이스 모델 로딩 완료\")\n",
    "        \n",
    "    print(f\"🔥 모델이 로딩된 디바이스: {model.device}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 모델 로딩 실패: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1763e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델별 LoRA target_modules 설정\n",
    "TARGET_MODULES_CONFIG = {\n",
    "    \"gemma-ko-7b\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    \"ax-4.0-light-7b\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], \n",
    "    \"midm-2.0-11.5b\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]  # 모델 구조에 따라 조정\n",
    "}\n",
    "\n",
    "# LoRA 설정 및 모델 준비\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                    # LoRA rank\n",
    "    lora_alpha=32,           # LoRA scaling parameter\n",
    "    target_modules=TARGET_MODULES_CONFIG.get(SELECTED_MODEL, [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]),\n",
    "    lora_dropout=0.1,        # LoRA dropout\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "print(\"🔧 LoRA 설정:\")\n",
    "print(f\"  - Rank: {lora_config.r}\")\n",
    "print(f\"  - Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  - Dropout: {lora_config.lora_dropout}\")\n",
    "print(f\"  - Target modules: {lora_config.target_modules}\")\n",
    "\n",
    "# 모델을 kbit 학습용으로 준비\n",
    "try:\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    print(\"✅ kbit 학습 준비 완료\")\n",
    "    \n",
    "    # LoRA 어댑터 적용\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    print(\"✅ LoRA 어댑터 적용 완료\")\n",
    "    \n",
    "    # 학습 가능한 파라미터 확인\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ LoRA 설정 실패: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038ee7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 토크나이징\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"데이터를 토크나이징하는 함수\"\"\"\n",
    "    # 텍스트 토크나이징\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=False,  # Dynamic padding 사용\n",
    "        max_length=1024,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # labels는 input_ids와 동일하게 설정 (language modeling)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# 데이터셋 토크나이징\n",
    "if 'dataset' in locals():\n",
    "    print(\"🔤 데이터 토크나이징 중...\")\n",
    "    \n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset[\"train\"].column_names,\n",
    "        desc=\"토크나이징 진행\"\n",
    "    )\n",
    "    \n",
    "    print(\"✅ 토크나이징 완료\")\n",
    "    print(f\"📊 토크나이징된 훈련 데이터: {len(tokenized_dataset['train'])}개\")\n",
    "    print(f\"📊 토크나이징된 검증 데이터: {len(tokenized_dataset['validation'])}개\")\n",
    "    \n",
    "    # 샘플 확인\n",
    "    sample = tokenized_dataset[\"train\"][0]\n",
    "    print(f\"🔍 샘플 토큰 길이: {len(sample['input_ids'])}\")\n",
    "    print(f\"🔍 샘플 디코딩: {tokenizer.decode(sample['input_ids'][:100])}...\")\n",
    "else:\n",
    "    print(\"❌ 데이터셋이 준비되지 않았습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba02139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 설정 - 모델별 출력 디렉토리\n",
    "OUTPUT_DIR = f\"./qlora-finetuned-{SELECTED_MODEL}\"\n",
    "CHECKPOINT_DIR = f\"./checkpoints-{SELECTED_MODEL}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    # 훈련 파라미터\n",
    "    num_train_epochs=3,              # 에포크 수\n",
    "    per_device_train_batch_size=1,   # 배치 크기 (메모리에 따라 조정)\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,   # 그래디언트 누적 (실질적 배치 크기 = 1*8=8)\n",
    "    \n",
    "    # 학습률 및 스케줄러\n",
    "    learning_rate=2e-4,              # 학습률\n",
    "    warmup_ratio=0.1,                # 워밍업 비율\n",
    "    lr_scheduler_type=\"cosine\",      # 학습률 스케줄러\n",
    "    \n",
    "    # 정규화\n",
    "    weight_decay=0.01,               # 가중치 감소\n",
    "    max_grad_norm=1.0,               # 그래디언트 클리핑\n",
    "    \n",
    "    # 로깅 및 저장\n",
    "    logging_steps=10,                # 로깅 주기\n",
    "    save_steps=100,                  # 체크포인트 저장 주기\n",
    "    eval_steps=100,                  # 평가 주기\n",
    "    save_total_limit=3,              # 저장할 체크포인트 최대 개수\n",
    "    \n",
    "    # 평가 설정\n",
    "    evaluation_strategy=\"steps\",     # 평가 전략\n",
    "    load_best_model_at_end=True,     # 최고 성능 모델 로딩\n",
    "    metric_for_best_model=\"eval_loss\", # 최고 성능 기준\n",
    "    greater_is_better=False,         # 낮을수록 좋음\n",
    "    \n",
    "    # 메모리 최적화\n",
    "    dataloader_pin_memory=False,     # 메모리 핀 비활성화\n",
    "    remove_unused_columns=False,     # 사용하지 않는 컬럼 제거 비활성화\n",
    "    \n",
    "    # 기타\n",
    "    report_to=None,                  # wandb 등 비활성화\n",
    "    run_name=\"qlora-finetune\",       # 실행 이름\n",
    "    seed=42,                         # 시드\n",
    ")\n",
    "\n",
    "print(\"⚙️ 훈련 설정:\")\n",
    "print(f\"  - 에포크: {training_args.num_train_epochs}\")\n",
    "print(f\"  - 배치 크기: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  - 그래디언트 누적: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  - 실질적 배치 크기: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  - 학습률: {training_args.learning_rate}\")\n",
    "print(f\"  - 출력 디렉토리: {OUTPUT_DIR}\")\n",
    "\n",
    "# 데이터 콜레이터 설정\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Causal LM이므로 False\n",
    "    pad_to_multiple_of=8  # 효율성을 위한 패딩\n",
    ")\n",
    "\n",
    "print(\"✅ 데이터 콜레이터 설정 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fee5aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트레이너 생성 및 훈련 실행\n",
    "if 'tokenized_dataset' in locals() and 'model' in locals():\n",
    "    print(\"🚀 트레이너 생성 중...\")\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    print(\"✅ 트레이너 생성 완료\")\n",
    "    print(f\"📊 훈련 샘플 수: {len(tokenized_dataset['train'])}\")\n",
    "    print(f\"📊 검증 샘플 수: {len(tokenized_dataset['validation'])}\")\n",
    "    \n",
    "    # 훈련 시작 전 메모리 상태 확인\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"🔥 훈련 전 GPU 메모리: {torch.cuda.memory_allocated()/1024**3:.2f}GB / {torch.cuda.memory_reserved()/1024**3:.2f}GB\")\n",
    "    \n",
    "    print(\"\\n🎯 파인튜닝 시작...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # 훈련 실행\n",
    "        trainer.train()\n",
    "        print(\"\\n✅ 파인튜닝 완료!\")\n",
    "        \n",
    "        # 훈련 후 메모리 상태\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"🔥 훈련 후 GPU 메모리: {torch.cuda.memory_allocated()/1024**3:.2f}GB / {torch.cuda.memory_reserved()/1024**3:.2f}GB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 훈련 중 오류 발생: {e}\")\n",
    "        print(\"💡 배치 크기를 줄이거나 그래디언트 누적 단계를 늘려보세요.\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ 모델 또는 데이터셋이 준비되지 않았습니다.\")\n",
    "    print(\"이전 셀들을 먼저 실행해주세요.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51ddd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 및 최종 정리\n",
    "if 'trainer' in locals():\n",
    "    print(\"💾 모델 저장 중...\")\n",
    "    \n",
    "    try:\n",
    "        # 최종 모델 저장\n",
    "        trainer.save_model(OUTPUT_DIR)\n",
    "        tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "        \n",
    "        print(f\"✅ 모델 저장 완료: {OUTPUT_DIR}\")\n",
    "        \n",
    "        # LoRA 어댑터만 별도 저장\n",
    "        lora_adapter_dir = f\"./lora-adapter-{SELECTED_MODEL}\"\n",
    "        model.save_pretrained(lora_adapter_dir)\n",
    "        print(f\"✅ LoRA 어댑터 저장 완료: {lora_adapter_dir}\")\n",
    "        \n",
    "        # 훈련 로그 저장\n",
    "        if hasattr(trainer, 'state') and trainer.state.log_history:\n",
    "            import json\n",
    "            with open(f\"{OUTPUT_DIR}/training_log.json\", 'w', encoding='utf-8') as f:\n",
    "                json.dump(trainer.state.log_history, f, indent=2, ensure_ascii=False)\n",
    "            print(\"✅ 훈련 로그 저장 완료\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 모델 저장 실패: {e}\")\n",
    "    \n",
    "    # 메모리 정리\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"🧹 GPU 메모리 정리 완료\")\n",
    "        \n",
    "    print(\"\\n🎉 QLoRA 파인튜닝 전체 과정 완료!\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"📁 저장된 파일들:\")\n",
    "    print(f\"  - 파인튜닝된 모델: {OUTPUT_DIR}\")\n",
    "    print(f\"  - LoRA 어댑터: {lora_adapter_dir}\")\n",
    "    print(f\"  - 체크포인트: {CHECKPOINT_DIR}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ 트레이너가 생성되지 않았습니다.\")\n",
    "    print(\"이전 셀들을 먼저 실행해주세요.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea29df0",
   "metadata": {},
   "source": [
    "# 🎯 QLoRA 파인튜닝 가이드\n",
    "\n",
    "## 📋 실행 순서\n",
    "1. **라이브러리 설치**: 필요한 패키지들이 설치되어 있는지 확인\n",
    "2. **데이터 로딩**: 모든 소스의 qa.csv, mcqa.csv 파일 로딩\n",
    "3. **모델 준비**: 4bit 양자화 및 LoRA 설정\n",
    "4. **훈련 실행**: 파인튜닝 진행\n",
    "5. **모델 저장**: 결과 모델 저장\n",
    "\n",
    "## ⚙️ 설정 조정 가이드\n",
    "\n",
    "### 메모리 부족 시:\n",
    "- `per_device_train_batch_size`: 1 → 1 (이미 최소)\n",
    "- `gradient_accumulation_steps`: 8 → 16 (실질적 배치 크기 유지)\n",
    "- `max_length`: 1024 → 512 (토큰 길이 줄이기)\n",
    "\n",
    "### 훈련 속도 향상:\n",
    "- `num_train_epochs`: 3 → 1 (에포크 줄이기)\n",
    "- `eval_steps`: 100 → 500 (평가 빈도 줄이기)\n",
    "- `save_steps`: 100 → 500 (저장 빈도 줄이기)\n",
    "\n",
    "### LoRA 설정 조정:\n",
    "- `r`: 16 → 8 (파라미터 수 줄이기)\n",
    "- `lora_alpha`: 32 → 16 (스케일링 조정)\n",
    "\n",
    "## 🔧 패키지 설치\n",
    "첫 번째 셀에서 자동으로 설치됩니다:\n",
    "- `transformers==4.55.0`: LLM 라이브러리\n",
    "- `safetensors==0.4.3`: 모델 저장 형식\n",
    "- `bitsandbytes==0.43.2`: 양자화 라이브러리\n",
    "- `accelerate==1.9.0`: 분산 학습 지원\n",
    "- `datasets`, `peft`, `scikit-learn`: 데이터셋 및 LoRA\n",
    "\n",
    "## 🤖 지원 모델들\n",
    "- **gemma-ko-7b**: 한국어에 특화된 Gemma 모델\n",
    "- **ax-4.0-light-7b**: Axolotl 경량화 모델  \n",
    "- **midm-2.0-11.5b**: Microsoft DialoGPT 기반 모델\n",
    "\n",
    "## 📊 예상 결과\n",
    "- **파인튜닝된 모델**: `./qlora-finetuned-{model_name}/`\n",
    "- **LoRA 어댑터**: `./lora-adapter-{model_name}/`\n",
    "- **체크포인트**: `./checkpoints-{model_name}/`\n",
    "- **훈련 로그**: `training_log.json`\n",
    "\n",
    "## 🔄 모델 변경 방법\n",
    "셀 4에서 `SELECTED_MODEL` 변수를 변경하세요:\n",
    "```python\n",
    "SELECTED_MODEL = \"gemma-ko-7b\"     # 또는 \"ax-4.0-light-7b\", \"midm-2.0-11.5b\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7b6274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 어댑터를 원본 모델에 병합하여 BF16 모델 생성\n",
    "def merge_and_save_bf16_model():\n",
    "    \"\"\"LoRA 어댑터를 원본 모델에 병합하고 BF16으로 저장\"\"\"\n",
    "    \n",
    "    print(\"🔄 LoRA 어댑터를 원본 모델에 병합 중...\")\n",
    "    \n",
    "    try:\n",
    "        # 원본 모델을 BF16으로 다시 로딩 (양자화 없이)\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_PATH if os.path.exists(MODEL_PATH) else MODEL_NAME,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        print(\"✅ 원본 모델 BF16 로딩 완료\")\n",
    "        \n",
    "        # LoRA 어댑터 로딩 및 병합\n",
    "        from peft import PeftModel\n",
    "        \n",
    "        lora_adapter_dir = f\"./lora-adapter-{SELECTED_MODEL}\"\n",
    "        if os.path.exists(lora_adapter_dir):\n",
    "            # LoRA 어댑터를 원본 모델에 로딩\n",
    "            model_with_lora = PeftModel.from_pretrained(base_model, lora_adapter_dir)\n",
    "            print(\"✅ LoRA 어댑터 로딩 완료\")\n",
    "            \n",
    "            # LoRA를 원본 모델에 병합\n",
    "            merged_model = model_with_lora.merge_and_unload()\n",
    "            print(\"✅ LoRA 어댑터 병합 완료\")\n",
    "            \n",
    "            # BF16 모델 저장\n",
    "            bf16_output_dir = f\"./merged-bf16-{SELECTED_MODEL}\"\n",
    "            merged_model.save_pretrained(\n",
    "                bf16_output_dir,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                safe_serialization=True\n",
    "            )\n",
    "            \n",
    "            # 토크나이저도 함께 저장\n",
    "            tokenizer.save_pretrained(bf16_output_dir)\n",
    "            \n",
    "            print(f\"✅ BF16 병합 모델 저장 완료: {bf16_output_dir}\")\n",
    "            \n",
    "            # 메모리 정리\n",
    "            del base_model, model_with_lora, merged_model\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            return bf16_output_dir\n",
    "            \n",
    "        else:\n",
    "            print(f\"❌ LoRA 어댑터를 찾을 수 없습니다: {lora_adapter_dir}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ BF16 모델 병합 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "# BF16 모델 생성 실행\n",
    "if 'trainer' in locals() and os.path.exists(f\"./lora-adapter-{SELECTED_MODEL}\"):\n",
    "    print(\"\\n🔄 LoRA 어댑터를 BF16 모델로 병합 중...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    bf16_model_path = merge_and_save_bf16_model()\n",
    "    \n",
    "    if bf16_model_path:\n",
    "        print(f\"\\n🎉 BF16 병합 모델 생성 완료!\")\n",
    "        print(f\"📁 저장 위치: {bf16_model_path}\")\n",
    "        print(\"💡 이 모델은 양자화 없이 직접 사용할 수 있습니다.\")\n",
    "    else:\n",
    "        print(\"❌ BF16 모델 생성 실패\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ 훈련이 완료되지 않았거나 LoRA 어댑터가 없습니다.\")\n",
    "    print(\"이전 셀들을 먼저 실행해주세요.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
