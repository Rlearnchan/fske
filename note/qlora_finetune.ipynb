{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a14cf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‘ì—… íë¦„\n",
    "\n",
    "# ../data ë‚´ì— ì†ŒìŠ¤ë³„ qa.csv, mcqa.csv íŒŒì¼ ë¡œë“œ\n",
    "# ../../models/name_of_llm í´ë”ì— ìˆëŠ” ëª¨ë¸ ë¡œë“œ\n",
    "# 4bit ì–‘ìí™” í›„ íŒŒì¸íŠœë‹\n",
    "# íŒŒì¸íŠœë‹ ì™„ë£Œ í›„ ëª¨ë¸ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db11f715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "%pip install -q pandas tqdm \n",
    "%pip install -q transformers==4.55.0 # llm requires >=4.46.0\n",
    "%pip install -q safetensors==0.4.3 # downgrade for torch 2.1.0\n",
    "%pip install -q bitsandbytes==0.43.2 accelerate==1.9.0 # quantization\n",
    "%pip install -q datasets peft scikit-learn # ì¶”ê°€ í•„ìš” íŒ¨í‚¤ì§€\n",
    "\n",
    "print(\"âœ… íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ!\")\n",
    "\n",
    "# QLoRA íŒŒì¸íŠœë‹ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU ë©”ëª¨ë¦¬ í™•ì¸\n",
    "print(f\"ğŸ”¥ CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ğŸ”¥ GPU ê°œìˆ˜: {torch.cuda.device_count()}\")\n",
    "    print(f\"ğŸ”¥ í˜„ì¬ GPU: {torch.cuda.current_device()}\")\n",
    "    print(f\"ğŸ”¥ GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63deee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬\n",
    "def load_and_combine_datasets():\n",
    "    \"\"\"ëª¨ë“  ì†ŒìŠ¤ì˜ qa.csvì™€ mcqa.csv íŒŒì¼ì„ ë¡œë”©í•˜ê³  ê²°í•©\"\"\"\n",
    "    \n",
    "    data_sources = [\n",
    "        'FinShibainu',\n",
    "        'SecBench', \n",
    "        'CyberMetric'\n",
    "    ]\n",
    "    \n",
    "    all_qa_data = []\n",
    "    all_mcqa_data = []\n",
    "    \n",
    "    print(\"ğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...\")\n",
    "    \n",
    "    for source in data_sources:\n",
    "        # QA ë°ì´í„° ë¡œë”©\n",
    "        qa_path = f\"../data/{source}/qa.csv\" if source != 'SecBench' else f\"../data/{source}/qa_org.csv\"\n",
    "        if os.path.exists(qa_path):\n",
    "            qa_df = pd.read_csv(qa_path, encoding='utf-8-sig')\n",
    "            qa_df['source'] = source\n",
    "            all_qa_data.append(qa_df)\n",
    "            print(f\"âœ… {source} QA ë°ì´í„°: {len(qa_df)}ê°œ\")\n",
    "        else:\n",
    "            print(f\"âŒ {qa_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            \n",
    "        # MCQA ë°ì´í„° ë¡œë”©  \n",
    "        mcqa_path = f\"../data/{source}/mcqa.csv\" if source != 'SecBench' and source != 'CyberMetric' else f\"../data/{source}/mcqa_org.csv\"\n",
    "        if os.path.exists(mcqa_path):\n",
    "            mcqa_df = pd.read_csv(mcqa_path, encoding='utf-8-sig')\n",
    "            mcqa_df['source'] = source\n",
    "            all_mcqa_data.append(mcqa_df)\n",
    "            print(f\"âœ… {source} MCQA ë°ì´í„°: {len(mcqa_df)}ê°œ\")\n",
    "        else:\n",
    "            print(f\"âŒ {mcqa_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # ë°ì´í„° ê²°í•©\n",
    "    combined_qa = pd.concat(all_qa_data, ignore_index=True) if all_qa_data else pd.DataFrame()\n",
    "    combined_mcqa = pd.concat(all_mcqa_data, ignore_index=True) if all_mcqa_data else pd.DataFrame()\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ì „ì²´ QA ë°ì´í„°: {len(combined_qa)}ê°œ\")\n",
    "    print(f\"ğŸ“Š ì „ì²´ MCQA ë°ì´í„°: {len(combined_mcqa)}ê°œ\")\n",
    "    \n",
    "    return combined_qa, combined_mcqa\n",
    "\n",
    "# ë°ì´í„° ë¡œë”©\n",
    "qa_data, mcqa_data = load_and_combine_datasets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55041fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ì „ì²˜ë¦¬ ë° í¬ë§·íŒ…\n",
    "def format_data_for_training(qa_data, mcqa_data):\n",
    "    \"\"\"í•™ìŠµìš© ë°ì´í„° í¬ë§·íŒ… - ëŒ€í™”í˜• í˜•íƒœë¡œ ë³€í™˜\"\"\"\n",
    "    \n",
    "    formatted_data = []\n",
    "    \n",
    "    # QA ë°ì´í„° ì²˜ë¦¬\n",
    "    if not qa_data.empty:\n",
    "        print(\"ğŸ”„ QA ë°ì´í„° í¬ë§·íŒ… ì¤‘...\")\n",
    "        for _, row in qa_data.iterrows():\n",
    "            question = row['Question']\n",
    "            answer = row['Answer']\n",
    "            \n",
    "            # ëŒ€í™”í˜• í¬ë§·ìœ¼ë¡œ ë³€í™˜\n",
    "            text = f\"ì§ˆë¬¸: {question}\\në‹µë³€: {answer}\"\n",
    "            formatted_data.append({\"text\": text, \"type\": \"qa\", \"source\": row.get('source', 'unknown')})\n",
    "    \n",
    "    # MCQA ë°ì´í„° ì²˜ë¦¬  \n",
    "    if not mcqa_data.empty:\n",
    "        print(\"ğŸ”„ MCQA ë°ì´í„° í¬ë§·íŒ… ì¤‘...\")\n",
    "        for _, row in mcqa_data.iterrows():\n",
    "            question = row['Question']\n",
    "            answer = row['Answer']\n",
    "            \n",
    "            # ëŒ€í™”í˜• í¬ë§·ìœ¼ë¡œ ë³€í™˜\n",
    "            text = f\"ì§ˆë¬¸: {question}\\në‹µë³€: {answer}\"\n",
    "            formatted_data.append({\"text\": text, \"type\": \"mcqa\", \"source\": row.get('source', 'unknown')})\n",
    "    \n",
    "    print(f\"ğŸ“ ì´ {len(formatted_data)}ê°œì˜ í•™ìŠµ ìƒ˜í”Œ ìƒì„±\")\n",
    "    \n",
    "    # ìƒ˜í”Œ í™•ì¸\n",
    "    if formatted_data:\n",
    "        print(\"\\nğŸ” ìƒ˜í”Œ ë°ì´í„°:\")\n",
    "        print(formatted_data[0]['text'][:200] + \"...\")\n",
    "    \n",
    "    return formatted_data\n",
    "\n",
    "# ë°ì´í„° í¬ë§·íŒ…\n",
    "formatted_data = format_data_for_training(qa_data, mcqa_data)\n",
    "\n",
    "# Hugging Face Datasetìœ¼ë¡œ ë³€í™˜\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if formatted_data:\n",
    "    # í›ˆë ¨/ê²€ì¦ ë°ì´í„° ë¶„í• \n",
    "    train_data, val_data = train_test_split(formatted_data, test_size=0.1, random_state=42)\n",
    "    \n",
    "    train_dataset = Dataset.from_list(train_data)\n",
    "    val_dataset = Dataset.from_list(val_data)\n",
    "    \n",
    "    dataset = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'validation': val_dataset\n",
    "    })\n",
    "    \n",
    "    print(f\"ğŸ¯ í›ˆë ¨ ë°ì´í„°: {len(train_dataset)}ê°œ\")\n",
    "    print(f\"ğŸ¯ ê²€ì¦ ë°ì´í„°: {len(val_dataset)}ê°œ\")\n",
    "else:\n",
    "    print(\"âŒ í¬ë§·íŒ…ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ac96fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì„ íƒ ë° ì„¤ì •\n",
    "AVAILABLE_MODELS = {\n",
    "    \"gemma-ko-7b\": \"beomi/gemma-ko-7b\",\n",
    "    \"ax-4.0-light-7b\": \"axolotl-ai-co/ax-4.0-light-7b\", \n",
    "    \"midm-2.0-11.5b\": \"microsoft/DialoGPT-medium-2.0-11.5b\"  # ì‹¤ì œ ëª¨ë¸ëª…ìœ¼ë¡œ ìˆ˜ì • í•„ìš”\n",
    "}\n",
    "\n",
    "# ì‚¬ìš©í•  ëª¨ë¸ ì„ íƒ (ì—¬ê¸°ì„œ ë³€ê²½í•˜ì„¸ìš”)\n",
    "SELECTED_MODEL = \"gemma-ko-7b\"  # \"gemma-ko-7b\", \"ax-4.0-light-7b\", \"midm-2.0-11.5b\" ì¤‘ ì„ íƒ\n",
    "\n",
    "MODEL_NAME = AVAILABLE_MODELS[SELECTED_MODEL]\n",
    "MODEL_PATH = f\"../../models/{SELECTED_MODEL}\"  # ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ\n",
    "\n",
    "print(f\"ğŸ¯ ì„ íƒëœ ëª¨ë¸: {SELECTED_MODEL}\")\n",
    "print(f\"ğŸ¤– ëª¨ë¸ëª…: {MODEL_NAME}\")\n",
    "print(f\"ğŸ“ ë¡œì»¬ ê²½ë¡œ: {MODEL_PATH}\")\n",
    "\n",
    "# 4bit ì–‘ìí™” ì„¤ì •\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë¡œë”©\n",
    "try:\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "        print(\"âœ… ë¡œì»¬ í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ\")\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "        print(\"âœ… í—ˆê¹…í˜ì´ìŠ¤ í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ\")\n",
    "        \n",
    "    # íŒ¨ë”© í† í° ì„¤ì •\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ í† í¬ë‚˜ì´ì € ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "# ëª¨ë¸ ë¡œë”©\n",
    "try:\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_PATH,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "        print(\"âœ… ë¡œì»¬ ëª¨ë¸ ë¡œë”© ì™„ë£Œ\")\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\", \n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "        print(\"âœ… í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ ë¡œë”© ì™„ë£Œ\")\n",
    "        \n",
    "    print(f\"ğŸ”¥ ëª¨ë¸ì´ ë¡œë”©ëœ ë””ë°”ì´ìŠ¤: {model.device}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1763e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ë³„ LoRA target_modules ì„¤ì •\n",
    "TARGET_MODULES_CONFIG = {\n",
    "    \"gemma-ko-7b\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    \"ax-4.0-light-7b\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], \n",
    "    \"midm-2.0-11.5b\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]  # ëª¨ë¸ êµ¬ì¡°ì— ë”°ë¼ ì¡°ì •\n",
    "}\n",
    "\n",
    "# LoRA ì„¤ì • ë° ëª¨ë¸ ì¤€ë¹„\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                    # LoRA rank\n",
    "    lora_alpha=32,           # LoRA scaling parameter\n",
    "    target_modules=TARGET_MODULES_CONFIG.get(SELECTED_MODEL, [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]),\n",
    "    lora_dropout=0.1,        # LoRA dropout\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "print(\"ğŸ”§ LoRA ì„¤ì •:\")\n",
    "print(f\"  - Rank: {lora_config.r}\")\n",
    "print(f\"  - Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  - Dropout: {lora_config.lora_dropout}\")\n",
    "print(f\"  - Target modules: {lora_config.target_modules}\")\n",
    "\n",
    "# ëª¨ë¸ì„ kbit í•™ìŠµìš©ìœ¼ë¡œ ì¤€ë¹„\n",
    "try:\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    print(\"âœ… kbit í•™ìŠµ ì¤€ë¹„ ì™„ë£Œ\")\n",
    "    \n",
    "    # LoRA ì–´ëŒ‘í„° ì ìš©\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    print(\"âœ… LoRA ì–´ëŒ‘í„° ì ìš© ì™„ë£Œ\")\n",
    "    \n",
    "    # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° í™•ì¸\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ LoRA ì„¤ì • ì‹¤íŒ¨: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038ee7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° í† í¬ë‚˜ì´ì§•\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"ë°ì´í„°ë¥¼ í† í¬ë‚˜ì´ì§•í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    # í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=False,  # Dynamic padding ì‚¬ìš©\n",
    "        max_length=1024,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # labelsëŠ” input_idsì™€ ë™ì¼í•˜ê²Œ ì„¤ì • (language modeling)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# ë°ì´í„°ì…‹ í† í¬ë‚˜ì´ì§•\n",
    "if 'dataset' in locals():\n",
    "    print(\"ğŸ”¤ ë°ì´í„° í† í¬ë‚˜ì´ì§• ì¤‘...\")\n",
    "    \n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset[\"train\"].column_names,\n",
    "        desc=\"í† í¬ë‚˜ì´ì§• ì§„í–‰\"\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… í† í¬ë‚˜ì´ì§• ì™„ë£Œ\")\n",
    "    print(f\"ğŸ“Š í† í¬ë‚˜ì´ì§•ëœ í›ˆë ¨ ë°ì´í„°: {len(tokenized_dataset['train'])}ê°œ\")\n",
    "    print(f\"ğŸ“Š í† í¬ë‚˜ì´ì§•ëœ ê²€ì¦ ë°ì´í„°: {len(tokenized_dataset['validation'])}ê°œ\")\n",
    "    \n",
    "    # ìƒ˜í”Œ í™•ì¸\n",
    "    sample = tokenized_dataset[\"train\"][0]\n",
    "    print(f\"ğŸ” ìƒ˜í”Œ í† í° ê¸¸ì´: {len(sample['input_ids'])}\")\n",
    "    print(f\"ğŸ” ìƒ˜í”Œ ë””ì½”ë”©: {tokenizer.decode(sample['input_ids'][:100])}...\")\n",
    "else:\n",
    "    print(\"âŒ ë°ì´í„°ì…‹ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba02139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í›ˆë ¨ ì„¤ì • - ëª¨ë¸ë³„ ì¶œë ¥ ë””ë ‰í† ë¦¬\n",
    "OUTPUT_DIR = f\"./qlora-finetuned-{SELECTED_MODEL}\"\n",
    "CHECKPOINT_DIR = f\"./checkpoints-{SELECTED_MODEL}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    # í›ˆë ¨ íŒŒë¼ë¯¸í„°\n",
    "    num_train_epochs=3,              # ì—í¬í¬ ìˆ˜\n",
    "    per_device_train_batch_size=1,   # ë°°ì¹˜ í¬ê¸° (ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •)\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,   # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  (ì‹¤ì§ˆì  ë°°ì¹˜ í¬ê¸° = 1*8=8)\n",
    "    \n",
    "    # í•™ìŠµë¥  ë° ìŠ¤ì¼€ì¤„ëŸ¬\n",
    "    learning_rate=2e-4,              # í•™ìŠµë¥ \n",
    "    warmup_ratio=0.1,                # ì›Œë°ì—… ë¹„ìœ¨\n",
    "    lr_scheduler_type=\"cosine\",      # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬\n",
    "    \n",
    "    # ì •ê·œí™”\n",
    "    weight_decay=0.01,               # ê°€ì¤‘ì¹˜ ê°ì†Œ\n",
    "    max_grad_norm=1.0,               # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘\n",
    "    \n",
    "    # ë¡œê¹… ë° ì €ì¥\n",
    "    logging_steps=10,                # ë¡œê¹… ì£¼ê¸°\n",
    "    save_steps=100,                  # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì£¼ê¸°\n",
    "    eval_steps=100,                  # í‰ê°€ ì£¼ê¸°\n",
    "    save_total_limit=3,              # ì €ì¥í•  ì²´í¬í¬ì¸íŠ¸ ìµœëŒ€ ê°œìˆ˜\n",
    "    \n",
    "    # í‰ê°€ ì„¤ì •\n",
    "    evaluation_strategy=\"steps\",     # í‰ê°€ ì „ëµ\n",
    "    load_best_model_at_end=True,     # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë”©\n",
    "    metric_for_best_model=\"eval_loss\", # ìµœê³  ì„±ëŠ¥ ê¸°ì¤€\n",
    "    greater_is_better=False,         # ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ìµœì í™”\n",
    "    dataloader_pin_memory=False,     # ë©”ëª¨ë¦¬ í•€ ë¹„í™œì„±í™”\n",
    "    remove_unused_columns=False,     # ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì»¬ëŸ¼ ì œê±° ë¹„í™œì„±í™”\n",
    "    \n",
    "    # ê¸°íƒ€\n",
    "    report_to=None,                  # wandb ë“± ë¹„í™œì„±í™”\n",
    "    run_name=\"qlora-finetune\",       # ì‹¤í–‰ ì´ë¦„\n",
    "    seed=42,                         # ì‹œë“œ\n",
    ")\n",
    "\n",
    "print(\"âš™ï¸ í›ˆë ¨ ì„¤ì •:\")\n",
    "print(f\"  - ì—í¬í¬: {training_args.num_train_epochs}\")\n",
    "print(f\"  - ë°°ì¹˜ í¬ê¸°: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  - ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì : {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  - ì‹¤ì§ˆì  ë°°ì¹˜ í¬ê¸°: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  - í•™ìŠµë¥ : {training_args.learning_rate}\")\n",
    "print(f\"  - ì¶œë ¥ ë””ë ‰í† ë¦¬: {OUTPUT_DIR}\")\n",
    "\n",
    "# ë°ì´í„° ì½œë ˆì´í„° ì„¤ì •\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Causal LMì´ë¯€ë¡œ False\n",
    "    pad_to_multiple_of=8  # íš¨ìœ¨ì„±ì„ ìœ„í•œ íŒ¨ë”©\n",
    ")\n",
    "\n",
    "print(\"âœ… ë°ì´í„° ì½œë ˆì´í„° ì„¤ì • ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fee5aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° í›ˆë ¨ ì‹¤í–‰\n",
    "if 'tokenized_dataset' in locals() and 'model' in locals():\n",
    "    print(\"ğŸš€ íŠ¸ë ˆì´ë„ˆ ìƒì„± ì¤‘...\")\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… íŠ¸ë ˆì´ë„ˆ ìƒì„± ì™„ë£Œ\")\n",
    "    print(f\"ğŸ“Š í›ˆë ¨ ìƒ˜í”Œ ìˆ˜: {len(tokenized_dataset['train'])}\")\n",
    "    print(f\"ğŸ“Š ê²€ì¦ ìƒ˜í”Œ ìˆ˜: {len(tokenized_dataset['validation'])}\")\n",
    "    \n",
    "    # í›ˆë ¨ ì‹œì‘ ì „ ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"ğŸ”¥ í›ˆë ¨ ì „ GPU ë©”ëª¨ë¦¬: {torch.cuda.memory_allocated()/1024**3:.2f}GB / {torch.cuda.memory_reserved()/1024**3:.2f}GB\")\n",
    "    \n",
    "    print(\"\\nğŸ¯ íŒŒì¸íŠœë‹ ì‹œì‘...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # í›ˆë ¨ ì‹¤í–‰\n",
    "        trainer.train()\n",
    "        print(\"\\nâœ… íŒŒì¸íŠœë‹ ì™„ë£Œ!\")\n",
    "        \n",
    "        # í›ˆë ¨ í›„ ë©”ëª¨ë¦¬ ìƒíƒœ\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"ğŸ”¥ í›ˆë ¨ í›„ GPU ë©”ëª¨ë¦¬: {torch.cuda.memory_allocated()/1024**3:.2f}GB / {torch.cuda.memory_reserved()/1024**3:.2f}GB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        print(\"ğŸ’¡ ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì´ê±°ë‚˜ ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ë‹¨ê³„ë¥¼ ëŠ˜ë ¤ë³´ì„¸ìš”.\")\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ ëª¨ë¸ ë˜ëŠ” ë°ì´í„°ì…‹ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"ì´ì „ ì…€ë“¤ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51ddd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì €ì¥ ë° ìµœì¢… ì •ë¦¬\n",
    "if 'trainer' in locals():\n",
    "    print(\"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...\")\n",
    "    \n",
    "    try:\n",
    "        # ìµœì¢… ëª¨ë¸ ì €ì¥\n",
    "        trainer.save_model(OUTPUT_DIR)\n",
    "        tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "        \n",
    "        print(f\"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {OUTPUT_DIR}\")\n",
    "        \n",
    "        # LoRA ì–´ëŒ‘í„°ë§Œ ë³„ë„ ì €ì¥\n",
    "        lora_adapter_dir = f\"./lora-adapter-{SELECTED_MODEL}\"\n",
    "        model.save_pretrained(lora_adapter_dir)\n",
    "        print(f\"âœ… LoRA ì–´ëŒ‘í„° ì €ì¥ ì™„ë£Œ: {lora_adapter_dir}\")\n",
    "        \n",
    "        # í›ˆë ¨ ë¡œê·¸ ì €ì¥\n",
    "        if hasattr(trainer, 'state') and trainer.state.log_history:\n",
    "            import json\n",
    "            with open(f\"{OUTPUT_DIR}/training_log.json\", 'w', encoding='utf-8') as f:\n",
    "                json.dump(trainer.state.log_history, f, indent=2, ensure_ascii=False)\n",
    "            print(\"âœ… í›ˆë ¨ ë¡œê·¸ ì €ì¥ ì™„ë£Œ\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\")\n",
    "        \n",
    "    print(\"\\nğŸ‰ QLoRA íŒŒì¸íŠœë‹ ì „ì²´ ê³¼ì • ì™„ë£Œ!\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"ğŸ“ ì €ì¥ëœ íŒŒì¼ë“¤:\")\n",
    "    print(f\"  - íŒŒì¸íŠœë‹ëœ ëª¨ë¸: {OUTPUT_DIR}\")\n",
    "    print(f\"  - LoRA ì–´ëŒ‘í„°: {lora_adapter_dir}\")\n",
    "    print(f\"  - ì²´í¬í¬ì¸íŠ¸: {CHECKPOINT_DIR}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ íŠ¸ë ˆì´ë„ˆê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"ì´ì „ ì…€ë“¤ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea29df0",
   "metadata": {},
   "source": [
    "# ğŸ¯ QLoRA íŒŒì¸íŠœë‹ ê°€ì´ë“œ\n",
    "\n",
    "## ğŸ“‹ ì‹¤í–‰ ìˆœì„œ\n",
    "1. **ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜**: í•„ìš”í•œ íŒ¨í‚¤ì§€ë“¤ì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸\n",
    "2. **ë°ì´í„° ë¡œë”©**: ëª¨ë“  ì†ŒìŠ¤ì˜ qa.csv, mcqa.csv íŒŒì¼ ë¡œë”©\n",
    "3. **ëª¨ë¸ ì¤€ë¹„**: 4bit ì–‘ìí™” ë° LoRA ì„¤ì •\n",
    "4. **í›ˆë ¨ ì‹¤í–‰**: íŒŒì¸íŠœë‹ ì§„í–‰\n",
    "5. **ëª¨ë¸ ì €ì¥**: ê²°ê³¼ ëª¨ë¸ ì €ì¥\n",
    "\n",
    "## âš™ï¸ ì„¤ì • ì¡°ì • ê°€ì´ë“œ\n",
    "\n",
    "### ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ:\n",
    "- `per_device_train_batch_size`: 1 â†’ 1 (ì´ë¯¸ ìµœì†Œ)\n",
    "- `gradient_accumulation_steps`: 8 â†’ 16 (ì‹¤ì§ˆì  ë°°ì¹˜ í¬ê¸° ìœ ì§€)\n",
    "- `max_length`: 1024 â†’ 512 (í† í° ê¸¸ì´ ì¤„ì´ê¸°)\n",
    "\n",
    "### í›ˆë ¨ ì†ë„ í–¥ìƒ:\n",
    "- `num_train_epochs`: 3 â†’ 1 (ì—í¬í¬ ì¤„ì´ê¸°)\n",
    "- `eval_steps`: 100 â†’ 500 (í‰ê°€ ë¹ˆë„ ì¤„ì´ê¸°)\n",
    "- `save_steps`: 100 â†’ 500 (ì €ì¥ ë¹ˆë„ ì¤„ì´ê¸°)\n",
    "\n",
    "### LoRA ì„¤ì • ì¡°ì •:\n",
    "- `r`: 16 â†’ 8 (íŒŒë¼ë¯¸í„° ìˆ˜ ì¤„ì´ê¸°)\n",
    "- `lora_alpha`: 32 â†’ 16 (ìŠ¤ì¼€ì¼ë§ ì¡°ì •)\n",
    "\n",
    "## ğŸ”§ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "ì²« ë²ˆì§¸ ì…€ì—ì„œ ìë™ìœ¼ë¡œ ì„¤ì¹˜ë©ë‹ˆë‹¤:\n",
    "- `transformers==4.55.0`: LLM ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "- `safetensors==0.4.3`: ëª¨ë¸ ì €ì¥ í˜•ì‹\n",
    "- `bitsandbytes==0.43.2`: ì–‘ìí™” ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "- `accelerate==1.9.0`: ë¶„ì‚° í•™ìŠµ ì§€ì›\n",
    "- `datasets`, `peft`, `scikit-learn`: ë°ì´í„°ì…‹ ë° LoRA\n",
    "\n",
    "## ğŸ¤– ì§€ì› ëª¨ë¸ë“¤\n",
    "- **gemma-ko-7b**: í•œêµ­ì–´ì— íŠ¹í™”ëœ Gemma ëª¨ë¸\n",
    "- **ax-4.0-light-7b**: Axolotl ê²½ëŸ‰í™” ëª¨ë¸  \n",
    "- **midm-2.0-11.5b**: Microsoft DialoGPT ê¸°ë°˜ ëª¨ë¸\n",
    "\n",
    "## ğŸ“Š ì˜ˆìƒ ê²°ê³¼\n",
    "- **íŒŒì¸íŠœë‹ëœ ëª¨ë¸**: `./qlora-finetuned-{model_name}/`\n",
    "- **LoRA ì–´ëŒ‘í„°**: `./lora-adapter-{model_name}/`\n",
    "- **ì²´í¬í¬ì¸íŠ¸**: `./checkpoints-{model_name}/`\n",
    "- **í›ˆë ¨ ë¡œê·¸**: `training_log.json`\n",
    "\n",
    "## ğŸ”„ ëª¨ë¸ ë³€ê²½ ë°©ë²•\n",
    "ì…€ 4ì—ì„œ `SELECTED_MODEL` ë³€ìˆ˜ë¥¼ ë³€ê²½í•˜ì„¸ìš”:\n",
    "```python\n",
    "SELECTED_MODEL = \"gemma-ko-7b\"     # ë˜ëŠ” \"ax-4.0-light-7b\", \"midm-2.0-11.5b\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7b6274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA ì–´ëŒ‘í„°ë¥¼ ì›ë³¸ ëª¨ë¸ì— ë³‘í•©í•˜ì—¬ BF16 ëª¨ë¸ ìƒì„±\n",
    "def merge_and_save_bf16_model():\n",
    "    \"\"\"LoRA ì–´ëŒ‘í„°ë¥¼ ì›ë³¸ ëª¨ë¸ì— ë³‘í•©í•˜ê³  BF16ìœ¼ë¡œ ì €ì¥\"\"\"\n",
    "    \n",
    "    print(\"ğŸ”„ LoRA ì–´ëŒ‘í„°ë¥¼ ì›ë³¸ ëª¨ë¸ì— ë³‘í•© ì¤‘...\")\n",
    "    \n",
    "    try:\n",
    "        # ì›ë³¸ ëª¨ë¸ì„ BF16ìœ¼ë¡œ ë‹¤ì‹œ ë¡œë”© (ì–‘ìí™” ì—†ì´)\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_PATH if os.path.exists(MODEL_PATH) else MODEL_NAME,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        print(\"âœ… ì›ë³¸ ëª¨ë¸ BF16 ë¡œë”© ì™„ë£Œ\")\n",
    "        \n",
    "        # LoRA ì–´ëŒ‘í„° ë¡œë”© ë° ë³‘í•©\n",
    "        from peft import PeftModel\n",
    "        \n",
    "        lora_adapter_dir = f\"./lora-adapter-{SELECTED_MODEL}\"\n",
    "        if os.path.exists(lora_adapter_dir):\n",
    "            # LoRA ì–´ëŒ‘í„°ë¥¼ ì›ë³¸ ëª¨ë¸ì— ë¡œë”©\n",
    "            model_with_lora = PeftModel.from_pretrained(base_model, lora_adapter_dir)\n",
    "            print(\"âœ… LoRA ì–´ëŒ‘í„° ë¡œë”© ì™„ë£Œ\")\n",
    "            \n",
    "            # LoRAë¥¼ ì›ë³¸ ëª¨ë¸ì— ë³‘í•©\n",
    "            merged_model = model_with_lora.merge_and_unload()\n",
    "            print(\"âœ… LoRA ì–´ëŒ‘í„° ë³‘í•© ì™„ë£Œ\")\n",
    "            \n",
    "            # BF16 ëª¨ë¸ ì €ì¥\n",
    "            bf16_output_dir = f\"./merged-bf16-{SELECTED_MODEL}\"\n",
    "            merged_model.save_pretrained(\n",
    "                bf16_output_dir,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                safe_serialization=True\n",
    "            )\n",
    "            \n",
    "            # í† í¬ë‚˜ì´ì €ë„ í•¨ê»˜ ì €ì¥\n",
    "            tokenizer.save_pretrained(bf16_output_dir)\n",
    "            \n",
    "            print(f\"âœ… BF16 ë³‘í•© ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {bf16_output_dir}\")\n",
    "            \n",
    "            # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "            del base_model, model_with_lora, merged_model\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            return bf16_output_dir\n",
    "            \n",
    "        else:\n",
    "            print(f\"âŒ LoRA ì–´ëŒ‘í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {lora_adapter_dir}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ BF16 ëª¨ë¸ ë³‘í•© ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "\n",
    "# BF16 ëª¨ë¸ ìƒì„± ì‹¤í–‰\n",
    "if 'trainer' in locals() and os.path.exists(f\"./lora-adapter-{SELECTED_MODEL}\"):\n",
    "    print(\"\\nğŸ”„ LoRA ì–´ëŒ‘í„°ë¥¼ BF16 ëª¨ë¸ë¡œ ë³‘í•© ì¤‘...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    bf16_model_path = merge_and_save_bf16_model()\n",
    "    \n",
    "    if bf16_model_path:\n",
    "        print(f\"\\nğŸ‰ BF16 ë³‘í•© ëª¨ë¸ ìƒì„± ì™„ë£Œ!\")\n",
    "        print(f\"ğŸ“ ì €ì¥ ìœ„ì¹˜: {bf16_model_path}\")\n",
    "        print(\"ğŸ’¡ ì´ ëª¨ë¸ì€ ì–‘ìí™” ì—†ì´ ì§ì ‘ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "    else:\n",
    "        print(\"âŒ BF16 ëª¨ë¸ ìƒì„± ì‹¤íŒ¨\")\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ í›ˆë ¨ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ê±°ë‚˜ LoRA ì–´ëŒ‘í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"ì´ì „ ì…€ë“¤ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
